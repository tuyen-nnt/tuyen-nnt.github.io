
var documents = [{
    "id": 0,
    "url": "https://tuyen-nnt.github.io/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://tuyen-nnt.github.io/about",
    "title": "Hello, My name is Tuyen (Marie Nguyen). Welcome to my page!",
    "body": "This website is a demonstration to see Memoirs Jekyll theme in action. The theme is compatible with Github pages, in fact even this demo itself is created with Github Pages and hosted with Github.  Get Memoirs for Jekyll → "
    }, {
    "id": 2,
    "url": "https://tuyen-nnt.github.io/authors",
    "title": "Authors",
    "body": "                                                                                                                                                                                    Tuyen:         Author of this blog, mostly about Technical which is the field I am interested in.                "
    }, {
    "id": 3,
    "url": "https://tuyen-nnt.github.io/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 4,
    "url": "https://tuyen-nnt.github.io/contact",
    "title": "Contact",
    "body": "  Please send your message to Tuyen Nguyen. We will reply as soon as possible!   "
    }, {
    "id": 5,
    "url": "https://tuyen-nnt.github.io/",
    "title": "Home",
    "body": "                                                      Apache Druid              :       Apache Druid:cre: https://druid. apache. org/:                               07 May 2022        &lt;/span&gt;                                                                                                                   Kafka Connect              :       Write-head logging:                                                                               Tuyen                 22 Jan 2022                                                                                                                           Query Elasticsearch              :       Các cách query::                                                                               Tuyen                 15 Jan 2022                                                                                                                           Làm được gì từ web data?              :       Mở socket » connect đến đường dẫn và port » Tạo biến string cmd request GET, POST,… » encode string cmd thành dạng byte » gửi request đi. :                                                                               Tuyen                 21 Dec 2021                                                                                                                           MongoDB cơ bản              :       Bắt đầu với MongoDB:                                                                               Tuyen                 14 Nov 2021                                                                                                                           Tìm hiểu Network              :       I. Mô hình mạng 5 tầng Tầng 1: Physical:                                                                               Tuyen                 18 Oct 2021                                   &laquo;          1        2        3        4       &raquo; "
    }, {
    "id": 6,
    "url": "https://tuyen-nnt.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 7,
    "url": "https://tuyen-nnt.github.io/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "https://tuyen-nnt.github.io/page3/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "https://tuyen-nnt.github.io/page4/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "https://tuyen-nnt.github.io/page5/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "https://tuyen-nnt.github.io/apache-druid/",
    "title": "Apache Druid",
    "body": "2022/05/07 - Apache Druid:: cre: https://druid. apache. org/  Là một loại database Có các đặc điểm sau:     Real-time ingest data   Hiệu suất query nhanh   High uptime đảm bảo luôn đáp ứng được tính toán (quan trọng)    Thường được sử dụng để:     Power cho GUIs   Các ứng dụng analytical   Là backend cho các highly-concurent APIs mà cần tính toán nhanh.    Thường sử dụng Druid cho những bài toán nào?  BI/OLAP   Clickstream analytics (web và mobile)Đây là nơi chứa lượng data lớn như clicks, impressions, hoạt động view pages, add to card. Thông thường người ta sẽ tính toán lại các logic lấy được từ Google Analytics (được embedd vào web/mobile) về và kết hợp với sale data để cho ra các adhoc analysis phục vụ Biz.     Risk/Fraud analysisKhi đã có đủ dữ liệu thì tìm insight dễ dàng.     Application performance metrics:Lấy các chỉ số ví dụ như startup time, load pages từ hệ thống tracking về đưa vào Druid để monitor và alert realtime, giúp theo dõi và optimize hiệu năng của chương trình.   Server metrics storage Supply chain analytics (manufacturing metrics) Application performance metrics Digital marketing/advertising analytics Network telemetry analytics (network performance monitoring)Triển khai thử với Druid cần gì?  3 em VM (8cores mỗi em thôi) kiến trúc microservice của Druid bắt buộc phải có: ZooKeeper, Coordinator, Overlord, Historical, Broker, Middlemanager.  Khi số lượng node lên hàng chục hoặc hàng trăm thì phải cài đặt thông qua Ansible. So sánh giữa Druid và Spark:: Spark:Spark là cluster computing framework được tạo ra dựa trên concept của Resilient Distributed Datasets (RDDs). RDDs cho phép tái sử dụng lại dữ liệu bằng cách lưu trữ lại các kết quả trung gian trong memory và cho phép Spark cung cấp các tính toán nhanh cho các thuật toán hay lặp lại. =&gt; phù hợp nhiều cho Machine learning Tính tổng quan của Spark giúp nó phù hợp để làm sạch và transform dữ liệu. Mặc dù Spark có khả năng truy vấn dữ liệu thông qua SparkSQL nhưng nó không phù hợp cho các tương tác query cần tốc độ nhanh trong vài giây. Druid:Thường được sử dụng để tăng tốc cho OLAP queries trong Spark. Sứ mệnh của nó là tập trung chủ yếu giảm độ trễ queries xuống mức tối đa và là framework lý tưởng để tăng tốc cho ứng dụng được sử dụng bởi hàng ngàn user. Làm sao để mà user có thể tương tác và nhận được kết quả nhanh chóng nhất. Druid index vào toàn bộ data và có thể làm lớp đứng giữa Spark và ứng dụng của bạn. Ứng dụng cơ bản nhất người ta setup đó là dùng Spark để xử lý data và load data đã được xử lý vào Druid để có thể truy cập query nhanh chóng. cre: https://druid. apache. org/docs/latest/comparisons/druid-vs-spark. html Đọc thêm: https://www. linkedin. com/pulse/combining-druid-spark-interactive-flexible-analytics-scale-butani Hadoop là gì?:    Là Apache framework viết bằng Java cho phép phát triển ứng dụng phân tán có cường độ dữ liệu lớn miễn phí. Được thiết kế để mở rộng máy chủ đơn sang hàng ngàn máy tính khác có tính toán và lưu trữ cục bộ (local computation và storage). =&gt; Hadoop cung cấp môi trường để thực thi các Map-Reduce task.     Hadoop có cấu trúc liên kết master-slave, chỉ có 1 node master hoạt động tại 1 thời điểm và nhiều node slave:      Node master có nhiệm vụ giao task cho các slave   Node slave lưu trữ dữ liệu thực, thực tế thường không mạnh lắm.    Node slave lưu trữ metadata (thông tin của tất cả các node).    Có 2 loại file metadata:FsImage: Là nơi lưu trữ trạng thái của các file system namespace (từ này hơi khó để dịch) kể từ khi namenode được khởi động. EditLogs: Là nơi lưu lại quá sửa đổi của các file system.  Kiến trúc Hadoop gồm có ba lớp chính đó là     HDFS (Hadoop Distributed File System)   Map-Reduce   Yarn   HDFS (Hadoop Distributed File System)Là hệ thống phân tán cung cấp khả năng lưu trữ dữ liệu khổng lồ và tính năng giúp tối ưu việc sử dụng băng thông giữa các node. Đặc biệt cho phép truy cập nhiều disk như là 1 disk. Nói cách khác là không bị giới hạn về dung lượng, nếu cần thêm chỉ cần thêm Node vào hệ thống là ok. Có thể được sử dụng để chạy trên cluster lớn với hàng chục ngàn node. Có kiến trúc Master-Slave với:  NameNode chạy trên Master, có task quản lý Namespace (địa chỉ của file đó được chia và lưu trên các datanode nào) và điều chỉnh truy cập tệp của client cũng như điều khiển load-balancing cho các datanode.  DataNode chạy trên các Slave, có task lưu trữ biz data thực tế. Tìm hiểu thêm tại: https://phambinh. net/bai-viet/kien-truc-cua-hdfs/ Một tập tin HDFS sẽ được chia làm nhiều block và được lưu trữ trong các DataNodes. Kích thước 1 block thường là 64MB và có thể thay đổi bằng việc cấu hình. Map-Reduce: Là framework dùng để viết các application giúp xử lý song song lượng dữ liệu lớn có khả năng https://www. youtube. com/watch?v=nfMANR13ZSA Apache Flink:: Là framework để xử lý data.  Use case:     Fraud Detection =&gt; realtime   Offer Recommendation based on browsing history =&gt; realtime    Batch: Data at rest -&gt; analysis on historical data Realtime: processing the event as and when it happens"
    }, {
    "id": 12,
    "url": "https://tuyen-nnt.github.io/kafka-connect/",
    "title": "Kafka Connect",
    "body": "2022/01/22 - Write-head logging: WAL là một trong các kỹ thuật cung cấp tính chất Atomicity và Durability (2 trong số 2 tính chất ACID) trong hệ thống database. Các thay đổi được lưu lại trong log và sẽ phải được ghi vào trong stable storage (thường không phải là disk vì không có tính chất automic) trước khi các thay đổi đó được ghi vào database. Trong 1 cái hệ thống sử dụng WAL, tất cả mọi thay đổi được ghi vào log trước khi được áp dụng. Thường thì các thay đổi redo và undo được lưu vào trong log. Ok, mục đích sử dụng của WAL có thể hiểu được qua ví dụ sau:    Khi mà chương trình của bạn đang chạy nhưng bị crash hay mất điện giữa chừng. Sau khi khởi động lại thì chương trình cần biết là operation đã thành công hay thành công 1 nửa hay thất bại hay như thế nào.     Lúc này nếu sử dụng WAL, thì chương trình có thể xem log để so sánh với những gì đã thực hiện và chưa thực hiện lúc chương trình bị crash và quyết định xem sẽ undo lại operation hay hoàn tất nốt operation hoặc giữ nguyên mọi thứ.  Các hệ quản trị cơ sở dữ liệu khác nhau sẽ có tên gọi cho WAL log khác nhau. Trong MySQL sẽ là binlog. Ngoài ra 1 transaction được xem là thành công thì các statements của nó phải toàn bộ thành công, ngược lại nếu 1 trong các statement cấu thành 1 transaction thất bại thì là thất bại. Điều này đảm bảo tính automicity trong ACID để được gọi là 1 database transaction. Kafka Connect: Kafka Connect là 1 component của Kafka giúp kết nối và transfer data giữa các external system với Kafka. Bây giờ đi từ use case, bạn cần kết nối data giữa Order database đến Kafka Cluster và từ Kafka Cluster kết nối đến Data warehouse để phân tích data, có 2 cách:  Cách 1: Sửa source code của Order service để tạo producer trong đó đọc data từ database rồi gửi đến Kafka cluster.  Nhược điểm: khi cần thay đổi code cho producer thì phải deploy lại service có thể gây downtime ảnh hưởng đến người dùng. Phụ thuộc vào Order service chạy hay không chạy (nếu service có multi-replicas thì không sao).  Cách 2: Tạo Kafka producer độc lập đọc data từ Order database và produce đến Kafka broker.  Nhược điểm: cần maintain thêm service này cho Kafka producer. Cần thiết kế để scalable, fault tolerance,… =&gt; Kafka Connect sử dụng cách 2. Hiện Kafka connect đã có open-source framework cho phép chọn loại connector (là loại source/target) rồi config và sử dụng, hoặc bạn có thể tự viết connector riêng dựa trên framework này. Ta sử dụng có sẵn thì sẽ không cần 1 dòng code nào mà chỉ cần tạo config để nó hoạt động còn lại Kafka connect sẽ tự lo. Tương tự ta sẽ cần config cho 1 Kafka connect để connect từ Kafka cluster đến Data warehouse.  Có 2 loại connect là Source connector và Sink connector:  Bên trái được gọi là Source connector: pull data từ source system và gửi đến Kafka cluster. Bên phải là Sink connector: consume data từ topic và sink đến hệ thống đích. Cách hoạt động của Kafka connect: Kafka connect như 1 interface, ở abstract level, tạo các API để 3rd party có thể sử dụng dựa trên cách hoạt động của từng service. Nó chỉ cung cấp phần API đóng mở connection, tạo statement, execute query, tương tác với database. Còn lại là việc của database driver cụ thể như JDBC driver cho Mysql,… Kafka connect framework bao gồm:   Source connector:    SourceConnector.     SourceTask.   Sink connector:    SinkConnector.     SinkTask. =&gt; Ta chỉ cần implement 2 class cho mỗi Kafka connect là:   SourceConnector và SourceTask.   Hoặc SinkConnector và SinkTask. Còn lại 1 số thứ cơ bản như độ scalibility , fault tolerance hay error handling đã có Kafka connect lo.  Kafka connect Transformation: Đôi khi trong quá trình transfer data, ta có thể cần phải điều chỉnh messages để biến đổi nó theo như format mà chúng ta muốn. Khi đó ta có thể dùng tính năng Single Message Transformations - SMTs để làm điều này trong config của Source connector hoặc Sink connector. SMTs bao gồm 1 số thao tác phổ biến:  Thêm field mới cho message.  Filter message dựa trên field.  Rename field.  Route message đến các topic khác nhau. Kafka connect Architecture: Gồm các keyword chính:  1) Worker : là công nhân thực thi các task để gửi msgs đến Kafka cluster. (còn việc read/write từ/đến source thì là của Source/SinkTask theo config của ta xong rồi mới chuyển cho worker thực thi nhé)Worker có 2 khả năng đặc biệt:* Fault tolerance: khi worker gặp sự cố sẽ phân task cho worker khác handle tiếp. * Self-managed: khi có worker mới các task sẽ được phân phối lại đảm bảo load balance. =&gt; Có 4 tính năng Reliability, High avalability, Scalability, Load balancing.  2) Connector : tạo API kết nối với các system 3) Task : ví dụ transfer từ table 1 sang table 2 là 1 taskOK, vậy việc của chúng ta bây giờ là:  Làm thế nào để phân chia input thành các phần có thể thực hiện đồng thời. Làm thế nào để tương tác với các external system. Debezium: Mysql có binlog dùng cho việc replication và recovery. File này chứa các thay đổi của database (trước khi ghi vào database) bao gồm cả thay đổi về dữ liệu table và cả schema của table. Sau đó các thay đổi này mới được ghi vào database rồi sẽ được commit. Debezium Mysql connector là một plugin trong Kafka giúp kết nối Mysql với Kafka. Connector này sẽ đọc sự thay đổi trong binlog và tạo ra các event INSERT/UPDATE/DELETE để đẩy lên Kafka. Mysql được cài đặt sẽ xóa data trong binlog sau 1 thời gian ngắn nên Debezium Mysql connector sẽ chụp nhanh snapshot của hiện trạng dữ liệu và đọc nó ngay tại thời điểm được chụp. Cách hoạt động của Debezium:  Schema history topic:Binlog sẽ vừa lưu thay đổi về row-level data vừa lưu DDL statement để biết được schema tại 1 thời điểm có thay đổi không và thay đổi ra sao để produce event lên kafka chính xác. Còn trên database của kafka topic, connector sẽ vừa lưu DDL statements vừa lưu vị trí có DDL statement trong binlog. Khi có sự cố crash phải restart lại connector, nó sẽ đọc trong binlog tại 1 thời điểm cụ thể x. Connector sẽ rebuild lại schema từ database history topic Kafka cho đến thời điểm x mà nó được start lại trong binlog.  Schema change topic:Bạn có thể cấu hình để Debezium Mysql connector produce các “schema change event” mô tả sự thay đổi của database. Message được connector gửi đến cho Kafka topic name được cấu hình trong connector với property ``database. server. name`` (https://debezium. io/documentation/reference/1. 8/connectors/mysql. html#mysql-property-database-server-name). Message này chứa payload và có thể tùy chọn để chứa luôn schema của change event message. Phần payload gồm: ddl Provides the SQL CREATE, ALTER, or DROP statement that results in the schema change. databaseName The name of the database to which the DDL statements are applied. The value of databaseName serves as the message key. pos The position in the binlog where the statements appear. tableChanges A structured representation of the entire table schema after the schema change. The tableChanges field contains an array that includes entries for each column of the table. Because the structured representation presents data in JSON or Avro format, consumers can easily read messages without first processing them through a DDL parser. Thực hành:    Debezium: https://debezium. io/documentation/reference/1. 8/tutorial. html#viewing-create-event     Streaming from Mysql to Postgres và Elasticsearchhttps://debezium. io/blog/2018/01/17/streaming-to-elasticsearch/  Ref: https://debezium. io/documentation/reference/1. 8/connectors/mysql. htmlhttps://docs. confluent. io/kafka-connect-elasticsearch/current/configuration_options. htmlhttps://docs. confluent. io/kafka-connect-elasticsearch/current/overview. htmlhttps://baonq5. notion. site/Sample-connector-config-55b33df3898346fc93c94c7a697d44a6 https://en. wikipedia. org/wiki/Write-ahead_logginghttps://viblo. asia/p/010-apache-kafka-connect-concept-gAm5ymNL5dbhttps://en. wikipedia. org/wiki/ACID "
    }, {
    "id": 13,
    "url": "https://tuyen-nnt.github.io/elasticsearch/",
    "title": "Query Elasticsearch",
    "body": "2022/01/15 - Các cách query:  Dùng Query string  GET /ecommerce/product/_search?q=*hoặc GET /ecommerce/product/_search?q=pastahoặc có field name:GET /ecommerce/product/_search?q=name:pasta  Dùng Compound Queries (Query DSL)GET /&lt;index&gt;/&lt;category&gt;/_search{	 query : {		 bool : {			 must : [				{  match : {  name :  pasta  } }			]			 should : [			{  match : {  name :  spaghetti  } }			]		}	}}Giải thích về must và should: must means: The clause (query) must appear in matching documents. These clauses must match, like logical AND. should means: At least one of these clauses must match, like logical OR. Basically they are used like logical operators AND and OR. See this. Now in a bool query:must means: Clauses that must match for the document to be included. should means: If these clauses match, they increase the _score; otherwise, they have no effect. They are simply used to refine the relevance score for each document. Cre: https://stackoverflow. com/questions/28768277/elasticsearch-difference-between-must-and-should-bool-query   must: truy vấn bắt buộc phải xuất hiện trong kết quả trả về, và các truy vấn con trong must có mối quan hệ AND với nhau.     should: nếu truy vấn có trong kết quả thì score sẽ tăng thêm, score này được tính bằng 1 số phép toán học. Các filter con trong should có mỗi quan hệ OR với nhau.  =&gt; Vì must và should nằm trong bool nên nếu chỉ có should mà không có must hay must_not thì truy vấn trong should (nếu chỉ có 1 filter) bắt buộc xuất hiện trong kết quả. Cách hoạt động của query clause phụ thuộc vào nó là Query context hay Filter context.  Query context?Trả lời câu hỏi ““document này phù hợp với mệnh đề truy vấn nhiều như thế nào?”Query context sẽ có _score để đo mức độ phù hợp với truy vấn của document, tối đa là 1. 0.  _score được tính bởi thuật toán đặc biệt chúng ta sẽ chưa tìm hiểu ở đây. Query context là khi mệnh đề truy vấn được truyền vào tham số “query” trong search API.  Filter context?Trả lời câu hỏi “document này có phù hợp với mệnh đề truy vấn không?”, CÓ hoặc KHÔNG. Filter context là khi truyền mệnh đề truy vấn vào tham số “filter” hay “must_not”.    match_all{}:Không cần query bên trong. Nó trả về tất cả document, tất cả đều có _score là 1. 0     match{ &lt;field_name&gt;  : { query  :  &lt;giá trị&gt; }}: trả về các document thỏa giá trị mà ta cung cấp. Đối với giá trị text, text sẽ được analyze khi tìm kiếm, kết quả sau analyze sẽ được xây dựng thành 1 boolean query. Ta có thể cấu hình thêm tham số operand (mặc định là or).  Ngoài ra còn rất nhiều các tham số khác sử dụng trong các trường hợp phức tạp, bạn có thể đọc thêm ở https://www. elastic. co/guide/en/elasticsearch/reference/7. 6/query-dsl-match-query. html  multi-match { query :&lt;giá trị&gt; ,  fields  : [ &lt;field_name1&gt;  ,  &lt;field_name2&gt; ]}Trả về các document mà các field thỏa giá trị query. Ở đây có thể dùng wildcards trong trường field để chỉ nhiều field. Ví dụ: *_name thì các field được tìm kiếm sẽ là first_name, last_name,…    mustMệnh đề trong must phải có trong kết quả, các mệnh đề con có mối quan hệ AND.     must-notMệnh đề trong must không được có trong kết quả, các query bên trong có mối quan hệ NOT và AND.     shouldKết quả trả về phải thỏa 1 trong các query của should, các query bên trong có mối quan hệ OR với nhau.     bool: là 1 compound query clause. Boolean query được dùng để kết hợp nhiều mệnh đề query khác nhau gồm must, must-not, should, filter.     filter: document trả về phải thỏa tất cả query bên trong filter, vì là filter context nên _score được bỏ qua và mệnh đề được xem xét để cache.     range { &lt;field_name&gt;  : {  gte  : &lt;giá trị&gt; ,  lt  : &lt;giá trị&gt; }}:Trả về các document có giá trị nằm trong range đề cập trong range query.     term { &lt;field_name&gt;  : { value  : &lt;giá trị chính xác&gt; }}Trả về các document có giá trị chính xác được đề cập trong term query. Lưu ý khi dùng term query đối với giá trị text, vì text sẽ được analyze và case sensitive.  "
    }, {
    "id": 14,
    "url": "https://tuyen-nnt.github.io/access-web-data/",
    "title": "Làm được gì từ web data?",
    "body": "2021/12/21 - Mở socket » connect đến đường dẫn và port » Tạo biến string cmd request GET, POST,… » encode string cmd thành dạng byte » gửi request đi. Nhận response về từ thế giới bên ngoài (ngầm hiểu là UTF-8 » decode response từ dạng byte thành dạng unicode.  UTF-8 là bảng mã tham chiếu có thể detect được bảng mã ASCII hay UTF-8 (xét từ ASCII trước) mà allocate động kích thước của ký tự trong khoảng từ 1-4 bytes. Được recommend để dùng cho việc encode data giữa 2 hệ thống khác nhau. Trong Python 3 thì tất cả kiểu dữ liệu string đều dùng bảng mã unicode. Còn Python 2 thì phân rõ 1 string bất kỳ đang được mã hóa thành kiểu dữ liệu là unicode hay string, còn byte và string thì giống nhau là kiểu string thôi. Beautiful soup (for HTML): Cài đặt vào chương trình python hoặc extract thư mục bs4. zip bỏ vào cùng folder chứa file code sử dụng thư viện này. from bs4 import BeautifulSoup read() : hàm đọc nguyên block data bao gồm cả \n soup = BeautifulSoup(html, 'html. parser') với html là data được read() từ url nhập vàosau đó sẽ được beautifulsoup parse (biến đổi) thành 1 soup object được clean sạch sẽ, xinh đẹp.  Sau đó, chúng ta sẽ parse ra list danh sách tất cả các thẻ &lt;a&gt; và đọc từng object trong list tags để get và in ra attribute href hoặc None. Summary:  Web scraping là lấy data từ web về, có 2 ngữ cảnh để diễn tả:     Là khi 1 chương trình hay đoan script giả vờ như là 1 browser thu thập data của web, extract, và tìm kiếm web khác để làm thu thập tiếp.    Web Crawling data là như 1 cỗ máy tìm kiếm đi thu thập rà data trên mạng từ nơi này đến nơi khác như spider (chú ý về pháp lý).        TCP/IP cung cấp pipes/sockets giữa các ứng dụng Người ta tạo ra protocol cho application để sử dụng các pipes này.  HTTP là 1 giao thức powerful đơn giản. =&gt; Với sự hỗ trợ cho HTML, HTTP và socket, Python đã giúp đơn giản hóa quá trình gửi và nhận dữ liệu từ web chỉ với vài dòng code khi dùng lib urllib. Data on Web: Việc trao đổi dữ liệu giữa 2 ứng dụng có network độc lập được support tốt khi sử dụng giao thức HTTP. Vậy tiếp theo ta phải thống nhất dữ liệu ở format nào để mà trao đổi giữa 2 ứng dụng (ví dụ 2 app xài 2 ngôn ngữ khác nhau chẳng hạn). Có 2 format phổ biến là XML và JSON.  XML: Dùng để share structure data giữa các system. Gồm các thành phần cơ bản sau: Các tag trong XML không quan trọng khoảng trắng như code, chỉ phân biệt khoảng trắng của text trong tag. Serialization/De-serialization: là quá trình chuyển đổi data từ 1 system ra cấu trúc chung và chuyển đổi ngược lại để gửi cho system khác. XML như 1 cái cây nhị phân. XML cũng có cấu trúc đường dẫn như thư mục a/b/c sẽ lấy được data bất kỳ. XML Schema: Như một cái hợp đồng giữa 2 ứng dụng trao đổi kết nối với nhau. Schema sẽ check cú pháp của XML thỏa các điều kiện đã đặt ra ban đầu. Điều này giúp 2 ứng dụng không bàn cãi về lỗi do XML hay do bản thân của ứng dụng khiến kết nối bị lỗi. (Ta phải xác định lỗi và chặn đứng ngay thời điểm trao đổi dữ liệu. ) =&gt; XML Schema được thiết lập để giải quyết những bất đồng giữa các ứng dụng hợp tác. Đuôi file là . XSD  XML Validation: là hành động lấy từ XML Document và XML Schema Contract và gửi đến XML Validator. Parsing XML: Có 2 xu hướng khi parsing đó là:    Hành trình đi từ đầu cây đến cuối cây.     Tạo ra list các cây rồi đi từ đầu cây đến cuối cây.  =&gt; XML schema rất quan trọng, đặc biệt khi cây trở nên phức tạp và có ai đó chỉnh sửa XML không theo cú pháp quy định làm ảnh hưởng đến code parsing. Khi đó Schema sẽ là thứ hỗ trợ tốt việc phân định nguyên nhân gây lỗi khi parsing. JSON: Là định dạng phổ biến nhất hiện tại vì nó đơn giản, ánh xạ trực tiếp vào cấu trúc nội bộ của các ngôn ngữ lập trình. Nó gần với các kiểu dữ liệu lập trình nên thân thiện và dễ sử dụng. JSON sẽ không bao giờ thay đổi mà chỉ có cái khác thay thế nó thôi. JSON là object literal notation in Javascript, được biểu diễn bằng kiểu dữ liệu “lists” và “dictionaries”.  API: Là specification để chỉ ra pattern của URL, cú pháp của dữ liệu mà ta dự định gửi đi và cú pháp dữ liệu ta muốn nhận về. Implementation của API là những người mà sử dụng API đó:  Nhà sản xuất API: là người giữ data và tạo ra API sau đó xuất khẩu nó và định nghĩa, đưa ra luật lệ.  Người dùng API: là người đọc document của nhà sản xuất, viết mã code và tuân thủ các quy tắc của nhà sản xuất đối với API đó. API sẽ trả về định dạng JSON, sau đó chúng ta sẽ parse nó để phục vụ mục đích riêng.  uh là handler, nó không thực sự kéo data về mà chỉ mới mở ra. data có kiểu dữ liệu là Python unicode string js là 1 Python dictionary (object) vì dùng hàm json. loads Securing API request: Nhà cung cấp API họ phải tốn nhiều nguồn lực để lưu trữ, nghiên cứu dữ liệu nên data được cung cấp bởi API thường rất có giá trị. Nhà cung cấp thường giới hạn truy cập hoặc yêu cầu có API key, hoặc charge phí. Họ sẽ thay đổi rule của API khi mọi thứ phát triển hơn. Authentification &amp; authorizationĐọc documentation của API và nó sẽ nói bạn sẽ cần làm gì. Dòng dưới đây execute phần authorization, twurl import từ file code tự viết twurl. py: Sau khi đọc data từ bên ngoài trở thành dạng string biểu diễn cho dictionary, để biết limit còn lại của việc pull data từ API của bên khác. Ta chỉ cần cast thành dict, đọc header và chỉ số x-rate-limit-remaining. OK, để setup authentication, ta cần tạo 1 file ở đây ví dụ là file hidden. py lấy trên trang web document API: Để làm điều này ta đã phải sử dụng giao thức OAuth, thực tế là chúng ta đang ký tên lên URL. Kết quả trả về sẽ là URL + chữ ký trên URL. import oauth là thư viện của Python Và đây là file main hoàn chỉnh: json. dumps là hàm convert 1 Python dictionary (object) thành json string "
    }, {
    "id": 15,
    "url": "https://tuyen-nnt.github.io/mongodb/",
    "title": "MongoDB cơ bản",
    "body": "2021/11/14 - Bắt đầu với MongoDB: Install MongoDB: sudo apt updatesudo apt install -y mongodbTạo folder mongodb/data/db &amp; mongodb/log để đảm bảo các dữ liệu được lưu vào file system. Navigate tới thư mục chạy chương trình: cd mongodb/bin (folder được tạo có sẵn sau khi install) Mount đường dẫn để cho phép chương trình run as service: mongod --directoryperdb --dbpath \home\tuyen\mongodb\data\db --logpath \home\tuyen\mongodb\log\mongo. log --logappend --rest --install Run as service in background: sudo systemctl start mongodhoặc service mongodb start Bắt đầu làm việc trên Mongo Shell gõ: mongo. Gõ cls trong shell nếu muốn clear màn hình console. Các câu lệnh thao tác với MongoDB:    List các database:show dbs     Tạo database mới và switch sang db đó: use &lt;tên db&gt;     Show db đang hoạt động : db     Cú pháp json của document (như 1 row trong Mysql):  {	first_name: John ,	last_name: Doe ,	memberships: [ mem1 ,  mem2 ] //array	address:{		   street: 4 main st ,		   city:  Boston   }	contacts:[		   {name:  Brad , relationship: friend }, 	 ]} lưu ý định dạng file json, phần tử street nằm dưới dấu {, phần tử {name nằm dưới dấu :  Tạo user admin:db. createUser({		 user: brad ,		 pwd: 1234 		 roles:[ readWrite ,  dbAdmin ]})   Tạo collections (như table trong relational DB) : db. createCollection('&lt;tên collection&gt;')     Show collections: show collections   Insert documents vào collection:  db. customers. insert({first_name: John , last_name: Doe }, {first_name: Steven , last_name: Smith , gender:  male });     Vì Mongo là NoSQL nên nó có thể add thêm các doc với fieldname tùy ý.       List ra các documents (hàm pretty() giúp dễ nhìn hơn): db. customers. find(). pretty();   Update document:Cách 1: ghi lại tất cả các field db. customers. update({first_name: John }, {first_name: John , last_name: Doe ,  gender : male }Cách 2: dùng $set để không cần lặp lại các field giữ nguyên. db. customers. update({first_name: John }, {$set:{gender: male }}); Tăng số cho 1 field dùng trong tính toán:Ví dụ tăng 5 tuổi cho John. ```db. customers. update({first_name:”John”}, {$set:{age:45}});db. customers. update({first_name:”John”}, {$inc:{age:5}}); * Xóa field name bất kỳ của 1 document:Ví dụ xóa trường age của John. db. customers. update({first_name:”John”}, {$unset:{age}}); * Update nếu không thỏa điều kiện thì insert doc mới dùng ``upsert``:db. customers. update({first_name:”Mary”}, {first_name:”Mary”, last_name:”Samson”},{upsert: true}); * Rename fieldname: db. customers. update({first_name:”Mary”}, {$rename:{“gender”: “sex”}}); * Remove document:db. customers. remove({first_name:”Mary”}) * Remove document limit 1:db. customers. remove({first_name:”Mary”}, {justOne: true}); * Query tìm kiếm:db. customers. find({first_name:”Mary”}); // show limit db. customers. find(). limit(4); //select doc có tuổi &gt; 40db. customers. find({age:{$gt:40}}) //select doc có name là Mary hoặc Johndb. customers. find({$or:[{first_name:”Mary”}, {first_name:”John”}]}) //select doc có field name là objectdb. customers. find({“address. city”:”Boston”}) //select doc có fieldname là array, chọn ra các doc có mem1 trong array membershipsdb. customers. find({memberships:”mem1”}) * Sort // ASC : 1db. customers. find(). sort({last_name:1); // DESC: -1db. customers. find(). sort({last_name:1); * Count documentdb. customers. find(). count(); // count có điều kiệndb. customers. find({gender:”male”}). count(); * Function:db. customers. find(). forEach(function(doc){print(“Customer Name”+ doc. first_name)});```  Customer Name: JohnCustomer Name: Mary "
    }, {
    "id": 16,
    "url": "https://tuyen-nnt.github.io/network/",
    "title": "Tìm hiểu Network",
    "body": "2021/10/18 - I. Mô hình mạng 5 tầng:  Tầng 1: PhysicalVí dụ như dây cáp mạng, hay các đầu nối, tín hiệu gửi để kết nối các máy tính với nhau. Ngoài ra tầng này còn mô tả cách tín hiệu được gửi qua các kết nối vật lý này.  Tầng 2: Data LinkDùng để diễn giải các tín hiệu của tầng 1 một cách chung chung để các thiết bị mạng có thể giao tiếp. Có nhiều giao thức ở tầng này nhưng phổ biến nhất là Ethernet, có nhiệm vụ nhận dữ liệu và truyền đến các nodes trên cùng 1 mạng hoặc link. Tầng này nhận dữ liệu trên 1 đường liên kết đơn.  Tầng 3: Network (or Internet) Là tầng cho phép các mạng khác nhau liên kết với nhau thông qua các thiết bị router (bộ định tuyến).  Tập hợp các mạng kết nối với nhau thông qua router gọi là Internetwork (mạng liên kết). Mạng liên kết phổ biến nhất là Internet :D. Tầng này chịu trách nhiệm nhận dữ liệu được phân phối từ 1 tập hợp các mạng. Giao thức phổ biến nhất ở tầng này là giao thức IP (Internet protocol). Một phần mềm mạng gồm Client và Server:  Client gửi request Server thì gửi về responseMột node có thể chạy nhiều chương trình client hoặc server cùng lúc. Kiểu như ở client (server host) của bạn cùng thao tác nhiều chương trình cùng lúc để request (như mail, trình duyệt web), thì server ngoài cũng có thể chạy nhiều chương trình để trả về response cho bạn.  Tầng 4: TransportTrong khi tầng Network cung cấp dữ liệu giữa 2 node riêng lẻ thì tầng Transport có nhiệm vụ phân loại ra chương trình client hay server nào có nhiệm vụ nhận dữ liệu đó. Giao thức phổ biến nhất ở tầng này chính là TCP (Transmission Control Protocol). Người ta hay gọi theo cụm là TCP IP.  Nhưng TCP IP ko phải là tên gọi của 1 giao thức, nó chỉ là tên gọi chung kết hợp của giao thức ở tầng 3 và tầng 4. Mỗi giao thức đều có công dụng khác nhau. Ngoài ra có 1 giao thức thuộc tầng 4 transport cũng sử dụng giao thức IP tầng 3 để truyền tải dữ liệu. Đó là giao thức UDP (User Datagram Protocol). Nhưng giao thức này không đảm bảo độ tin cậy bằng TCP, mình sẽ nói thêm ở mục sau. Tóm lại, các giao thức ở tầng này hầu hết có nhiệm vụ đảm bảo dữ liệu được truyền tải đến “đúng” các ứng dụng đang chạy trên các nút đó.  Tầng 5: ApplicationGiao thức phổ biến ở tầng này là HTTP (cho phép chúng ta duyệt web), SMTP (gửi/nhận email), etc. . Tóm tắt: Giải thích 1 cách dễ hiểu hơn::  Tầng 1: Chiếc xe tải Tầng 2: Mô tả cách xe tải đi từ giao lộ này đến giao lộ kia.  Tầng 3: Xác định đường để đi từ địa chỉ A đến địa chỉ B để giao hàng.  Tầng 4: Đảm bảo là tài xế biết cách gõ cửa nhà để biết đến đúng địa chỉ nhà rồi và thông báo ra nhận gói tin.  Tầng 5: Chính là nội dung của gói hàng (dữ liệu) !Có thể bạn chưa biết?:  Ngoài model 5 tầng còn nhiều model khác. Nổi tiếng nhất là OSI model (7 tầng), khác với loại 5 tầng mà chúng ta học ở chỗ tầng 5 được tách ra 3 tầng.  TCP/IP truyền thống chỉ có 4 tầng vì Tầng 1 và tầng 2 được gộp làm 1. Về cơ bản cái truyền thống 4 tầng và 5 tầng hiện nay không khác nhau là mấy.  Chúng ta có thể tìm hiểu thêm về OSI Model tại đây:```https://www. sans. org/reading-room/whitepapers/standards/osi-model-overview-543https://en. wikipedia. org/wiki/OSI_model ## II. Tầng 1 - Các thiết bị mạng cơ bản### CableDùng để kết nối các thiết bị với nhau và cho phép truyền dữ liệu thông qua nó. Giúp tạo kết nối mạng đơn giữa 1 điểm với 1 điểm. Có 2 loại:* Copper cable: Cat3, Cat5, Cat5e, Cat6,. . . ![](cooper-cable. png)	* Các cable đời mới hơn như Cat5e, Cat6 tốt hơn trong việc truyền dữ liệu nhiều hơn và chính xác hơn do chúng có thông số kỹ thuật nâng cao và lõi xoắn của chúng được cấu tạo khác đi để giảm sự xuyên âm (crosstalk- khi mà xung điện trên 1 dây được phát hiện trên 1 dây khác) giúp giảm tình trạng đầu nhận dữ liệu không hiểu được dữ liệu gây ra lỗi mạng. 	* Gửi giao tiếp dữ liệu nhị phân qua dây đồng bằng cách thay đổi điện áp giữa 2 phạm vi. Hệ thống ở đầu nhận dữ liệu sẽ dịch sự thay đổi điện áp này thành dữ liệu nhị phân 0-1. Sau đó từ dữ liệu 0-1 này sẽ được dịch sang các loại dữ liệu khác nhau. 	* Sử dụng áp điện để biểu diễn dữ liệu dưới dạng 0-1* Fiber optic	* Sử dụng xung ánh sáng để biểu diễn dữ liệu dưới dạng 0-1. 	* Được dùng ưu tiên cho các môi trường có nhiều nhiễu điện từ. 	* Vận chuyển dữ liệu nhanh hơn, khoảng cách xa hơn mà khó làm mất dữ liệu tiềm ẩn hơn. 	* Đắt và dễ vỡ hơn. 		### Hub &amp; Switch Là những thiết bị giúp kết nối các máy tính trong cùng 1 mạng, thường gọi là mạng LAN (mạng cục bộ) #### Hub1 thiết bị bất kỳ trong mạng lưới của hub khi truyền dữ liệu tới hub sẽ kết nối đến tất cả các máy tính khác mà cùng được kết nối với hub. ![](/assets/images/hub. png) Các máy tính nhận dữ liệu có hệ thống riêng để xác định xem dữ liệu đó có đúng là dữ liệu mà nó có nhiệm vụ nhận hay không. Nếu không thì trả về không, nếu có thì thông báo nhận thành công. Cơ chế này gây ra nhiều tiếng ồn (noise) tạo ra cái gọi là collision domain. Collision domain:* Một phân đoạn mạng mà chỉ có 1 thiết bị có thể giao tiếp tại 1 thời điểm. ![](/assets/images/collision. png) * Nếu nhiều hệ thống (máy tính) cố gắng gởi dữ liệu cùng 1 lúc, các xung điện được gởi qua cáp (cable) có thể gây nhiễu lẫn nhau. Khiến cho các hệ thống phải chờ đợi một khoảng thời gian yên tĩnh trước khi cố gắng gửi dữ liệu lần nữa. &gt; Điều này làm chậm truyền thông mạng nên hiện nay Hub không còn được dùng phổ biến. ### SwitchMô hình hoạt động tương đối giống Hub, khác biệt ở chỗ:* Hub ở layer 1, Switch ở layer 2 (Data Link device) * Do đó Switch có thể kiểm tra dữ liệu của giao thức Ethernet được gửi đi xung quanh network. =&gt; Switch có thể xác định hệ thống (máy tính) nào thuộc về dữ liệu đó mà chỉ gửi thẳng đến hệ thống đó thôi. ![](/assets/images/switch. png)&gt; Điều này giúp loại bỏ hoàn toàn kích thước của collision domain trong network. Gíup giảm sự truyền tải lại và tăng thông lượng tổng thể. ### RouterLà thiết bị biết các forward dữ liệu giữa các mạng độc lập với nhau. ![](/assets/images/router. png)* Hoạt động ở lớp thứ 3 (Network)* Cũng giống như switch, router có thể kiểm tra dữ liệu IP để xác định địa chỉ cần gửi dữ liệu đến. * Router chứa 1 số bảng nội bộ chứa các thông tin về định tuyến traffic (lưu lượng truy cập) giữa nhiều mạng khác nhau trên thế giới. Phổ biến là các router gia đình hoặc văn phòng nhỏ. Mục đích của các bộ định tuyến nhỏ này chỉ để lấy lưu lượng truy cập có nguồn gốc từ trong nhà hoặc văn phòng nhỏ sử dụng LAN và chuyển tiếp nó đến ISP (nhà cung cấp dịch vụ Internet)![](/assets/images/router-isp. png)* ISP là một loại router phức tạp hơn nhiều, nó sẽ tiếp quản traffic từ router nhỏ. Router này gọi là router lõi, tạo thành xương sống của Internet, chịu trách nhiệm về cách gửi và nhận dữ liệu khắp thế giới mỗi ngày. * ISP tiếp nhận rất nhiều traffic và phải xử lý việc quyết định nơi nào là nơi gửi traffic này đến. Core router này thường có nhiều kết nối đến nhiều core router khác bằng giao thức BGP (Border Gateway Protocol) giúp chúng tìm hiểu xem đường nào là tối ưu nhất để chuyển lưu lượng truy cập đến. &gt; Khi bạn mở trình duyệt truy cập đến 1 web server bất kỳ, traffic giữa máy tính và web server có thể đã đi qua hàng chục router khác nhau. Các Router là những hướng dẫn viên toàn cầu giúp đưa traffic đến đúng nơi. ## Server và ClientNodes: là từ dùng để gọi chung cho các thiết bị trong network ở trên, có thể là máy tính, server, client hay router,. . . Server: là node có nhiệm vụ cung cấp data cho client, còn được dùng để chỉ mục đích chính của node trên mạng. Client: là node request data. Đôi khi 1 node vừa làm server vừa là client. Ví dụ như Email server, vừa cung cấp data về email cho client, mà vừa là 1 client gửi yêu cầu đến DNS server và được DNS trả data về. =&gt; Hầu hết các thiết bị không hoàn toàn là 1 server hay 1 client, mà đảm nhận 1 trong 2 vị trí trên ở 1 thời điểm. ![](/assets/images/server-client. png)## III. Tầng 2 - Data Link Layer### Ethernet * Là protocol mạng phổ biến nhất ở tầng 2 để gửi dữ liệu qua các liên kết riêng lẻ. (còn Wi-Fi thì dạng khác)* Là phương tiện giúp trừu tượng hóa tầng 1 (vật lý hay phần cứng) để các tầng khác có thể dễ tiếp cận sử dụng. * Chịu trách nhiệm về các thông tin kết nối, liên kết thiết bị để các tầng trên dựa vào. Vì vậy, các tầng trên không cần quan tâm thiết bị được kết nối ra sau, mà chỉ quan tâm tầng dưới gửi dữ liệu gì để xử lý mà thôi. * Ethernet có 1 kỹ thuật là CSMA/CD (Carrier-sense multiple access with collision detection) giúp nhận biết  collision domain  sóng mang xung đột. * CSMA/CD được dùng để quyết định khi nào kênh giao tiếp đang rãnh rỗi trên phân đoạn mạng (network segment) và khi nào 1 thiết bị đang rãnh để truyền dữ liệu. &gt; Cách hoạt động khá đơn giản, khi có xung đột thì lập tức dừng truyền dữ liệu của các thiết bị gặp xung đột và chờ 1 khoảng thời gian ngẫu nhiên giúp tránh vụ va chạm (random interval trước khi thử gửi lại, và không đồng thời). * Phân đoạn mạng của tất cả các thiết bị được kết nối sẽ có tất cả thông tin, dữ liệu đang trao đổi. Do đó, ta cần xác định thiết bị nào là nơi dữ liệu cần đến bằng MAC Address. ### MAC Addresses (Media Access Control Address)* Là địa chỉ để nhận diện toàn cầu được đính kèm với một network interface cá nhân. * Giúp nhận dạng các máy tính khác nhau. * Là một con số **48-bit** thường được biểu diễn bởi 6 nhóm với 2 con số hệ thập lục phân cho mỗi nhóm. * Nghĩa là sẽ có tối đa 2^48 MAC address. &gt; Hệ thập lục phân là cách biểu diễn các số sử dụng 16 ký tự. ![](/assets/images/hexa. png)* Octet: là 1 cách để tham chiếu số MAC. Trong mạng máy tính, bất kỳ số nào có thể biểu diễn bằng 8 bits được gọi là Octet. =&gt; 2 chữ số thập lục phân có thể biểu diễn các số tương tự các số được biểu diễn bằng 8 bits. * 03 Octets đầu tiên của địa chỉ MAC được gọi là OUI (Organizationally unique identifier), được gán cho nhà sản xuất phần cứng riêng lẻ. =&gt; Từ 3 octets đầu có thể giúp ta xác định được NSX. * 03 Octets sau được chỉ định theo ý nhà sản xuất mong muốn với điều kiện để mỗi thiết bị sản xuất ra có 1 MAC address duy nhất. ![](/assets/images/mac-1. png)* Tóm lại, Ethernet protocol dùng MAC address để đảm bảo dữ liệu mà nó gửi có thông tin thiết bị nguồn và thiết bị đích gửi đến. Bằng cách này, dù ở trong một phân đoạn mạng hoạt động như thể có collision domain đơn lẻ, thì các thiết bị luôn biết được khi nào dữ liệu thuộc về nó. ### Unicast, Multicast &amp; Broadcast frame#### UnicastLà sự truyền tải data đến 1 địa chỉ đích duy nhất. Được nhận diện bằng cách nhìn vào bit ít quan trọng nhất trong octet đầu tiên của địa chỉ MAC đích trong Ethernet frame. * Nếu là 0 : unicast* Nếu là 1 : multicast #### MulticastLà sự truyền tải data đến nhiều địa chỉ MAC đích. Các thiết bị mạng (network interface) có thể được cấu hình để chấp nhận 1 danh sách địa chỉ MAC được nhận data &amp; giao tiếp trong network cục bộ. #### BroadcastEthernet broadcast address : FF:FF:FF:FF:FF:FFBroadcast được sử dụng để tất cả các thiết bị có thể hiểu lẫn nhau. Người gửi sẽ gửi tới tất cả các thiết bị trên mạng LAN. ### Ethernet Frame* Data-packet: là 1 concept chung để chỉ 1 tập hợp dữ liệu nhị phân được gửi thông qua network link. * Là 1 tập hợp (data-packet) các thông tin mang tính tổ chức cao, biểu diễn theo 1 thứ tự cụ thể. * Bằng Ethernet frame, network interface có tại tầng 1 có thể chuyển đổi chuỗi bit qua một liên kết (data link) để cho ra 1 data có ý nghĩa (decode). ![](/assets/images/line-coding. png)* Các phần trong frame là bắt buộc và có độ dài cố định. ![](/assets/images/ethernet-frame. png)Thứ tự và các thành phần của Ethernet Frame gồm:* Preamble (8 bytes): phần mở đầu, chia làm 7 bytes đệm + 1 byte SDF (dùng để làm dấu giới hạn khung bắt đầu, sau nó là khung dữ liệu thực tế). Phần này được các network interface dùng để đồng bộ hóa các đồng hồ nội bộ mà chúng sử dụng nhằm điều chỉnh speed tại nơi mà chúng gởi dữ liệu. * Destination MAC Address (48 bits/6 bytes): địa chỉ phần cứng đích, là bên nhận data. * Source Address (6 bytes): địa chỉ MAC nguồn. * VLAN header (4 bytes): nếu trong khung có 4 bytes này thì nghĩa là đây là VLAN frame. Nếu có VLAN, thì sau nó phải có EtherType field. &gt; VLAN (Virtual LAN) là kỹ thuật cho phép ta có nhiều LAN hoạt động trên cùng 1 thiết bị vật lý (switch,. . ). VLAN được sử dụng để tách biệt traffic truy cập khác nhau với các thiết bị cùng mạng kết nối. ![](/assets/images/vlan. png)* EtherType field (2 bytes): được dùng để diễn giải protocol (giao thức) của nội dung dữ liệu. * Payload (từ 46-1500 bytes): là data chính cần được vận chuyển, chứa data tầng 3,4,5. * FCS (4 bytes): viết tắt của Frame Check Sequence, là 1 con số có độ dài 32 bit, biểu diễn checksum value cho toàn bộ ethernet frame. 	* Checksum value: được tính bằng cách biểu diễn chu kỳ kiểm tra dự phòng (CRC) trên frame. 	* Cyclical redundancy check (CRC): là một khái niệm quan trọng đối với tính toàn vẹn dữ liệu (data integrity), và được dùng cho toàn bộ các tính toán học, không chỉ riêng truyền tải network. Đây là 1 phép toán học sử dụng phân chia đa thức để tạo ra 1 con số đại diện cho 1 tập dữ liệu lớn hơn và luôn kết thúc bằng 1 số checksum. =&gt; Mục đích của checksum là để kiểm tra xem data bên đầu nhận có bị hỏng không. Quy trình thực hiện:* Thiết bị sẵn sàng lên frame để gửi đi* Thiết bị bắt đầu thu thập đủ các thông tin cần có của 1 frame (trừ checksum)* Thực hiện CRC trên các thông tin đó và gắn số checksum thu được. * Đưa FCS vào frame hoàn chỉnh. * Đưa frame vào một data link dùng protocol Ethernet* Đầu thiết bị bên nhận thu thập các trường dữ liệu nhận được và tính toán lại checksum để so sánh với số checksum của frame gởi. Nếu không khớp nhau, nó sẽ vứt ra ngoài vì trong quá trình truyền đã khiến data bị hỏng. =&gt; Và nhiệm vụ của tầng trên sẽ quyết định xem data đó có nên được truyền lại hay không. =&gt; Kết luận: bản thân Ethernet chỉ báo cáo cho tầng trên biết về tính toàn vẹn của dữ liệu, chứ không có nhiệm vụ phục hồi dữ liệu. Cụ thể hình ảnh cho quy trình:![](/assets/images/CRC. png)Sau đó đưa vào link và bên nhận nhận được, rồi bên nhận sẽ tính toán lại số checksum:![](/assets/images/checksum. png)## IV. Tầng 3 - Network### IP Address* Là một con số 32-bit được tạo thành từ 4 octet. Mỗi octet có độ dài 8-bit và biểu diễn dưới dạng số thập phân từ 0 đến 255. Ví dụ: 12. 34. 56. 78* Các tổ chức lớn thường sử dụng IP address để xác định máy nào hơn là sử dụng MAC address để truyền tải dữ liệu trong mạng. =&gt; IP có phân cấp rõ ràng và lưu trữ dữ liệu dễ dàng hơn. &gt; Ví dụ công ty IBM có octet đầu được cấp riêng là 9. Thì khi router từ sender gửi dữ liệu thấy số 9 ở octet đầu của địa chỉ IP nhận (IP là 9. 0. 0. 1 chẳng hạn), nó sẽ biết là cần gửi đến router của IBM, sau đó router của IBM sẽ tự xử lý phân phối dữ liệu đến các máy tính trong network của IBM thông qua các octet còn lại. * Lưu ý: IP address thuộc về networks, chứ không thuộc về thiết bị được kết nối với các network. &gt; Nghĩa là network (mạng) khác nhau sẽ có nhiệm vụ cấp phát IP khác nhau cho thiết bị của bạn bằng công nghệ DHCP khi bạn bật mạng của network đó. * DHCP hay còn gọi là Dynamic IP Address (địa chỉ IP động) được dùng để cấp phát IP động cho các clients. * Static IP Address thì thường được dùng để cấp phát IP tĩnh cho server và các thiết bị mạng (router,. . . )### IP Datagrams &amp; Encapsulation#### IP Datagram* Là data-packet (gói tin) ở tầng network theo giao thức IP. * 2 phần chính của IP datagram là header và payload. ![](/assets/images/ip-datagram. png)Ý nghĩa các trường trong datagram header:* Version (4-bit): version của Internet protocol. Ví dụ: IP version 4 thì Internet protocol là IPv4. * Header Length (4-bit): ghi số độ dài của header, thường là 20 bytes đối với IPv4. 20 byte cũng là độ dài tối thiểu của IP header. * Service Type (8-bit): chi tiết về chất lượng dịch vụ, hoặc QoS, các công nghệ áp dụng. * Total Length (16-bit): tổng chiều dài của IP datagram. =&gt; Kích thước tối đa là con số lớn nhất có thể biểu diễn bằng 16 bits là 65,535. =&gt; Nếu kích thước lớn hơn con số tối đa mà network hiện tại cho phép, IP layer sẽ chia làm các data-packet (gói tin/datagram) nhỏ hơn. * Identification (16-bit): là số 16 bit được dùng để group các messages lại với nhau. =&gt; Trường này giúp nhóm các gói tin bị tách ra lại với nhau thành 1 phần của đường truyền. * Flags (4-bit): chỉ ra rằng datagram này có được phép phân mảng hay không, hoặc đã được phân mảnh chưa. * Fragment Offset (12-bit): là quá trình phân mảnh datagram ra các datagram nhỏ hơn (nếu vượt quá giới hạn của network nó đang được truyền đến). * Time to Live - TTL (8-bit): là trường chỉ ra bao nhiêu bước nhảy để 1 datagram có thể đi đến IP đích trước khi bị vứt đi. Đến số 0 thì router biết được là nó ko cần forward data nữa. ![](/assets/images/TTL. png)Mục đích của bước này là để đảm bảo khi có cấu hình sai trong router gây ra vòng lặp vô tận như hình:![](/assets/images/TTL-loop. png)* Protocol (8-bit): thông tin về transport layer protocol được sử dụng ở gói tin này. (TCP, UDP,. . . )* Header checksum (16-bit): số checksum nội dung của toàn bộ IP datagram header. =&gt; Checksum phải được tính toán lại mỗi lần TTL chạm 1 bộ định tuyến (vì có IP thay đổi)* Source &amp; Destination IP Address (32-bit) : địa chỉ IP nguồn và IP đích. * Options: trường tùy chọn được dùng để thiết lặp các đặc tính đặc biệt cho datagram được dùng chủ yếu cho các mục đích testing. Trường này có độ dài biến đổi. * Padding: thường dùng khi có trường Option. Là chuỗi số 0 để đảm bảo kích thước của header đúng kích thước tổng. #### EncapsulationPhần data payload section của Ethernet frame sẽ là toàn bộ IP datagram. Quá trình gôm các data trong IP datagram lại với nhau thành frame gọi là encapsulation (tương tự payload của Ethernet frame). * Toàn bộ data của IP datagram được đóng gói lại làm payload cho tầng ngay dưới nó (tầng 2), đó gọi là quá trình encapsulation. Các tầng khác cũng hoạt động như thế. * Tầng sau đó sẽ dùng toàn bộ payload của tầng trước đó. ![](/assets/images/encapsulation. png)### IP Address ClassesLà cách xác định cách địa chỉ global IP phân chia không gian. * Class A: octet đầu là network ID, 3 octet sau là host ID. * Class B: 2 octet đầu là network ID, 2 octet sau là host ID. * Class C: 3 octet đầu là network ID, octet cuối là host ID. ![](/assets/images/ip-class. png)### Address Resolution Protocol (ARP)Là giao thức được dùng để khám phá địa chỉ MAC của một node với địa chỉ IP tương ứng. Các thiết bị mạng sẽ lưu giữ 1 bảng ARP local chứa thông tin địa chỉ MAC ứng với IP của các thiết bị mạng đã từng kết nối (trong 1 khoảng thời gian nhất định). Thiết bị truyền sẽ tìm đến địa chỉ đích bằng cách phát tín hiệu ``broadcast`` FF:FF:FF:FF:FF:FF cho tất cả các máy tính trong mạng local rằng tôi muốn tìm địa chỉ MAC của máy có địa chỉ IP này. Máy có địa chỉ IP đó sẽ lên tiếng và gửi ARP response chứa địa chỉ MAC lại cho sender (thiết bị truyền). Giờ thì thiết bị truyền có thể đưa MAC address vào Ethernet frame để truyền tải dữ liệu. ### Subnetting Là quá trình tách network lớn thành nhiều network nhỏ hơn gọi là các subnetworks hoặc subnets. ![](/assets/images/subnetting. png)#### Subnet Mask* IP: 9. 100. 100. 100* Subnet mask: 255. 255. 255. 224* Subnet mask in binary: 11111111 11111111 11111111 11100000=&gt; 9. 100. 100. 100/27&gt; Có 27 số 1 (các số 1 này là subnet ID). ![](/assets/images/subnet-id. png)Hình trên là ví dụ của subnet mask: 255. 255. 255. 0Subnet mask là cách để máy tính sử dụng toán tử AND để xác định địa chỉ IP đó có tồn tại trên cùng network hay không. #### CIDR (/ notation)Là một cách tiếp cận linh hoạt để cung cấp số lượng địa chỉ IP phù hợp với nhu cầu. CIDR cho phép network tự phân chia kích thước. ![](/assets/images/subnet-2. png)Nó dựa vào subnet để Demarcate (phân định). Demarcation point là cách để mô tả nơi 1 network hay system kết thúc và cái khác bắt đầu. Một IP thường có Network ID, Subnet ID, Host ID. Tuy nhiên với CIDR, Network ID và Subnet ID thường được kết hợp làm một. &gt; Nếu công ty cần nhiều hơn 1 class C số lượng địa chỉ IP max mà class C có thể cung cấp (254). Thay vì tạo 2 class C để gấp đôi thì giờ đây với CIDR, nó chỉ cần kết tnoosi không gian địa chỉ liên tục với nhau với mask /23 hay có thể nói là 255. 255. 254. 0. Giờ đây router chỉ cần biết 1 entry trong routing table để vận chuyển lưu lượng đến các địa chỉ đích thay vì 2. &gt; Trước đây network ID là tĩnh và chỉ có class A,B,C để phân định kích thước cố định và chỉ có subnet mới có thể thay đổi. Với CIDR thì mọi chuyện đã khác. ![](/assets/images/CIDR-23. png)Phân tích hình trên 1 chút:* Hãy nhớ là ta sẽ luôn mất 2 host ID trên mỗi mạng. (giá trị đầu và cuối trong range) * Nếu không dùng CIDR, ta cần 2 network class C nên = 254+254IDR* /23 nghĩa là thêm được 1 bit cho Host ID = 2^9### Routing (lộ trình) #### Concept* Định nghĩa Router: Là thiết bị mạng chuyển tiếp traffic phụ thuộc vào địa chỉ đích của traffic đó. Router thường có 2 network interface giúp nó có thể vận chuyển traffic giữa 2 network với nhau. ![](/assets/images/routing. png)Lộ trình ở hình trên sẽ được lặp lại cho đến khi traffic đến được địa chỉ đích. ![](/assets/images/routing-2. png)Diễn giải hình trên như sau:* Máy tính có IP 192. 168. 1. 100 (máy A) muốn gửi gói tin đến máy có IP 10. 0. 0. 10 (máy B)* Máy A nằm trong Network A và biết rằng máy B không nằm trong local Network A. * Máy A gửi gói tin này đến địa chỉ MAC của gateway của nó là Router A có IP gateway là 192. 168. 1. 1. * Router interface trên Network A nhận được gói tin vì nó thấy địa chỉ MAC đích trên Ethernet frame thuộc về nó. Sau đó, nó bỏ đi phần header của tầng Data-link để đưa đến tầng 3 phân tích tiếp. * Router sẽ trực tiếp kiểm tra header của IP datagram ở tầng 3. Nó tìm thấy IP đích chính là 10. 0. 0. 10 thuộc về mạng B (10. 0. 0. 0/24) trong Routing Table. Mạng B có interface kết nối với router này. * Tiếp theo Router sẽ tạo 1 gói tin mới để gửi cho Network B bằng cách sao chép dữ liệu IP datagram nhưng giảm trường TTL xuống 1 đơn vị + tính toán giá trị checksum mới. * Sau đó đóng gói IP datagram mới này bên trong 1 Ethernet frame mới với địa chỉ MAC đích là địa chỉ MAC của IP 10. 0. 0. 10 của network B mà nó lưu sẵn trong ARP table. * Và cuối cùng gói tin sẽ được gửi ra khỏi interface của router trên Network B để đến với máy B. ![](/assets/images/routing-3. png)* Máy A muốn gửi đến máy C phải thông qua router interface của máy A trên Network B và router interface của máy C trên Network B. * Cụ thể là máy A sau khi thấy IP đích không thuộc local network thì sẽ gửi đến gateway của nó là Router A thì router A này kiểm tra trong routing table thấy cách nhanh nhất là gửi gói tin đến router B để đến với máy C. * Sau đó router A sẽ -1 bước nhảy cho trường TTL và gửi cho router B thông qua router interface của router A trên network B và router interface của router B trên network B. * Do network của máy C được kết nối trực tiếp với router B thông qua router interface thuứ 2 của router B. Nên router B sẽ gửi thẳng gói tin đến máy C trên Network C và kết thúc lộ trình. #### Routing table* Cơ bản nhất sẽ có 4 cột. Trong đó có cột Network. Các cột còn lại có thể là IP, Subnet Mask hay CIDR. * Sẽ có từng hàng cho từng Network mà router này biết. * Khi nhận được gói tin, router sẽ biết IP đích đó thuộc network nào dựa vào routing table của nó. * Routing table sẽ luôn có 1 mục chung (catchall entry) cho bất kỳ địa chỉ IP nào không có network rõ ràng. * Next hop: là địa chỉ IP của router tiếp theo sẽ nhận dữ liệu* Total hops: tổng số bước nhảy còn lại được tính toán đường đi ngắn nhất (sử dụng thuật toán) để đưa gói tin đến đích. * Các routing table sẽ luôn được update thông tin mới về đường dẫn nhanh nhất đến mạng đích. #### Gateway Protocol Router sử dụng Routing Protocol để nói chuyện với các Router khác để chia sẻ thông tin với nhau, tìm ra con đường ngắn nhất để chia sẻ gói tin,. . . Routing protocol có 2 loại : * Interior Gateway Protocol* Exterior Gateway Protocol##### Interior Gateway ProtocolĐược router sử dụng để chia sẻ thông tin trong 1 hệ thống autonomous đơn (1 tổ chức điều hành mạng). Autonomous system là tập hợp các Network dưới sự quản lý của 1 nhà điều hành mạng duy nhất. &gt; Ví dụ như 1 tập đoàn cần routing traffic mạng của họ giữa các office của họ và mỗi office có thể có 1 local network. Hoặc ví dụ khác là 1 tổ chức nhà mạng lớn của quốc gia quản lý các router thuộc nhà mạng đó. ![](/assets/images/interior-protocol. png)Interior Gateway Protocol bao gồm 2 loại:* Distance-vector protocol (tiêu chuẩn cũ)Router dựa vào danh sách khoảng cách giữa các network trong routing table của router dưới dạng bao nhiêu hops, sau đó gửi danh sách này cho các router lân cận bằng Distance-vector protocol. Do đó các router lân cận có thể đoán được đường đi nào nhanh hơn. =&gt; Router sẽ được cập nhật routing table thường xuyên. ![](/assets/images/distance-protocol. png)* Link state routing protocol![](/assets/images/link-state. png)Giao thức này có cách tiếp cận tinh vi hơn. Từng router trong autonomous system sẽ broadcast thông tin của nó đến tất cả các router còn lại, và chúng sẽ có hết thông tin của nhau. Các router sẽ chạy các thuật toán phức tạp để tìm đường đi ngắn nhất đến địa chỉ đích. Do vậy cần bộ nhớ nhiều hơn và bộ xử lý tốn kém hơn. &gt; Khi phần cứng trở nên mạnh mẽ hơn đã khiến giao thức vector trước đó trở nên cũ và lạc hậu. ##### Exterior Gateway ProtocolĐược sử dụng bởi các Edge router của tổ chức cần chia sẻ thông tin với các tổ chức điều hành mạng khác. &gt; Đây là chìa khóa chính của Internet vì Internet là mạng lưới khổng lồ của các autonomous system. Ở mức cao nhất sẽ có một Core internet router nắm giữ thông tin của các autonomous system. Nhiệm vụ chính của nó là đưa dữ liệu vào Edge Router của các autonomous system. ![](/assets/images/core-router. png)* Internet Assigned Numbers Authority (IANA) : Là 1 tổ chức giúp quản lý những thứ như phân bổ IP address, ASN,. . . * ASN (Autonomous System Number): Là con số thập phân 32-bit định danh cho mỗi autonomous system. ASN không cần phải thay đổi để đại diện cho mỗi autonomous system. ASN chỉ thường được dùng bởi các core Internet router và được cập nhật thường xuyên trong routing table để biết mà điều phối traffic. * Người ta thường nhìn vào ASN để biết autonomous system thuộc về nhà quản trị mạng nào. Ví dụ AS19604 = IBM. ##### Non-routable Address Space Cho phép các thiết bị mạng giao tiếp với nhau mà không cần thông qua gateway router. * Network address translation (NAT): là loại công nghệ cho phép các không gian địa chỉ mạng không được định tuyến giao tiếp với các thiết bị khác trên internet. RFC 1918 (Request for comments) đã định nghĩa 3 dải IP không bao giờ được định tuyến trên Internet bên ngoài, mà chỉ có thể được định tuyến bởi các giao thức interior gateway protocol trong local network. Do vậy bất cứ ai cũng có thể sử dụng những IP này cho mạng nội bộ của họ. ![](/assets/images/rfc-1918. png)## V. Tầng 4 - Transport LayerSocket number hay còn gọi là Socket port: 10. 1. 1. 23:80* Nếu muốn request HTTP web page trên con server đang listen IP 10. 1. 1. 23 thì traffic sẽ được direct đến port 80 của con server với IP đó. * FTP (File transfer protocol) là giao thức cũ dùng để transfer file từ máy này sang máy khác, dùng port 21. ![](/assets/images/layer4. png)### TCP SegmentLà gói tin bao gồm phần TCP header và payload (data section). Và gói tin này là payload của IP datagram. ![](/assets/images/tcp-header. png)TCP header gồm 1 số phần quan trọng:* Sequence number (32-bit): số dùng để xác định vị trí của gói tin trong sequence gồm các TCP segment. &gt; Do tầng 3 có giới hạn size là 1,518 bytes nên cần phải tách các gói tin ra, TCP chia data thành nhiều segment để gửi cho tầng 3. Vậy nên số này dùng để xác định segment hiện tại là segment nào trong sequence của 1 gói tin lớn đã được tách ra. * Acknowledgment number (32-bit): là số tiếp theo sau sequence number. * Checksum: Khi gói tin đã nhận được ở đầu nhận, nó sẽ được tính toán checksum lại để so sánh với checksum ở header này. * Urgent pointer field: gắn liền với TCP control flag để chỉ ra segment nào quan trọng hơn các segment còn lại trong sequence. ### Control FlagsLà 1 số 6 bit, mặc định 000000 theo thứ tự như bên dưới:* URG (Urgent): nếu bit này là 1, chỉ rằng segment này là khẩn cấp và trường Urgent pointer field sẽ có nhiều thông tin hơn để lý giải cho flag này. * ACK (acknowledge): nếu là 1 nghĩa là cần kiểm tra trường acknowledge number trong TCP header* PSH (push): thiết bị gửi mong muốn thiết bị nhận push các data trong bộ nhớ đệm đến ứng dụng ở phía đầu cuối (vd web browser) càng sớm càng tốt. * RST (reset): 1 phía kết nối TCP chưa thể khôi phục lại hoàn toàn sau khi trải qua các segment bị mất thông tin hoặc gặp sự cố. &gt; Đây là cách mà các TCP khác trong sequence thông báo là  Từ từ tôi chưa thể tổng hợp thông tin từ bạn được * SYN (synchronize): gửi tín hiệu yêu cầu động bộ, được sử dụng cho lần đầu thiết lập TCP connection và đảm bảo đầu cuối biết để kiểm tra sequence number field. * FIN (finish): khi được set là 1 nghĩa là bên máy tính gửi không còn data nào để gửi và kết nối có thể được đóng lại. ### TCP connection#### The three-way handshake Sau khi three-way handshake hoàn tất, TCP connection sẽ được thiết lập. &gt; Handshake là cách để 2 thiết bị đảm bảo là nó đang nói chuyện trên cùng 1 protocol và có thể hiểu nhau. ![](/assets/images/3way-handshake. png)Giải thích:* Máy A gửi gói tin TCP segment gắn cờ SYN cho máy B. * Máy B phản hồi   Hãy thiết lập kết nối, tôi biết sequence num của bạn rồi . * Máy A phản hồi  Tôi nhận tin, hãy bắt đầu gửi data thôi . ![](/assets/images/2side-tcp. png)* Cả 2 máy đã được thiết lập TCP connection sẽ vận hành 2 chiều với nhau vì đã pair cùng nhau. Gói tin gửi theo chiều nào cũng được phản hồi =&gt; phía còn lại luôn biết đã nhận được gì. #### The four-way handshake Khi mà 1 trong các máy tính sẵn sàng ngắt kết nối thì nó sẽ gửi cờ FIN, máy tính nhận được sẽ gửi ACK. Nếu máy gửi ACK sẵn sàng ngắt thì cũng gửi nốt cờ FIN. Sau đó máy đòi ngắt sẽ gửi lại cờ ACK  ok tôi ngắt đây . ![](/assets/images/4way-handshake. png)### TCP Socket#### Định nghĩa:* Socket là khởi tạo (instantiation) của 1 end-point trong kết nối TCP tiềm năng. * Instantiation là việc triển khai thực tế ở 1 thứ gì đó được định nghĩa ở đâu đó. =&gt; TCP Socket cần có các chương trình thực để khởi tạo nó. =&gt; Bạn có thể gửi dữ liệu đến bất cứ port nào, nhưng bạn chỉ nhận được phản hồi từ server nếu chương trình mở socket ở port đó. Các trạng thái của TCP:* LISTEN : TCP socket đã sẵn sàng và chờ đợi các kết nối vào. (chỉ thấy trạng thái này ở server-side)* SYN_SENT: có 1 request đồng bộ đã được gửi đi nhưng kết nối chưa được tạo. (chỉ thấy trạng thái này ở client-side)* SYN-RECEIVED: 1 socket trước đó trong trạng thái LISTEN đã nhận được yêu cầu đồng bộ và đã gửi lại tín hiệu SYN/ACK cho client. * ESTABLISHED: kết nối TCP đang hoạt động và cả 2 phía đều thoải mái gửi dữ liệu cho nhau. (trạng thái đều có ở cả client và server). * FIN_WAIT: 1 tín hiệu FIN đã được gửi đi nhưng chưa nhận được ACK từ phía còn lại. * CLOSE_WAIT: kết nối đã được đóng tại tầng TCP nhưng ứng dụng dùng để mở socket này chưa release socket này. * CLOSED: kết nối đã chấm dứt và không còn giao tiếp với nhau được nữa. Ngoài ra còn có các trạng thái TCP khác. Thực tế trạng thái TCP còn phụ thuộc vào hệ điều hành vì nằm ngoài phạm vi của TCP. ### Connection-oriented protocol (vd TCP)Là giao thức thiết lập kết nối, sử dụng nó để đảm bảo tất cả các data được gửi đúng. Đảm bảo tất cả segment đều được ACK. TCP là loại này. TCP sẽ mong đợi từng ACK cho mỗi bit dữ liệu để đảm bảo data truyền tải chính xác và dựa vào sequence number để đưa các gói tin vào đúng thứ tự. Tầng Transport có protocol này là tầng sẽ quyết định data có cần phải gửi lại không. Còn các tầng 1,2 thì chỉ kiểm tra và vứt gói tin nếu không thỏa checksum. ![](/assets/images/tcp-flow. png)Do vậy, giao thức này sẽ tốn nhiều chi phí cho việc:* Tạo connection* Gửi liên tục các luồng ACK* Phá vỡ kết nối=&gt; Chỉ hữu ích khi bạn hoàn toàn muốn dữ liệu được truyền tải chính xác. ### Connectionless protocol (vd UDP - User Datagram protocol)UDP không phụ thuộc vào connection và cũng không hỗ trợ khái niệm ACK. Với UDP bạn chỉ cần thiết lập 1 cổng port rồi gửi gói tin đi là được. Điều này hữu ích với các message không quá quan trọng như streaming video. =&gt; Bằng việc loại bỏ chi phí khi sử dụng TCP, ta có thể tối ưu bằng cách tăng chất lượng video với UDP (giúp tiết kiệm băng thông để dành cho việc truyền data chất lượng cao thay vì dành cho việc thiết lập các kết nối TCP). ### Firewall Là thiết bị giúp ngăn chặn traffic truy cập thỏa tiêu chí nhất định để đảm bảo security. Firewall có thể hoạt động ở nhiều tầng lớp khác nhau, nhưng phổ biến nhất là ở tầng 4 - Transport. Ví dụ như chặn truy cập đến 1 số port nhất định trong khi cho phép traffic đến các port khác. ![](/assets/images/firewall. png)Như hình trên, firewall được đặt trước server để chặn truy cập bên ngoài vào port 445 (phục vụ service quản trị file nội bộ), và cho phép port 80 mở (phục vụ website của công ty) để bên ngoài mạng LAN có thể truy cập vào. Firewall đôi khi là thiết bị mạng độc lập, hoặc có thể xem nó là 1 chương trình có thể chạy ở bất cứ đâu, nó có thể tích hợp chung với router. Ngoài ra nó có thể chạy trên server cá nhân thay vì là 1 network device, các hệ điều hành lớn hiện nay đều được tích hợp sẵn firewall.  ## VI. Tầng 5 - Application layer ### Mô hình OSI (Open Systems Intersection) ![](/assets/images/OSI. png)* Session Layer: Tạo điều kiện giao tiếp giữa các actual application với tầng transport. Nó là 1 phần của hệ điều hành OS đã được un-encapsulation để lấy  payload  của application từ các tầng dưới và chuyển lên tầng Presention. * Presentation layer: có nhiệm vụ đảm bảo việc unencapsulation dữ liệu của tầng application layer có thể được hiểu bởi ứng dụng đang yêu cầu nhận dữ liệu. Đây là 1 phần của OS giúp xử lý mã hóa encryption hoặc nén dữ liệu. =&gt; Nhận thấy 3 tầng cuối không còn cơ chế encapsulate dữ liệu nữa nên được gộp lại thành mô hình 5 tầng và được xem là hiệu quả nhất. Tuy nhiên, việc hiểu rõ cơ chế hoạt động và mô hình nguyên thủy sẽ giúp ta có kiến thức cơ bản để hiểu về network tốt nhất. Dưới đây là mô phỏng quy trình network truyền tải dữ liệu từ máy 1 đến máy 2:![](/assets/images/all-layers. png)Máy 1 sẽ truyền data payload kèm sign SYN/ACK và máy 2 trả về SYN/ACK cứ thế cho đến khi cả 2 hoàn thành nhiệm vụ gửi nhận dữ liệu. -------------------------------#### DNSĐịa chỉ IP là 1 số binary 32-bit nhưng được viết dưới dạng thập phân để dễ đọc hơn. Vì não người nhớ số không tốt nên DNS ra đời để giúp giải quyết vấn đề truy cập địa chỉ IP dễ dàng hơn, con người chỉ cần nhớ domain bằng chữ. * DNS là dịch vụ mạng toàn cầu giúp giải mã chữ thành địa chỉ IP cho bạn. * Domain name là thuật ngữ ta dùng để chỉ những gì DNS có thể xử lý. Địa chỉ IP ứng với tên miền có thể thay đổi vì nhiều lý do như thay đổi hợp đồng, trụ sở data center. Bằng cách sử dụng DNS, quản trị viên có thể thay đổi IP ứng với tên miền, lúc này end user thậm chí sẽ không bao giờ biết là IP đã thay đổi. =&gt; Lợi ích của DNS:* Giúp dễ nhớ đường dẫn trang web thông qua Domain name hơn* Cho phép thay đổi quản trị ngầm mà không cần end user thay đổi hành vi. * DNS cho phép đối ứng domain name theo vùng. Ở vùng A thì có địa chỉ IP x ở vùng B thì địa chỉ y. Giúp lưu lượng truy cập máy chủ ở gần khu vực khi mà công ty bạn muốn đặt data center ở vùng nào đó để truyền dữ liệu gần nhau nhanh hơn. ![](/assets/images/dns. png) Để cấu hình 1 mạng tiêu chuẩn, có 4 thứ cần cấu hình để host vận hành trong mạng như mong muốn:* IP address* Subnet mask* Gateway for a host* DNS serverCó 5 loại DNS server:![](/assets/images/dns-server. png)* Caching and recursive name servers:Mục đích để lưu các tra cứu domain name trong 1 khoảng thời gian nhất định. Loại này thường được ISP hoặc local network cấp phát. Hầu hết các caching name server thường là recursive name server. * Recursive name servers: thực thi đầy đủ yêu cầu đối ứng tra cứu toàn bộ (full resolution) DNS. Hầu hết các local name server sẽ thực hiện nghĩa vụ của cả 2 loại, nhưng hoàn toàn có thể có trường hợp name server làm 1 trong 2 nhiệm vụ caching hoặc recursive thôi. ![](/assets/images/name-server. png)Khi Friend yêu cầu local name server cung cấp IP của fb. com thì recursive server sẽ thực thi full resolution tra cứu toàn bộ trên máy chủ dịch vụ DNS để trả về IP. Sau đó IP này sẽ được lưu trữ trong cache với trong khoảng thời gian TTL nhất định. Vài phút sau nếu You truy cập cùng fb. com thì local name server sẽ trả về máy tính của You địa chỉ IP tương tự mà không cần tra cứu toàn bộ. * TTL (Time to live): là giá trị bằng giây có thể được cấu hình bởi chủ của tên miền, là thời gian mà name server cho phép cache trước khi nó bị hủy và thực thi tra cứu lại DNS (full resolution). ##### Full recursive resolution:![](/assets/images/full-resolution. png)Name server đã cấu hình sẽ thực hiện tra cứu toàn bộ DNS theo quy trình sau:* B1: liên hệ với root name server. Chúng chịu trách nhiệm điều hướng truy vấn tên miền đến TLD server thích hợp. Ngày nay chúng thường phân bố trên toàn thế giới thông qua ``anycast``. Đây là 1 kỹ thuật được sử dụng để điều hướng truy cập đến các destination khác nhau dựa trên yếu tố địa lý, tắc nghẽn (congestion) hoặc link health.  13 root name servers được xem như 13 object được cấp quyền để cung cấp dịch vụ tra cứu tên router. Root server sẽ trả về địa chỉ của TLD name server (Top Level Domain), trong ví dụ là ``. com``. ![](/assets/images/TLD. png)* B2: TLD name server sẽ tra cứu máy chủ nào được cấp quyền cho trang web ví dụ domain ``weather. com``, đó là 1 tổ chức chạy trang web này. * B3: Sau đó tra cứu DNS sẽ tiếp tục được điều hướng đến máy chủ được cấp quyền domain name ``weather. com``, nơi sẽ cho ra IP thật của server cần truy vấn/cần request data. #### DNS over Protocol (TCP, UDP)##### TCPThường không dùng TCP, chỉ dùng khi gói tin lớn. Khi gói tin quá lớn UDP không chứa được hết thì DNS server sẽ phản hồi quá lớn. Lúc này DNS Client sẽ thiết lập TCP connection để tra cứu. ![](/assets/images/tcp-dns. png)##### UDPKhông cần tạo các kết nối như TCP. ![](/assets/images/udp-dns. png)UDP không có cơ chế khôi phục lỗi. Nhưng đơn giản nếu khi phát sinh lỗi, DNS chỉ cần request lại nếu không nhận được phản hồi, không cần phải khôi phục. ##### Resource Record Type DNS hoạt động dựa trên 1 tập hợp các record. Có nhiều loại record phục vụ các mục đích khác nhau:* ``A record``: được dùng để trỏ 1 tên miền đến 1 địa chỉ IPv4 nhất định. Cơ bản thì 1 domain name sẽ trỏ đến 1 bản ghi chứa địa chỉ IPv4, nhưng thực ra 1 domain cũng có thể trỏ đến nhiều bản ghi A. =&gt; Kỹ thuật ``DNS Round Robin``được sử dụng để cân bằng lưu lượng truy cập trên nhiều IP. Ví dụ www. microsoft. com có thể cấu hình 4 A records ứng với 4 name servers có thẩm quyền cho tên miền này. Sau đó từng lượt truy cập sẽ được điều hướng đến IP của name server lần lượt theo cơ chế round robin (thay đổi thứ tự đầu tiên) giúp giảm traffic. Máy tính phân giải DNS biết cả 4 record nhưng nó chỉ dùng record thứ tự đầu tiên, trường hợp kết nối thất bại nó sẽ kết nối tới record tiếp theo. * ``AAAA-Quad A``: tương tự A record nhưng trả về địa chỉ IPv6 thay vì IPv4. * ``CNAME record``: được sử dụng để chuyển hướng traffic từ 1 domain sang 1 domain khác. Vd: nếu bấm microsoft. com thì CNAME được cấu hình trước đó sẽ phân giải thành www. microsoft. com và DNS server sẽ phân giải tiếp ra IP của www. microsoft. comCNAME còn giúp thay đổi địa chỉ IP của 1 máy chủ ở 1 nơi thôi. =&gt; Có 2 cách để khi bấm microsoft. com và www. microsoft. com đều đến cùng 1 nơi:	* C1: Thiết lập record tương tự nhau cho cả 2 domain. Nếu thay đổi IP thì phải thay đổi cho cả 2 nơi của record. 	* C2: Setup CNAME trỏ microsoft. com đến www. microsoft. com, khi cần thay đổi IP chỉ cần thay đổi domain chính www. microsoft. com. * ``MX record - mail exchange``: đảm bảo traffic thư từ sẽ được gửi đến mail server của công ty. * ``SRV record``: giống MX nhưng khác chỗ là nó có thể trả về các record dịch vụ khác nhau như calendar. * ``TXT record``: giúp ta có thể chèn các note vào trong message gửi đi hoặc cấu hình network. TXT record thường được sử dụng để truyền tải thông tin bổ sung đến email dưới dạng NCC dịch vụ ví dụ  Mail Delivery Failure ,. . . * Ngoài ra còn các bản ghi NS hoặc SOA được sử dụng để xác định thông tin thẩm quyền về các DNS Zone. #### Anatomy of a Domain name* ICANN là tổ chức tiền thân của IANA, cả hai cùng phối hợp để quản lý việc cấp phát và điều phối IP + DNS + TLD của Internet toàn cầu. www. google. com Trong đó chia làm các vùng sau:* ``. com``: phần TLD* ``google``: phần Domain * ``www``: phần sub-domain =&gt; Tất cả là Fully qulified Domain name (FQDN)Registrar là các công ty có thỏa thuận với ICANN để cấp phát/bán domain name chưa đăng ký cho bất kỳ ai. * Mỗi vùng domain phải &lt;=63 ký tự. * FQDN thì tối đa 255 ký tự. * DNS support tối đa 127 level domain trong 1 FQDN. ##### DNS ZonesRa đời để cho phép kiểm sóat nhiều level của domain name một cách dễ dàng hơn. ![](/assets/images/dns-zone. png)Như hình ta sẽ có 4 cấu hình DNS server sẽ được thiết lập. 1 cho domain chính và 3 cái còn lại cho subdomains. Chúng sẽ được cấu hình thông qua ``Zone files``. * ``Zone files`` là những tệp tin cấu hình đơn giản khai báo tất cả các record của 1 zone nào đó. Zone file phải bao gồm SOA (Start of Authority). * SOA sẽ khai báo zone và tên của name server được cấp quyền để khai báo zone. * NS record cũng có thể xuất hiện trong file để khai báo những name server khác có thể chịu trách nhiệm cho zone này. * Ngoài ra cũng có thể xuất hiện các record A, AAAA, CNAME trong zone file. Ngoài việc tra cứu từ domain sang IP trong record. Ta còn sẽ bắt gặp kỹ thuật Resolve lookup zone file cho phép DNS server yêu cầu IP và trả về FQDN cùng với nó, dùng PTR record. * ``PTR`` (Pointer resource record) : phân giải IP thành name ### DHCPĐể cấu hình 1 mạng tiêu chuẩn cần có IP address, Subnet mask, Gateway, Name server. Trong đó chỉ có IP là cái khác nhau cho mỗi node, còn lại thì hầu như cố định cho các node trong 1 mạng. =&gt; DHCP ra đời để giúp cấu hình network cho OS máy bạn, giúp phân bổ IP* DHCP là protocol ở tầng application giúp tự động hóa việc cấu hình mạng cho các host trong network. Một node trong mạng sẽ truy cập DHCP server để truy vấn IP và sẽ nhận được toàn bộ cấu hình chuẩn gồm 4 cái trên. Các máy client được assign bất kỳ trong 1 dải quy định. Các thiết bị khác như router hay DNS server,. . . cần IP tĩnh để dễ diagnose khi gặp sự cố. #### DHCP discovery Là quá trình client cấu hình sử dụng DHCP để có được thông tin cấu hình network. #### Quy trình DHCP cấp phát IP * Client gửi thông báo broadcast tìm kiếm DHCP (gọi là DHCP discover message)![](/assets/images/DHCPdiscover. png)* DHCP server sẽ kiểm tra cấu hình của client để quyết định IP nào sẽ cấp. Tùy thuộc vào static hay dynamic hay fixed. * DHCP server sẽ phản hồi broadcast bằng DHCPOFFER message và trong msg có địa chỉ MAC của destination address. ![](/assets/images/DHCPoffer. png)* Client có thể từ chối cấp phát IP từ 1 máy chủ DHCP bất kỳ trong mạng. Client sẽ phản hồi broadcast bằng DHCPREQUEST nói rằng ok đến DHCP server. Vì chưa có IP nên client sẽ mặc định gửi từ 0. 0. 0. 0. ![](/assets/images/DHCPrequest. png)* Sau đó DHCP server sẽ phản hồi broadcastr xác nhận bằng DHCPACK. ![](/assets/images/DHCPack. png)* Client nhận message và bắt đầu thiết lập cấu hình network cho OS máy tính theo message nhận được. #### DHCP lease Là hạn thời gian được sử dụng cấu hình cấp bởi DHCP. Hết thời gian này thì client phải thực hiện lại quy trình trên. Ngoài ra, client cũng có thể setup thời gian lease của nó để ngắt kết nối network đến DHCP server thông qua cách tắt mạng tự động. Điều này sẽ trả địa chỉ IP của client về danh sách IP có thể cấp phát của DHCP. ### NAT (Network Address Translation)Là kỹ thuật cho phép gateway thường là router hay firewall rewrite IP source của 1 IP datagram đi ra network ngoài trong khi vẫn lưu giữ lại IP ban đầu để rewrite lại sau khi nhận được response từ các service ở network ngoài. Router thông thường chỉ nhận IP diagram kiểm tra trừ TTL 1 đơn vị, tính lại checksum. Khi có NAT thì nó sẽ rewrite lại IP. ![](/assets/images/NAT. png)IP soure từ máy 1 khi hiển thị với máy 2 sẽ là IP gateway của router ở máy 2. &gt; NAT sẽ giúp bảo vệ, ẩn IP của máy 1 khi gói tin đến với máy 2 và sẽ viết lại IP chính xác của máy 1 khi router nhận được response từ máy 2. Về mặt security, không ai có thể thiết lập kết nối với máy tính của bạn nếu họ không biết địa chỉ IP. Đây gọi là ``IP masquerading`` (giả mạo IP) và là khái niệm bảo mật quan trọng. #### NAT và Transport layer##### Port preservationLà kỹ thuật mà source port được chọn bởi máy client sẽ giống với port dùng bởi router. ![](/assets/images/port-preservation. png)Outbound connection chọn ngẫu nhiên 1 ephomeny source port hoặc trong dải 49152 đến 65535. Lúc này router NAT chỉ cần lưu lại source port để trả response về đúng máy tính đã gửi msg đi. ##### Port forwardingLà kỹ thuật mà destination port cụ thể có thể được cấu hình để luôn được chuyển đến node cụ thể nào đó. ![](/assets/images/port-forwarding. png)Ví dụ trong network công ty có nhiều node (server) làm nhiều nhiệm vụ/service khác nhau. Các máy tính bên ngoài không cần biết IP của các service này là gì mà chỉ cần IP gateway của router kết nối đến network đó + destination port của service. Với port forwarding, traffic của 2 service có thể trả về cho cùng 1 IP mạng ngoài (gateway) và sẽ có cùng DNS name. Nhưng ngược lại, response có thể được trả về đến những internal server khác nhau nhờ các destination port khác nhau. &gt; Kỹ thuật này giúp giả mạo IP (IP masquerading) và giúp người dùng bên ngoài tương tác với nhiều dịch vụ chạy trong cùng 1 tổ chức. ##### NAT và Non-Routable Address SpaceKể từ khi IANA đảm nhận phân bổ IP trên toàn cầu với số lượng có sẵn là 4,2 tỷ IPV4 thì đến khoảng từ năm 2010, các RIRs bắt đầu cạn kiệt IPv4. *RIRs là 5 lãnh thổ được IANA phân chia quản trị IPv4.   AFRINIC, which serves the continent of Africa  ARIN, which serves the United States, Canada and parts of the Caribbean  APNIC, which is responsible for most of Asia, Australia and New Zealand and Pacific Island nations  LACNIC, which covers Central and South America and any parts of the Caribbean not covered by ARIN  RIPE, which serves Europe, Russia,the Middle East, and portions of Central Asia  &gt; IPv6 sẽ giúp giải quyết vấn đề cạn kiệt IPv4, tuy nhiên để biến IPv6 phổ biến toàn cầu cần có thêm thời gian. Trong thời gian này, chúng ta sẽ dùng NAT và Non-Routable Address Space làm giải pháp tạm thời. Điều này sẽ giúp cho hàng ngàn máy tính chỉ cần dùng 1 public IPv4 là IP gateway của router thông qua kỹ thuật NAT ở trên, giúp gửi nhận msg bình thường qua internet. ![](/assets/images/limitIPv4. png)Cho đến khi IPv6 được sử dụng phổ biến, ta phải dùng kỹ thuật này để giải quyết tạm thời vấn đề cạn kiệt IPv4. Tham khảo thêm tại: https://en. wikipedia. org/wiki/IPv4_address_exhaustion### VPN &amp; Proxies#### Virtual Private Network* Là công nghệ giúp mở rộng private hoặc local network ra các host mà không nằm trong network đó. * Là tunelling protocol. Thiết lập VPN connection là thiết lập VPN tunnel. ![](/assets/images/vpn. png)&gt; Phổ biến nhất là dùng cho nhân viên truy cập vào mạng công ty để xem/sử dụng resource tại nhà. Quy trình:* Nhân viên sử dụng VPN client để thiết lập tunnel đến mạng công ty. Nó giúp tạo 1 interface ảo để được cấp phát IP trong mạng công ty. * Lúc này client sẽ được cấp 1 IP trong mạng công ty. * Vậy là từ lúc này client có thể access resource của công ty bằng cách gửi message đi thông qua IP interface ảo này như thể đang kết nối mạng private của công ty. Hầu hết VPN hoạt động bằng cách sử dụng phần payload của Transport layer. ![](/assets/images/vpn2. png)* Phần payload của message gửi từ client sẽ được mang đến VPNs end point nơi mà tất cả các layer khác trừ network, transport và application bị bỏ đi. * Phần payload lúc này sẽ được mã hóa để lại VPN server. * Sau đó phần payload sẽ được encapsulated với data link layer đúng thông tin và gửi ra đâu đó như thể từ private network của công ty. ##### Two-factor authenticationLà kỹ thuật dùng nhiều hơn username và password để authenticate. Thông thường người ta dùng mã token được generate bởi người dùng thông qua 1 phần cứng hay phần mềm chuyên dụng nào đó. ![](/assets/images/vpn2. png)Ngoài ra, người ta thường dùng VPN để thiết lập ``tunnel`` giữa 2 network của 2 tòa nhà với nhau (site-to-site connectivity) thông qua việc kết nối router hoặc thiết bị VPN chuyên dụng trên 1 mạng đến 1 router trên mạng khác. &gt; Từ đó network của 2 toà nhà có thể truy cập resource của toà nhà còn lại mà ko bị chặn. Tóm lại, VPN là 1 công nghệ tạo tunnel mã hóa để cho phép remote network hoặc 1 máy tính hoạt động như thể nó được kết nối vật lý đến 1 network nào đó. #### Proxies Dịch vụ proxy là 1 server đại diện cho client để access 1 service khác. Nó ngồi giữa client và các servers khác. 1 số lợi ích của proxy:* Anonymity: ẩn danh* Security: bảo mật* Content filtering: lọc nội dung* Increased performance: tăng hiệu suấtGateway router là 1 ví dụ cho proxy. Proxy là khái niệm trừu tượng và nó hiện diện ở bất cứ layer nào. ##### Web proxy* Lúc trước dùng để caching data giúp truy xuất lại nhanh hơn vì web proxy là cái sẽ đại diện cho client để truy xuất data từ service. Thời nay ít được dùng vì các tổ chức có khả năng xử lý mạnh mẽ nên không cần nữa. * Hiện nay web proxy được sử dụng để ngăn chặn 1 ai đó truy cập vào các trang web. Ví dụ: công ty chặn nhân viên vào facebook trong giờ làm việc để tăng năng suất. ![](/assets/images/web-proxy. png)##### Reverse proxyLà 1 dịch vụ như là 1 server độc lập nhưng thực chất đại diện cho nhiều server đằng sau nó. Ví dụ: * Tất cả traffic sẽ được kết nối đến 1 máy chủ web server của Twitter (gọi là reverse proxy) sẽ có nhiều web server sống phía sau nó để giúp quản lý số lượng lớn các request. Proxy server sẽ phân phối đến các app server theo hình thức Round robin để đảm bảo load balancing như DNS. ![](/assets/images/reverse -proxy. png) * Một ứng dụng khác là để giải mã. Lúc này các application server chỉ cần phục vụ việc show web, còn lại proxy server lo. ### Một số định nghĩa khác cần nắm:* ``Traceroute`` và ``mtr``: là 1 tính năng cho phép thấy đường đi giữa 2 nodes, và cho thông tin về mỗi hop trên đường đi. Nó dùng kỹ thuật thông minh tại TTl field ở IP level. Đặt TTL đầu tiên là 1 và tăng dần giúp xác định lỗi nằm ở bước hop nào. ![](/assets/images/traceroute. png) Traceroute ở mỗi bước sẽ gửi 3 gói tin, số đầu mỗi dòng là số thứ tự hop.  ![](/assets/images/traceroute2. png) ``mtr`` cũng tương tự nhưng chạy thời gian thực và thấy rõ hơn các thay đổi, liên tục cập nhật data tổng hợp được về traceroute. * Netcat/Test-NetConnection: để xem tầng transport có đang hoạt động không. Cú pháp: ``nc host port``Cách hoạt động: câu lệnh sẽ cố thiết lập kết nối tới port X của webserver (gửi data từ tầng app đến listening service). 	* Nếu kết nối thành công: 1 con trỏ nháy đầu dòng xuất hiện đợi input. 	* Nếu kết nối thất bại: câu lệnh sẽ dừng. =&gt; Nếu muốn không cần input chỉ để biết trạng thái thì thêm flag ``-z `v`` nghĩa là Zero input/output mode và verbose. * Name resolution tool: ``nslookup &lt;webserver_optional&gt;`` để trả về record tra cứu lỗi. &gt;Hoặc có thể nhập nslookup rồi nhập tiếp trong env của nó. Ví dụ: webserverset type=MX (loại tài nguyên)set debug (thêm chi tiết)```    Public DNS server: là các name server được thiết lập đặc biệt để bất cứ ai cũng có thể sử dụng miễn phí. Ví dụ 8. 8. 8. 8 và 8. 8. 4. 4 của Google Lưu ý dùng của các công ty uy tín tránh bị attack. Nếu không phải debug thì nên dùng name server cung cấp bởi ISP của bạn.  DNS Registration &amp; expirationRegistrar là 1 tổ chức có nhiệm vụ giao domain name cho tổ chức hoặc cá nhân. Mua domain name:	* Đăng ký tài khoản trên trang của registrar	* Tìm tên miền muốn mua	* Đồng ý với số tiền và thời hạn sử dụng	* Thanh toán	* Ghi nhớ HSD mà gia hạn trước thời gian hết hạn. Ngoài ra ta có thể transfer domain giữa các registrar và cá nhân:	* Bên nhận generate 1 chuỗi ký tự độc nhất để gửi cho bên transfer. 	* Bên transfer sẽ ghi cùng chuỗi ký tự đó trên 1 text record được cấu hình trong DNS setting để xác nhận là họ sở hữu domain này và được phép transfer. 	* Hai bên chấp thuận trao đổi quyền sở hữu.  Host filesLà flat file chứa trên mỗi dòng, 1 địa chỉ network đi theo bởi host name. Nói cách khác nó cho phép máy tính cá nhân cho rằng 1 tên miền luôn trỏ vào 1 IP cụ thể. Ví dụ: 1. 2. 3. 4 webserver Interview sample: Tình huống: nhân viên nội bộ truy cập URL nội bộ của công ty gặp lỗi “Trang không hiển thị được”. Quy trình debug có thể tham khảo như sau:  Hỏi xem thông báo lỗi là gì.  Hỏi xem người khác có vào được ko.  Xin tên web để thử tra cứu trên máy mình.  Nói user thử truy cập trang web khác.  Hỏi user dùng os nào và hướng dẫn nhập lệnh ifconfig. Kết quả là 1 loạt các thông số sau:``` IP: là 1 địa chỉ duy nhất được cấp cho máy tính để giao tiếp trên Internet với các máy tính khác.  Default gateway: như 1 điểm truy cập được máy tính sử dụng để chia sẻ thông tin với 1 máy tính khác hoặc trên Internet, có thể xem như là 1 router.  DNS: là domain name system dịch domain name thành IP.    DHCP: giúp tự động cấp địa chỉ IP cho máy tính và gửi cấu hình mạng cho máy tính. ```=&gt; phát hiện IP range của network là 172. x. x. x nhưng IP của máy tính lại là 192. x. x. x.     Hỏi xem máy tính có dùng DHCP không? (nên dùng để tự động cấp phát)=&gt; Có thể do mình cấu hình IP tĩnh cho máy nên không truy cập được, cần vào DHCP settings để điều chỉnh cho gán tự động. =&gt; Cũng có thể DHCP cấu hình sai hoặc kết nối với mạng sai nên địa chỉ IP bị sai range.   Hỏi xem đang kết nối mạng dây hay không dây. Nếu laptop hay điện thoại thì không dây (wi-fi). =&gt; Kiểm tra wifi đang kết nối mạng nào, có phải mạng công ty không. =&gt; Lỗi do kết nối mạng ngoài đường không phải mạng công ty. Tóm lại, luôn tìm ra lỗi nằm ở đâu trước khi vào sửa nó. "
    }, {
    "id": 17,
    "url": "https://tuyen-nnt.github.io/airflow/",
    "title": "Setup Airflow cơ bản cần biết",
    "body": "2021/09/11 - Các bước set-up:https://airflow. apache. org/docs/apache-airflow/stable/start/docker. html  Lỗi khi run database migrations and create the first user account:docker-compose up airflow-inithttps://github. com/Mailu/Mailu/issues/853  Do docker-compose chưa update Kết quả: DB_HOST=postgresDB_PORT=5432DB: postgresql+psycopg2://airflow:***@postgres/airflow  Sau khi docker-compose up:Success. You can now start the database server using:pg_ctl -D /var/lib/postgresql/data -l logfile start Build custom image tại :https://airflow. apache. org/docs/docker-stack/build. html  Cách tạo và import package trong Python:Ví dụ trong link là import package airflow_operators. https://airflow. apache. org/docs/apache-airflow/stable/modules_management. html  Thêm thư mục đường dẫn vào sys. path:VD: PYTHONPATH=/home/arch/projects/airflow_operators python  Dùng Macros nhưng phải cài Jinja Templating trước:https://airflow. apache. org/docs/apache-airflow/stable/macros-ref. html  *Args và **kwargs:https://realpython. com/python-kwargs-and-args/ "
    }, {
    "id": 18,
    "url": "https://tuyen-nnt.github.io/data-analysis/",
    "title": "Tìm hiểu về Data Analysis",
    "body": "2021/09/03 - Bài viết này sẽ hướng dẫn các bước cơ bản:  Tìm data phù hợp rồi collect chúng Đọc data trong môi trường dev Chuẩn bị phân tích bằng cách cleaning và validation. 1. Reading data: Dùng pandas để đọc các format phổ biến như CSV, Excel, HDF5,… 2. Columns và Rows: ####df. shape  in ra (n, m) với n là số row và m là số column ####df. columns  in ra list các column ở định dạng string Dataframe giống như Dictionary khi variable name là key (tên cột) còn các giá trị trong row là values. Do đó bạn có thể select column dùng df[ key ]. Kết quả là 1 Series các value của cột đó. Dtype của nó là float64. **Lưu ý: NaN là là giá trị đặc biệt chỉ định value không hợp lệ hoặc bị thiếu. ** 3. Clean và Validate: Đây là bảng data chứa cân nặng của baby ta dùng trong các ví dụ sắp tới:  Đầu tiên ta rút trích mỗi cột mà ta muốn phân tích lưu vào 1 biến:pounds= df[ tên cột ] ounces= df[ tên cột ]  Ta xem có value gì xuất hiện trong cột mà ta muốn phân tích và mỗi value xuất hiện bao nhiêu lần:pounds. value_counts(). sort_index() Mặc định kết quả sort theo giá trị nào gặp nhiều nhất. Nên ta thêm sort_index() để nó sort theo giá trị. =&gt; Sau đó ta quay lại xem bảng data ban đầu để thấy sự hợp lý. Ta có thể kết luận data này đúng và chúng ta đang phân tích đúng. Series. Describe(): Ta cũng có thể dùng attribute describe để có bảng thống kê mean, phương sai, min và max rồi kết luận như trên. pounds. describe() Giải thích bảng trên:  50% : giá trị trung vị =7 mean: trung bình =8, do có chứa các giá trị đặc biệt ít gặp như quá cao hay quá thấp nên không có ý nghĩa lắm. Do vậy, trước khi đưa vào tính toán mean thực sự, ta phải thay thế những giá trị đặc biệt trên bằng NaN (thuộc thư viện numpy) để nó có nghĩa là data này bị mất đi giúp không ảnh hưởng đến số liệu phân tích chung của chúng ta. Series. Replace(): Tham số đầu tiên là list các giá trị ta muốn replace. Tham số thứ 2 là giá trị mà ta muốn được replace thành. Tham số thứ 3 tùy chọn là inplace=True, mặc định không đề cập thì là False. True nghĩa là thay thế series cũ, false là tạo mới series sau khi thay thế.   Trả về kiểu dữ liệu series. Nếu inplace=True thì không cần gán vào biến mới. Ta nhận thấy sau khi thay thế dữ liệu, mean() của series sẽ thay đổi. Arithmetic với Series: Tùy nhu cầu của bạn muốn tính toán hay combine giá trị của các cột với nhau. Ở ví dụ bài học này, ta sẽ cộng pounds và ounces lại với nhau. Đầu tiên ta phải đổi giá trị ounces thành pounds bằng cách chia 16 (cách đổi đơn vị cân nặng). Sau đó ta cộng lại. Kết quả trả về là 1 series là tổng giá trị của 2 series pounds và ounces. Đến đây ta có thể đưa kết luận giá trị trung bình của 1 đặc tính như cân nặng trong dataset bằng series. describe().  4. Filter và Visualize data: Histogram: Dùng để biểu thị tần suất xuất hiện của giá trị trong dataset. Để dùng biểu đồ này trong python, ta dùng thư viện matplotlib.     Tham số thứ 1 là series. Do histogram không nhận giá trị NaN nên chúng ta phải dùng hàm dropna() để loại bỏ nó trong series.     Tham số thứ 2 là bin. Số Bin nói với hist là nó muốn chia giá trị trên biểu đồ thành bao nhiêu interval (có thể hiểu là cột theo cân nặng đối với ví dụ) và đếm có bao nhiêu values trong dataset ứng với mỗi bin đó.  Ngoài ra còn các thông số tùy chọn khác xem thêm tại đây: https://matplotlib. org/stable/api/_as_gen/matplotlib. pyplot. hist. html Quan sát biểu đồ hình trên, ta có thể thấy tần suất baby có cân nặng nhẹ xuất hiện nhiều hơn.  Ta thấy điều này hợp lý vì trong dataset có chứa dữ liệu các em bé sinh non có số tuần mang thai &lt; 37 tuần. =&gt; Theo đó, ta tiếp tục rút trích dữ liệu của cột chứa số tuần của baby. Để xem những em bé nào sinh non, ta dùng Boolean Series. Boolean Series: Trả về series gồm các giá trị True hoặc False cho điều kiện mà ta áp dụng. Ta gán biểu thức gồm series của cột và điều kiện để trả về series chứa True và False: preterm = df[ tên cột tuần sinh ] &lt;37 Nếu ta tính tổng hay trung bình cho Boolean Series, python sẽ treat True=1 và False=0. =&gt; Do vậy kết quả của preterm. sum() là 3742. Đây là số lượng baby sinh non ứng với True. Và khi ta tính trung bình của Series, ta sẽ được tỉ lệ của True. Do đó preterm. mean() cho kết quả ~0. 39987. Nghĩ là khoảng 40% số baby có trong dataset này là baby sinh non. Filter: Ta có thể dùng Boolean Series để filter ra các Series giá trị của cột nào đó mà thỏa điều kiện mong muốn. Ví dụ để select ra các giá trị cân nặng trong Series birth_weight của các baby sinh non, ta dựa vào Boolean series preterm tương ứng các record True. Python sẽ đối chiếu cùng index với series chứa cân nặng của baby. Sau đó gán kết quả cho biến lưu series trả về chứa các giá trị cân nặng của em bé sinh non. Từ đó ta dễ dàng tính trung bình cân nặng của đối tượng này. preterm_weight = birth_weigth[preterm] Ngược lại, để tính toán trung bình cân nặng ta dùng dấu ~ trước Boolean series để lấy giá trị ngược lại là False. Kết quả:  Nhận xét: Trung bình cân nặng của em bé thường sẽ &gt; hơn em bé sinh non, và điều này hiển nhiên hợp lý. Nâng cao hơn, ta có thể sử dụng kết hợp &gt;2 boolean series để filter. Khi đó ta sẽ cần dùng đến Logical Operators AND hoặc OR. birth_weight[A &amp; B]	# both truebirth_weight[A | B]	# either or both trueResampling: Cuối cùng trước khi có thể trả lời câu hỏi cho set dữ liệu, ta cần thực hiện Resampling. Nói sơ về Sampling (lấy mẫu) trước. Sampling là quá trình chọn ra một tập con của một quần thể với mục tiêu đánh giá các tính chất của quần thể đó. Cách thức lấy mẫu phụ thuộc trực tiếp vào mục tiêu đánh giá của chúng ta, do đó sampling nằm gần ranh giới giữa việc quan sát khách quan và việc thực hiện các thực nghiệm mang tính chủ quan. Một số khía cạnh chúng ta cần cân nhắc khi lấy mẫu dữ liệu bao gồm:  Mục tiêu. Tính chất của quần thể mà chúng ta muốn khảo sát đánh giá.  Quần thể. Phạm vi khảo sát dựa trên lý thuyết.  Tiêu chí lựa chọn. Các nguyên tắc cho việc chấp nhận / loại bỏ các quan sát.  Kích thước mẫu. Số lượng các quan sát được thu nhận trong mẫu. =&gt; Resampling datacó ý nghĩa là cần khảo sát trên mẫu dữ liệu ta thu được nhiều lần để đánh giá độ chắc chắn cho các ước tính. Có hai phương pháp resampling thường được sử dụng là bootstrap và k-fold cross-validation:  Bootstrap. Các mẫu được lấy ra từ dataset một cách ngẫu nhiên, cho phép một mẫu được xuất hiện nhiều hơn một lần.  k-fold Cross-Validation. Dataset được chia thành k nhóm, mỗi nhóm sẽ được sử dụng để đánh giá 1 lần5. Probability mass functions (PMF): PMF Class: Ngoài histogram, ta có thể dùng PMF để quan sát tần số xuất hiện của từng giá trị trong dataset. PMF Class làm việc dựa trên Pandas Series và cung cấp 1 số functions không có trong Pandas. Tham số đầu tiên có thể là các loại sequence bất kỳ. pmf_educ = Pmf(educ, normalize=False)*educ ở đây là Series object  Kết quả trả về 1 PMF object với giá trị ở bên trái và count số lần xuất hiện trong tập dataset ở bên phải. Để lookup tần suất cho giá trị ở bên trái, ta chỉ cần dùng dấu ngoặc vuông:pmf_educ[12] Tuy nhiên thông thường khi cần dùng đến PMF thì thường ta muốn biết tỉ lệ xuất hiện của giá trị hơn là đếm Lúc này ta chỉ cần set tham số thứ 2 là normalize = True. Khi đó cột giá trị bên phải trả về tỉ lệ và tổng cột sẽ =1. Nếu muốn biết % ta chỉ cần *100 là được. Cách lookup cho 1 giá trị bất kỳ cũng tương tự trên.  PMF Bar chart: PMF có method riêng để hiển thị biểu đồ tần suất. Tùy ta muốn hiển thị tần suất theo tỉ lệ hay count thì ta dùng method lên biến lưu series ở bước trên.  So sánh Histogram và PMF: Tùy trường hợp nhưng trong ví dụ hình trên ta nhận xét:  PMF show tất cả unique value giúp ta thấy rõ chính xác peak của data ở đâu.  Histogram đặt giá trị theo bin nên làm mập mờ các chi tiết quan trọng, như việc ta không thấy peak nằm ở giá trị 12, 14 và 16. 6. Cumulative distribution functions (CDF): CDF Class: Ngoài PMF, còn các cách khác để thể hiện distribution đó là CDF. CDF là cách hay để visualize và so sánh distribution của giá trị. CDF cách hoạt động gần giống như PMF, khác nhau ở chỗ:  PMF cho ra tỉ lệ của 1 giá trị trong series từ 0 đến 1.  CDF cho ra tỉ lệ xuất hiện của các giá trị &lt;= giá trị đang tính toán từ 0 đến 1 (percentile). cdf_educ = Cdf(educ, normalize=False)Xem ví dụ sau: Vẽ biểu đồ plot dùng CDF: Ta chỉ cần dùng class Cdf() và input tham số là sequence của data mà ta muốn biểu thị tần suất. Ở hình dưới là “tuổi”: Đặc biệt, cdf có thể được sử dụng như 1 function với input là 1 giá trị cụ thể (biến số nguyên). q=51p=cdf(q)print(p) Kết quả cho ra 0. 66. Nghĩa là có 66% số người có tuổi &lt;=51. Inverse CDF: CDF là 1 function đảo ngược. Nghĩa là bạn có thể từ giá trị probability (tỉ lệ) mà tra ngược lại giá trị tuổi bằng cách dùng cdf. inverse(giá trị tỉ lệ) Từ hình trên có thể nói rằng, tuổi 30 là percentile thứ 25 của distribution này. (do p=0,25) Nói sơ thêm về percentile. Khoảng cách từ 25th đến 75th percentile gọi là interquartile range (IQR), nó giúp đo lường sự trải rộng của distribution. Do vậy IQR tương tự như phương sai (variance) hoặc độ lệch chuẩn (standard deviation). Vì IQR được tính toán dựa trên percentiles nên nó sẽ không bị loại đi những giá trị cực hoặc ngoài rìa (cách mà phương sai làm). Do đó IQR “mạnh mẽ” hơn phương sai, nghĩa là nó vẫn làm tốt công việc của nó dù có giá trị lỗi hoặc giá trị cực trong tập data. Giá trị cực (extreme value) là gì? Là những giá trị khi xuất hiện sẽ ảnh hưởng lớn đến sự thay đổi về xu hướng hội tụ (độ chụm, độ chính xác) kết quả tính toán chung của tập các số như các phép tính trung bình cộng, trung bình nhân,… 7. So sánh Distributions: Ta có thể dùng PMF hoặc CDF để plot rồi visualize và phân tích. Tuy nhiên với CDF ta sẽ có cái nhìn rõ ràng, không bị nhiễu và biểu đồ đường (line chart) trông sẽ mượt hơn nhiều. Ta ví dụ tập dataset là income của cư dân trước và sau 1995: Nếu dùng PMF : Kết quả chart: Nếu dùng CDF (khuyến khích) Kết quả chart: Nhận xét data:  Dưới 300000$ thì income hầu như không thay đổi trước và sau 1995. Đường màu cam lệch sang phải ở mốc income 100000-150000$ nghĩa là income sau 1995 của những người có thu nhập cao có xu hướng tăng lên. Changelog: Changelogs take you down the last mile A changelog can build on your automated version history by giving you an even more detailed record of your work. This is where data analysts record all the changes they make to the data. Here is another way of looking at it. Version histories record what was done in a data change for a project, but don’t tell us why. Changelogs are super useful for helping us understand the reasons changes have been made. Changelogs have no set format and you can even make your entries in a blank document. But if you are using a shared changelog, it is best to agree with other data analysts on the format of all your log entries. Typically, a changelog records this type of information: Data, file, formula, query, or any other component that changedDescription of what changedDate of the changePerson who made the changePerson who approved the change Version number Reason for the changeLet’s say you made a change to a formula in a spreadsheet because you observed it in another report and you wanted your data to match and be consistent. If you found out later that the report was actually using the wrong formula, an automated version history would help you undo the change. But if you also recorded the reason for the change in a changelog, you could go back to the creators of the report and let them know about the incorrect formula. If the change happened a while ago, you might not remember who to follow up with. Fortunately, your changelog would have that information ready for you! By following up, you would ensure data integrity outside your project. You would also be showing personal integrity as someone who can be trusted with data. That is the power of a changelog! Finally, a changelog is important for when lots of changes to a spreadsheet or query have been made. Imagine an analyst made four changes and the change they want to revert to is change #2. Instead of clicking the undo feature three times to undo change #2 (and losing changes #3 and #4), the analyst can undo just change #2 and keep all the other changes. Now, our example was for just 4 changes, but try to think about how important that changelog would be if there were hundreds of changes to keep track of. Version control system: What also happens IRL (in real life)Image of a woman writing something down. There are two speech bubbles floating near her A junior analyst probably only needs to know the above with one exception. If an analyst is making changes to an existing SQL query that is shared across the company, the company most likely uses what is called a version control system. An example might be a query that pulls daily revenue to build a dashboard for senior management. Here is how a version control system affects a change to a query: A company has official versions of important queries in their version control system. An analyst makes sure the most up-to-date version of the query is the one they will change. This is called syncing The analyst makes a change to the query. The analyst might ask someone to review this change. This is called a code review and can be informally or formally done. An informal review could be as simple as asking a senior analyst to take a look at the change. After a reviewer approves the change, the analyst submits the updated version of the query to a repository in the company's version control system. This is called a code commit. A best practice is to document exactly what the change was and why it was made in a comments area. Going back to our example of a query that pulls daily revenue, a comment might be: Updated revenue to include revenue coming from the new product, Calypso. Data cleaning: Data-cleaning verification: A checklist This reading will give you a checklist of common problems you can refer to when doing your data cleaning verification, no matter what tool you are using. When it comes to data cleaning verification, there is no one-size-fits-all approach or a single checklist that can be universally applied to all projects. Each project has its own organization and data requirements that lead to a unique list of things to run through for verification. Image of a clipboard, pencil and post-it notes Keep in mind, as you receive more data or a better understanding of the project goal(s), you might want to revisit some or all of these steps. Correct the most common problems Make sure you identified the most common problems and corrected them, including: Sources of errors: Did you use the right tools and functions to find the source of the errors in your dataset?Null data: Did you search for NULLs using conditional formatting and filters?Misspelled words: Did you locate all misspellings?Mistyped numbers: Did you double-check that your numeric data has been entered correctly?Extra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?Duplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function or DISTINCT in SQL?Mismatched data types: Did you check that numeric, date, and string data are typecast correctly?Messy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?Messy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?Misleading variable labels (columns): Did you name your columns meaningfully?Truncated data: Did you check for truncated or missing data that needs correction?Business Logic: Did you check that the data makes sense given your knowledge of the business? Review the goal of your project Once you have finished these data cleaning tasks, it is a good idea to review the goal of your project and confirm that your data is still aligned with that goal. This is a continuous process that you will do throughout your project– but here are three steps you can keep in mind while thinking about this: Confirm the business problem Confirm the goal of the projectVerify that data can solve the problem and is aligned to the goal After the change is submitted, everyone else in the company will be able to access and use this new query when they sync to the most up-to-date queries stored in the version control system. If the query has a problem or business needs change, the analyst can undo the change to the query using the version control system. The analyst can look at a chronological list of all changes made to the query and who made each change. Then, after finding their own change, the analyst can revert to the previous version. The query is back to what it was before the analyst made the change. And everyone at the company sees this reverted, original query, too. "
    }, {
    "id": 19,
    "url": "https://tuyen-nnt.github.io/http/",
    "title": "Tìm hiểu về giao thức HTTP",
    "body": "2021/09/02 - HTTP request: HTTP là giao thức (protocol) giúp browser (client) giao tiếp với web server (server). Khi bạn truy cập 1 trang web nghĩa là bạn đang gửi 1 HTTP request đến web server. Trong HTTP request sẽ chứa Header và Body. Port mặc định của các giao thức HTTP hiện nay trên các trang web:  HTTP: 80 HTTPS: 443Dòng bắt đầu của HTTP request (như 1 message) chứa 3 phần sau::  Version của protocol: HTTP/1. 1 HTTP Method như GET, POST, PUT,HEAD, OPTION, DELETE Path của request : thông thường là 1 URL và format của nó phụ thuộc vào HTTP method. Ví dụ:Dùng đường dẫn tuyệt đối, phổ biến và thường dùng với GET, POST, HEAD, OPTIONS:  POST / HTTP/1. 1 GET /background. png HTTP/1. 0 HEAD /test. html?query=alibaba HTTP/1. 1 OPTIONS /anypage. html HTTP/1. 0Dùng đường dẫn hoàn chỉnh khi cần kết nối với proxy thông qua method GET:  GET https://developer. mozilla. org/en-US/docs/Web/HTTP/Messages HTTP/1. 1Chỉ dùng authority form khi cần setup HTTP tunnel (domain name : port) bằng method CONNECT:  CONNECT developer. mozilla. org:80 HTTP/1. 1Dùng dấu '*' khi muốn đường dẫn đại diện cho toàn bộ máy chủ thông qua method OPTIONS:  OPTIONS * HTTP/1. 1Header chứa các thông tin của request và hầu hết ở dưới dạng key:value. : Header của request được chia thành vài group chính:  General header: apply cho toàn bộ message Response header: chỉ định thêm cho request bằng cách sửa đổi 1 số thông số Representation headers: như Content-Type để mô tả format của data gửi lên server và cho biết nếu data đó có apply encoding nào không (chỉ có phần này khi request có Body)Header key có thể được set tùy chỉnh valueTrong POST method, bạn tùy chỉnh theo mẫu sau: # Use case: Set the output type as JSON and json. dumps your output. # Set_default_headers in a parent class called RESTRequestHandler. If you want just one request that is returning JSON you can set the headers in the post call. class strest(tornado. web. RequestHandler):  def set_default_headers(self):    self. set_header( Content-Type , 'application/json')  def post(self):    value = self. get_argument('key')    cbtp = cbt. main(value)    r = json. dumps({'cbtp': cbtp})    self. write(r)Tham khảo thêm các key info trong header tại: https://developer. mozilla. org/en-US/docs/Web/HTTP/Headers Body: Không phải method nào cũng cần Body, hầu như GET, HEAD, DELETE, OPTIONS ít cần. Thông thường khi cần gửi request update thông tin gì đó lên server thì người ta dùng method POST request (chứa dữ liệu HTML form). Body chia thành 2 loại :  Single-resource body: chứa 1 loại file duy nhất, được định nghĩa bởi Content-Type và Content-Length (trong header).  Multiple-resource body : chứa multipart data, mỗi part chứa một số thông tin khác nhau. Các part được phân tác bởi dấu -- trong header ở phần Content-Type. Loại này thường được sử dụng với HTML form. Content-Type: multipart/form-data; boundary=aBoundaryString(other headers associated with the multipart document as a whole)--aBoundaryStringContent-Disposition: form-data; name= myFile ; filename= img. jpg Content-Type: image/jpeg(data)--aBoundaryStringContent-Disposition: form-data; name= myField (data)--aBoundaryString(more subparts)--aBoundaryString--HTTP Method:: GET: Dữ liệu request sẽ hiển thị trên URL nên không bảo mật. Phù hợp khi cần download về dữ liệu gì đó vì nó truy xuất nhanh khi dữ liệu không hoặc ít bị thay đổi. Dữ liệu của phương thức này gửi đi thì hiện trên thanh địa chỉ (URL) của trình duyệt. /test/demo_form. php?user=itplus&amp;password=admin Đặc điểm:  HTTP GET có thể được cache bởi trình duyệt HTTP GET có thể duy trì bởi lịch sử đó cũng là lý do mà người dùng có thê bookmark được.  HTTP GET không được sử dụng nếu trong form có các dữ liệu nhạy cảm như là password, tài khoản ngân hàng HTTP GET bị giới hạn số trường độ dài data gửi điPOST: Dữ liệu gửi đi (khi request) sẽ không bị hiển thị trên thanh URL vì nó đã được encode (mã hóa) nên độ bảo mật cao. Đặc điểm:  HTTP POST không cache bởi trình duyệt HTTP POST không thể duy trì bởi lịch sử đó cũng là lý do mà người dùng không thê bookmark HTTP POST được.  HTTP POST không giới hạn dữ liệu gửi điPhân biệt POST và GET: Điểm chung: là các HTTP method dùng để trao đổi dữ liệu giữa client và server. Điểm khác nhau:  POST: Bảo mật hơn GET vì dữ liệu được gửi GET: Dữ liệu được gửi tường minh, chúng ta có thể nhìn thấy trên URL, đây là lý do khiến nó không bảo mật so với POST.  GET thực thi nhanh hơn POST vì những dữ liệu gửi đi luôn được webbrowser cached lại.  Khi dùng phương thức POST thì server luôn thực thi và trả về kết quả cho client, còn phương thức GET ứng với cùng một yêu cầu đó webbrowser sẽ xem trong cached có kết quả tương ứng với yêu cầu đó không và trả về ngay không cần phải thực thi các yêu cầu đó ở phía server.  Đối với những dữ liệu luôn được thay đổi thì chúng ta nên sử dụng phương thức POST, còn dữ liệu ít thay đổi chúng ta dùng phương thức GET để truy xuất và xử lý nhanh hơn. HEAD: Trả về response là header của request. HTTP response: Dòng đầu phần response chứa 3 thông tin:  Protocol version Status code: mã trạng thái trả về để biết request thành công hay thất bại Status text: text để giải thích cho codeHeader của response được chia thành vài group chính:  General header: apply cho toàn bộ message Response header: chỉ định thêm cho request bằng cách sửa đổi 1 số thông số.  Representation headers: như Content-Type để mô tả format của data trong response message và cho biết nếu data có apply encoding nào không (chỉ có phần này khi request có Body). Body: Không phải response nào cũng có body khi mà response đã đáp ứng đủ request mà không cần payload gì thêm. VÍ dụ như các status code như 201 Created hoặc 204 No Content. Body response có thể chia thành 3 loại :  Single-resource body: chứa 1 loại file duy nhất, được định nghĩa bởi Content-Type và Content-Length (trong header).  Single-resource body: chứa 1 loại file duy nhất, không biết độ dài, được mã hóa bằng các khối với key Transfer-Encoding : chunked.  Multiple-resource body : chứa multipart section, mỗi section chứa một số thông tin khác nhau. Loại này ít gặp. HTTP Location: Trong phần này có liên quan đến Redirection Ref: https://developer. mozilla. org/en-US/docs/Web/HTTP/Messageshttps://developer. ibm. com/articles/what-is-curl-command/ "
    }, {
    "id": 20,
    "url": "https://tuyen-nnt.github.io/Jekyll-install/",
    "title": "Setup Jekyll themes",
    "body": "2021/08/10 - Fork Jekyll theme &amp; Play: Đây là theme blog mình đang sử dụng: https://github. com/tuyen-nnt/jekyll-theme-memoirs Các bước build Jekyll cho blog này: https://bootstrapstarter. com/jekyll-theme-memoirs/(Tìm hiểu thêm tại: https://jekyllrb. com/docs/)  B1: git clone https://github. com/wowthemesnet/jekyll-theme-memoirs. git B2: Tải Ruby https://www. ruby-lang. org/en/documentation/installation/ B3: cd vào thư mục theme rồi gem install bundler B4: bundle installKết quả build thành công:  B5: Sửa lại _config. yml theo blog của mình B6: bundle exec jekyll serve --watch B7: Xem web blog tại http://127. 0. 0. 1:4000/jekyll-theme-memoirs (nếu folder vẫn giữ tên cũ) B8: Thêm blogs định dạng . md vào folder _posts. Trước mỗi bài viết sẽ có ô YAML là định dạng chung, bạn chỉ cần điền vào thông tin của mình là được (nhưng vẫn giữ form nhé). Chi tiết xem link các bước thực hiện. Tìm hiểu về Bundler install: https://bundler. io/ Tìm hiểu về Gemfile: https://bundler. io/gemfile. html "
    }, {
    "id": 21,
    "url": "https://tuyen-nnt.github.io/python-intermediate/",
    "title": "Python - Part 2",
    "body": "2021/08/02 - 1. Dictionaries: Là tập hợp các cặp key:value khi cần kết nối dữ liệu với nhau như 1 table để tra cứu nhanh và có thể chỉ ra unique keys khi tra cứu, thay vì nối 2 list lại để lấy index rồi tra cứu value.    So sánh giữa list và dict:     Cú pháp tạo dict như sau:  my_dict = {	key:value,	key:value}    Access dict như sau:my_dict['key'] =&gt; cho ra value của key đó.     Thêm key:value vào dict:world[ sealand ] = 0. 25     Check xem dict đã được thêm key ở trên vào chưa: sealand in world =&gt; trả về True/False. Với ‘seadland’ là key và word là tên dict.     Cập nhật lại giá trị cho key sealand:world['sealand'] = 0. 28Vì key trong dict là unique nên Python hiểu là bạn muốn thay đổi giá trị chứ không phải tạo mới cặp key:value.     Xóa cặp key:value:del(world['sealand'])     Dict trong dict:Cũng như list có thể chứa list trong list. Xem ví dụ:   # Dictionary of dictionarieseurope = { 'spain': { 'capital':'madrid', 'population':46. 77 },     'france': { 'capital':'paris', 'population':66. 03 },     'germany': { 'capital':'berlin', 'population':80. 62 },     'norway': { 'capital':'oslo', 'population':5. 084 } }  Để access giá trị của dict, ta sẽ dùng dấu [] như trong array:  europe['spain']['population'] =&gt; Vậy để add thêm cặp key:dict vào trong dict trên thì làm thế nào? # Create sub-dictionary datadata = {  'capital':'rome',  'population':59. 83}# Add data to europe under key 'italy'europe['italy'] = dataTa chỉ cần tách ra tạo dict phụ trước và gán nó vào biến lưu object value. Sau đó ta thêm cặp key:value vào dict như bình thường. Lưu ý:  Keys không được trùng nhau trong 1 dict, vì nếu trùng, nó sẽ chỉ lấy giá trị cuối cùng.  Keys phải là immutable object (không đổi), còn list thì mutable nên list cũng không được là key trong dict. 2. Pandas: Tabular dataset trong Pythonrow = observations column = variable  Để làm việc với dạng data này thì cần cấu trúc dạng chữ nhật. =&gt; 2D Numpy array =&gt; Nhưng với các dữ liệu có nhiều thông tin với nhiều datatype khác nhau như str, float,… thì Numpy chưa hiệu quả. Vậy nên pandas package chính là solution và quen thuộc trong Data science.  Nó được build dựa trên Numpy.  Là tool ở cấp độ cao trong thao tác với dữ liệu.  Pandas lưu dữ liệu bảng trong object gọi là Dataframe. Cách tạo dataframe từ dictionary: import pandas as pdbrics = pd. DataFrame(dict) Để tạo index cho observations trong df, ta dùng attribute index và gán 1 list với thứ tự chính xác các index mong muốn: brics. index = [. . . ,. . . ] CSV fileNhưng thực tế trong Data science, ta phải đối mặt với lượng data khổng lồ tùy trường hợp cụ thể, nên thông thường ta không tự tạo dataframe. Giả sử các data đến từ file có . csv viết tắt của comma separated values.  Để import vào môi trường Python ta dùng cú pháp:brics = pd. read_csv( path/to/brics. csv ) Tuy nhiên đối với file có index, khi import vào thì cột index đầu tiên sẽ bị ngầm hiểu là cột đầu của dữ liệu chính. Để tránh điều này, ta phải thêm argument index_col=0, kết quả:  Để thay đổi index tự động từ 0-n bằng label được định nghĩa trong 1 list tạo riêng tự chọn, ta dùng cú pháp:```  Definition of row_labels  row_labels = [‘US’, ‘AUS’, ‘JPN’, ‘IN’, ‘RU’, ‘MOR’, ‘EG’]  Specify row labels of carscars. index = row_labels ##### Cách để access DataframeTa input vào label hoặc index của column hoặc row. Ví dụ cụ thể ta có df là cars, xem cú pháp dưới đây: * Access COLUMN (cột):	* Dùng [] : 	- Nếu muốn output là Series object: ``cars['country', . . . ]``	- Nếu muốn output là Dataframe, dùng double ngoặc vuông:``cars[['country', . . . ]]``&gt; Nhìn ở góc khác, ta đang input vào ngoặc vuông 1 list chứa column labels. 	* Dùng ``loc`` (chọn 1 phần data dựa trên label-based) hoặc ``iloc`` (chọn data dựa trên integer position-based)	- cars. loc['country', . . . ] hoặc cars. loc[['country', . . . ]] 	- car. iloc[0, . . . , . . . ] hoặc car. iloc[[0, . . . , . . . ]]	* Access ROW (quan sát)		* Chỉ có cách là dùng [] nhưng input vào số:	- Nếu muốn lấy row từ index 1 đến 3:``cars[1:4]`` 	* Dùng loc hoặc iloc và input vào index của row thay vì tên cột như truy cập vào column. * Access ROWs &amp; COLUMNs bất kì:	* Sử dụng loc và iloc tiện lợi:	- Ta chỉ cần đặt vào label của row và column trong loc hoặc iloc theo thứ tự ``row, column``. 	- Nếu chọn nhiều hơn 1 label trong row hoặc column, ta biến argument row hoặc column thành list. Xem	 hình ví dụ:	![](/assets/images/loc-iloc. png)	&gt; Nhận xét: - Dấu ngoặc vuông``[]`` có giới hạn chức năng và lý tưởng nhất là sử dụng trong 2D Numpy array để access value dễ dàng nhất. 	- Nếu muốn dấu``[]``có thể mở rộng khả năng access value trong pandas như dấu``[]``trong 2D Numpy array, thì ta cần sử dụng ``loc`` và ``iloc``. ![](/assets/images/iloc-loc. png)		##### Filter dataframe* Bước 1: Access cột trả về series object. * Bước 2: Xác định điều kiện filter và trả về Boolean Series. Nếu &gt; 2 điều kiện thì phải sử dụng Numpy variants của toán tử and, or, not. * Bước 3: Dùng Boolean Series là kết quả của bước 1 làm input trong dấu ngoặc vuông của Dataframe. Kết quả trả về các record thỏa điều kiện. &gt; You'll want to build up a boolean Series, that you can then use to subset the cars DataFrame to select certain observations. If you want to do this in a one-liner, that's perfectly fine!#### 3. LOOP##### WHILE##### FOR for var in seq :	expression Trong đó ``var`` là biến bất kỳ có thể đặt tên sao cũng được. Python dùng nó để quét lần lượt cái phần tử trong ``seq``. FOR còn dùng để lặp từng char trong string. ![](/assets/images/string-loop. png)* enumerate() : cung cấp 2 giá trị cho mỗi lần lặp gồm ``index`` và ``value (giá trị)``. ![](/assets/images/enumerate-for. png)Mỗi data structure sẽ có cách loop các nhau và cách định nghĩa sequence khác nhau (seq). Cụ thể các bạn xem dưới đây nhé:##### Loop với List của Lists * Nếu list mà bạn cần lặp là list của list, thì dùng cách như sau:house list of listshouse = [[“hallway”, 11. 25],     [“kitchen”, 18. 0],     [“living room”, 20. 0],     [“bedroom”, 10. 75],     [“bathroom”, 9. 50]] Build a for loop from scratchx quét từng list trong list, dùng [] để truy cập phần tử của sub-listfor x in house :  print(“the “ + x[0] + “ is “ + str(x[1]) + “ sqm”)``` Loop với DictionarySử dụng method items() : for key, val in my_dict. items() : Loop với Numpy arraySử dụng function np. nditer(my_array) đặc biệt là với 2D array. for val in np. nditer(my_array) :    Với 1D array ta có thể sử dụng loop thông thường, nhưng với 2D thì nó sẽ in ra 2D array thay vì ra các giá trị cần lấy trong loop.     Dùng nditer sẽ giúp in ra từng giá trị từ trái sang phải từ trên xuống dưới của 2D array.  Pandas DataFrameiterrows() : Trong mỗi lần lặp, method này sẽ generate ra 2 giá trị:  Label của row (nếu ko có thì là index tự động) Data của row (là Pandas Series có index/label là tên cột - còn gọi là fieldname) Để loop in ra giá trị của cột mong muốn cho mỗi lần lặp, ta chỉ cần: print(row[“tên cột”])  Thêm cột vào Dataframe bằng loop:Ví dụ ta muốn thêm cột tính độ dài của cột “country”:brics. loc[lab,  tên cột mới ] = len(row[ country ])  Nhận xét: Cách này tốt trong trường hợp ít record. Vì ta đang tạo ra Series object cho mỗi vòng lặp và nó sẽ không hiệu quả với các dataset khổng lồ, thậm chí gây ra vấn đề khi xử lý dữ liệu. Vậy nên, cách tốt nhất là ta sử dụng function apply(tên function) cho mỗi cột mà ta muốn tính toán rồi gán vào cột mới trong dataframe: brics[ cột mới ] = brics[ country ]. apply(len)  Cách hoạt động: Function apply() sẽ gọi function len() mà mỗi giá trị của cột country sẽ là input để tính độ dài từng country. Kết quả trả về là 1 array mà chúng ta có thể dễ dàng lưu thành cột mới trong Dataframe. "
    }, {
    "id": 22,
    "url": "https://tuyen-nnt.github.io/import-data-medium/",
    "title": "Import Data - Part 2",
    "body": "2021/07/18 - 1. Import + Load + Tạo HTTP/GET REQUEST:  Lưu file mềm xuống local:urlretrieve(url, 'filename. csv')Trước tiên phải from urllib. request import urlretrieve  Mở và đọc file mềm trên web:df = pd. read_csv(url, sep=';')   Show các dòng đầu tiên của df:print(df. head())   URL (Uniform/Universal Resource Locator)phần lớn là các địa chỉ web, ngoài ra còn là FTP (file transfer protocol)URL gồm 2 phần:  Protocol Identifier : http hoặc https   Tên resource: datacamp. com=&gt; tạo thành 1 địa chỉ web     HTTP (HyperText Transfer Protocol)Là protocol ứng dụng cho các hệ thống thông tin phân tán, cộng tác và siêu phương tiện, nền tảng giao tiếp dữ liệu cho WWW. HTTPS - có độ an toàn bảo mật cao hơn HTTP   Mỗi khi truy cập vào 1 trang web nghĩa là bạn đang gửi 1 HTTP request cho 1 server. Request này được gọi là GET request, đây là loại request phổ biến nhất.  urlretrieve : gửi GET request và lưu dữ liệu xuống local máy  HTML (HyperText Markup Language)2. Các cách gửi GET request::  Cách 1: sử dụng urllib. request=&gt; from urllib. request import urlopen, RequestMột số functions của package urllib. request  request = Request(url) : đóng gói GET request response = urlopen(request) : gửi request và catch phản hồi =&gt; trả về HTTP response object có tích hợp method read() và close() html = response. read() : trả về HTML định dạng string   response. close(): dùng xong nhớ đóng lại   Cách 2: rất phổ biến, sử dụng package requestsCho phép gửi HTTP request có tổ chức mà ko cần làm thủ công requests. get(url) : sau khi import package requests, hàm request. get() sẽ đóng gói request thông qua url, gửi request đi và nhận lại phản hồi và lưu vào biến r.  Ở đây hàm request. get() sẽ làm nhiệm vụ của Request(url) và urlopen(request đã đóng gói)của cách 1 r. text : r là biến lưu response của hàm trên, sử dụng method . text cho response để chuyển HTML của url sang dạng string. 3. Scraping web trong Python: HTML là sự kết hợp của data có cấu trúc và không cấu trúc.  Hàm BeautifulSoup() có tác dụng parse và trích xuất data từ HTML, và làm cho các tag được biểu diễn đẹp hơn. Cách sử dụng: from bs4 import BeautifulSoup Sau khi gửi nhận phản hồi của GET request, ta được file html như trên. Sau đó, ta dùng hàm BeautifulSoup để extract các data có cấu trúc của file html, lưu kết quả là một object vào một biến mới. Kết quả của hàm BeautifulSoupcó tích hợp hàm . prettify() để làm đẹp kết quả. soup = BeautifulSoup(html_doc)print(soup. prettify())    Các hàm khác có thể dùng sau khi parse và nhận kết quả từ BeautifulSoup:      soup. title() : trích title của file html   soup. get_text(): trích tất cả text của file html   soup. find_all() : tìm tất cả các data theo điều kiện hoặc tag nào đó.  Ví dụ:     for link in soup. find_all('a'): print(link. get('href')    hoặc      for link in a_tags: print(link. get('href'))      =&gt; Ở đây, ta sử dụng vòng lặp for kết hợp hàm . find_all() extract data nằm trong tag &lt;a&gt; của file html và in ra từng link của mỗi dòng tìm được, ta cũng có thể lưu soup. find_all() vào một biến nào đó. Hàm link. get('href') dùng để extract giá trị link của attribute href trong tag &lt;a&gt; 4. Load và khám phá file JSON: FIle JSON nằm ở local Bước 1: Tạo connection với file JSON trong local và load file  with open( tên file. json ) as json_file : json_data = json. load(json_file)  Ở đây json_data là 1 object dictionary, ta check bằng type(json_data) ra kết quả dict   Bước 2: Sử dụng vòng lặp for để in cặp key-value ra  for k in json_data. keys(): print(k + ': ', json_data[k])  Từ object dictionary trên, ta dùng àm . keys để truy cập vào keys của file và dùng cú pháp dictionary[key] để truy cập vào value.  5. APIs và tương tác cơ bản: API là gì? Là một bộ protocols và routines để xây dựng và tương tác với phần mềm Một tập hợp code cho phép 02 chương trình phần mềm giao tiếp với nhauVí dụ nếu muốn stream data của Twitter thì ta cùng API của Twitter.  Thông thường data thường được lấy về từ APIs ở định dạng JSON. URL có gì và làm thể nào để nó biết pull data từ API về?url = 'http://www. omdbapi. com/?t=hackers'  http - dấu hiệu là ta đang tạo 1 HTTP request www. omdbapi. com - nghĩa là ta đang query OMDB API ?t=hackers     Đây gọi là Query String   Không có quy ước và không buộc có trong đường dẫn   Sau dấu ? là phần query. Theo document trên trang chủ OMDB API thì có nghĩ là ta đang muốn trả về data của bộ phim có title (t) ‘Hackers’. Cụ thể xem phần Usage + Parameters.    # Import packageimport requests# Assign URL to variable: urlurl = 'https://en. wikipedia. org/w/api. php?action=query&amp;prop=extracts&amp;format=json&amp;exintro=&amp;titles=pizza'# Package the request, send the request and catch the response: rr = requests. get(url)# Decode the JSON data into a dictionary: json_datajson_data = r. json()print(json_data)# Print the Wikipedia page extractpizza_extract = json_data['query']['pages']['24768']['extract']print(pizza_extract)Ở 2 dòng code cuối, để biết tại sao code như thế, ta truy cập url trên browser, nó sẽ hiện ra các tab. Ta muốn extract data từ api của url thì ta mở từ tab query &gt; pages &gt; 24768 &gt; extract thì sẽ nhận được data từ api đó. Load và khám phá Twitter data# Import packageimport json# String of path to file: tweets_data_pathtweets_data_path = 'tweets. txt'# Initialize empty list to store tweets: tweets_datatweets_data = []# Open connection to filetweets_file = open(tweets_data_path,  r )# Read in tweets and store in list: tweets_datafor line in tweets_file:  tweet = json. loads(line)  tweets_data. append(tweet)# Close connection to filetweets_file. close()# Print the keys of the first tweet dictprint(tweets_data[0]. keys()) Đầu tiên ta gán đường dẫn tên file chứa Twitter data ở local máy vào 1 biến.  Tiếp theo ta tạo 1 mảng rỗng để chứa mỗi dòng tweet là 1 phần tử trong mảng.  Sau đó ta mở connection đến file local đó thông qua dường dẫn tweets_data_path và lưu vào tweets_file.  Tiếp theo ta dùng vòng lặp for để đọc từng dòng của tweets_file:     Dùng hàm json. load(line) để load từng dòng lưu vào biến tweet, để dùng hàm trên phải import json package         Note: mỗi lần load line để lưu vào tweet là một dictionary.           Sử dụng biến mảng tweets_data kết hợp hàm append(tweet) để add thêm phần tử dictionary mới (hay còn gọi là tweet) vào mảng.    In ra tất cả các keys của phần tử đầu tiên (tweet hay dict) của mảng.    Đưa mảng vào Dataframe sử dụng package pandas để phân tích# Import packageimport pandas as pd# Build DataFrame of tweet texts and languagesdf = pd. DataFrame(tweets_data, columns=['text','lang']) # Print head of DataFrameprint(df. head()) Hàm pd. Dataframe() cần 2 tham số là data và column để xây dựng df     Tham số đầu có thể là mảng, dict hoặc dataframe   Tham số thứ 2 là column label, nếu không có label thì dùng RangeIndex(0,1,2,…n). Nếu có label trong data như ví dụ trên, ta chỉ cần columns=['text','lang'] để chọn label cho column muốn rút giá trị. Ở ví dụ trên ta sẽ tạo 2 cột text và lang.    Streaming# Initialize Stream listenerl = MyStreamListener()# Create your Stream object with authenticationstream = tweepy. Stream(auth, l)# Filter Twitter Streams to capture data by the keywords:stream. filter(track=['clinton', 'trump', 'sanders', 'cruz'])   Class MyStreamListener() được khai báo sẵn tại đây: https://gist. github. com/hugobowne/18f1c0c0709ed1a52dc5bcd462ac69f4     Ta tạo object streambằng cách đưa vào hàm tweepy. Stream() athentication handler auth và object l - stream listener trên.     Object stream có tích hợp hàm . filter(), trong hàm này có attribute track=[] là list chứa các keyword mà ban muốn filter.  Phân tích data cơ bảnimport redef word_in_text(word, text):  word = word. lower()  text = text. lower()  match = re. search(word, text)  if match:    return True  return False Ở trên ta có hàm word_in_text() để đếm số lượng tweet chứa keyword. Nhưng ở đây chúng ta chưa đếm, mà chỉ đưa kết quả nếu True sẽ +1 vào biến đếm ở bước tiếp theo. # Initialize list to store tweet counts[clinton, trump, sanders, cruz] = [0, 0, 0, 0]# Iterate through df, counting the number of tweets in which# each candidate is mentionedfor index, row in df. iterrows():  clinton += word_in_text('clinton', row['text'])  trump += word_in_text('trump', row['text'])  sanders += word_in_text('sanders', row['text'])  cruz += word_in_text('cruz', row['text']) Tiếp theo ta sẽ tạo list trong Python, mỗi item sẽ có giá trị đếm bắt đầu =0. Mục đích ở đây là để đếm số tweet count được cho mỗi keyword.  Sử dụng vòng lặp để đi từng row và check, nếu gặp keyword sẽ +1 vào biến đếm. Nếu True (nghĩa là có keyword đó) thì += 1. Basic Data visualization# Import packagesimport seaborn as snsimport matplotlib. pyplot as plt# Set seaborn stylesns. set(color_codes=True)# Create a list of labels:cdcd = ['clinton', 'trump', 'sanders', 'cruz']# Plot the bar chartax = sns. barplot(cd, [clinton, trump, sanders, cruz])ax. set(ylabel= count )plt. show() Đầu tiên cần import 2 package như trên để vẽ biểu đồ Hàm sns. barplot() có 2 tham số:     Tham số đầu: list label cần biểu diễn giá trị   Tham số thứ 2: list chứa giá trị của các label cần biểu diễn. List này đã được khởi tạo và đếm ở code trước đó.    "
    }, {
    "id": 23,
    "url": "https://tuyen-nnt.github.io/spark2/",
    "title": "Spark2",
    "body": "2021/07/15 - layout: posttitle: “(ENG) The basics of Spark PART 2”author: tuyennntcategories: [ Data ]image: assets/images/spark. png—    PySpark APIs tương tự như Pandas và Scikit-learn   Schema là nơi lưu và control thông tin của dataframe bao gồm tên các cột, kiểu dữ liệu, empty values,… Ngoài ra nó còn giúp DFs tối ưu queries. Nếu để schema=None thì spark sẽ tự suy luận kiểu data,… dựa trên data của bạn. Còn không có thể tự quy định schema như ví dụ sau:  schema1 = StructType([StructField('id', IntegerType(), True),          StructField('create_date', StringType(), True),          StructField('txn_date', StringType(), True)])  Retrieve SparkContext versionsc. version     2. 3. 1     Retrieve Python version of SparkContext sc. pythonVer     3. 6     master: URL of cluster or “local” string to run in local mode of SparkContextsc. master     local[*]     Tạo spark Dataframespark. createDataFrame()Cách Load data vào PySpark: Load data vào SparkContext:  Cách 1 : Dùng parallelize method của SparkContext với input là 1 list.   rdd = sc. parallelize([1,2,3,4,5])     Tạo parallelize collection lưu số từ 1 đến 5     Cách 2:Dùng method của SparkContext textFile()  rdd2 = sc. textFile( test. txt ) Load data vào Spark dataframe:  Cách 1: load data từ file và tạo thành dataframeDùng method của SparkSession  spark. read. csv( . . .  , header = True, inferSchema=True)spark. read. json( . . .  )spark. read. txt( . . .  )  Cách 2 : Tạo dataframe từ RDDĐầu tiên phải tạo RDD từ list. Optional là tạo thêm list của columns tên là namesdf = spark. createDataFrame(RDD, schema=names) Read &amp; Write file từ HDFS:: https://saagie. zendesk. com/hc/en-us/articles/360029759552-PySpark-Read-and-Write-Files-from-HDFS "
    }, {
    "id": 24,
    "url": "https://tuyen-nnt.github.io/python-basic/",
    "title": "Python - Part 1",
    "body": "2021/07/11 - I. LIST:  Thêm phần tử cho list:Chỉ cần + [list] x = [ a ,  b ,  c ,  d ]y = x + [ e ,  f ] Xóa phần tử list:Dùng del(phần tử). Lưu ý là khi xóa thì các phần tử ở sau bị đẩy index lên. x = [ a ,  b ,  c ,  d ]del(x[1]) Dấu [index:index] trong list     x[start:end] Với start là chỉ số lấy end chỉ số không lấyVí dụ:   x[2:5] : lấy giá trị của index từ 2 đến 4   x[:3] : từ đầu đến index 2   x[3:] : từ index 3 đến hết   x[:] : lấy hết phần tử    Dấu ;:Dùng để tách command code trên cùng 1 dòng, nếu khác dòng thì không cần. ```  Same line  command1; command2  Separate linescommand1command2 * Copy list:Khi copy kiểu ``x=y`` thì thực tế ta đang copy địa chỉ list của y cho x. Nghĩa là khi ta thay đổi phần tử trong x thì y cũng thay đổi theo. Để xử lý tình huống này, nếu chỉ muốn copy giá trị list thì ta dùng: ``x = list(y)`` hoặc ``x = y[:]``* Convert datatypex = str(y)x = int(y)x = float(y)&gt; Check datatype bằng function ``type()``* Xem cấu trúc của 1 function có sẵn:``help(max)`` hoặc ``?max``#### II. METHOD* Cũng là function nhưng dành cho từng type* Tất cả mọi thứ trong Python đều là object. * Object có các method riêng, phụ thuộc vào data type #### III. NumpyTại sao dùng Numpy?* Rất quyền lực, có thể sử dụng cho nhiều data type khác nhau. Tuy nhiên mỗi tập hợp (array) chỉ được chứa 1 loại data type. * Có thể thêm, xóa, sửa* Quan trọng trong Data Science	* Làm các phép toán cho các tập hợp	* Tốc độ nhanh* Ta thấy nếu áp dụng phép toán như -*/ trên kiểu dữ liệu list thì sẽ throw Error ``không hỗ trợ``. Nếu + thì sẽ ghép 2 list lại thành 1 list. &gt; Tuy nhiên, ta đang cần +-*/ trên 2 list theo index tương ứng. Thì Numpy sẽ giúp ta giải quyết khó khăn này. Giải pháp Numpy có gì?* Python kiểu số. Có hàm ``np. mean()`` và ``np. median()`` rất phổ biến trong data science. * Thay thế cho Python List : kiểu dữ liệu ``Numpy Array``* Giúp tính toán trên toàn bộ array* Nhanh và dễ dàng* Cài đặt trong terminal: ``pip3 install numpy``Như vậy, để thực hiện các phép toán trên list, ta phải chuyển nó sang kiểu dữ liệu Numpy Array như sau:import numpy as npnp_height = np. array(height)np_weight = np. array(weight)bmi = np_weight / np_height ** 2 **Lưu ý quan trọng:*** Mỗi array chỉ được chứa 1 loại data type. * Type khác nhau thì hành vi của nó sẽ khác nhau. VD: 	* Phép + 2 list thì là ghép 2 list thành 1. 	* Phép + 2 array thì là cộng theo giá trị index tương ứng của 2 array với nhau. ##### 1. Numpy Subsetting * Ta có array ``bmi``. * Để truy cập vào array ta dùng cú pháp: ``bmi[index]``* Để xét các giá trị trong array có thỏa điều kiện không, ta dùng: ``bmi &gt; 23``&gt; Trả về array có kiểu Boolean (True/False)* Để trả về array chứa các giá trị thỏa điều kiện, ta dùng: ``bmi[bmi &gt; 23]`` ##### 2. Type của Numpy arrayNếu ``print(type(np_height))``&gt; numpy. ndarrayVới numpy là package, n là layer của array. ndarray là kiểu dữ liệu chỉ sử dụng trong Numpy. ##### 3. 2D Numpy Arrays Cách tạo 2D array bằng 2 array:* array1* array2* meas = np. array([array1, array2])Có thể xem 2D numpy array như phiên bản nâng cấp của  list của list  vì ta có thể thực hiện các phép toán với nó. Để tạo array 2D, ta chỉ cần input các giá trị vào như dưới đây, input 1 list có 2 sub-list vào argument của method ``np. array`` theo cấu trúc hình chữ nhật:![](/assets/images/2d-array. png)Mỗi sub-list là một row của array.  Nếu ta thay đổi kiểu dữ liệu của bất kỳ giá trị nào trong array sang kiểu khác như từ float sang string, thì mặc nhiên numpy sẽ chuyển tất cả các giá trị còn lại sang string (in ra sẽ thấy). &gt; Vì mỗi numpy array chỉ chứa 1 kiểu dữ liệu. Để biết cấu trúc data của array như thế nào, ta dùng attribute ``shape`` của array:![](/assets/images/array-shape. png) *Vì là attribute nên nó không có ``()`` như method~*Có 2 cú pháp để truy cập giá trị:![](/assets/images/2d-subset. png)* Cách 1: np_2d[row][column]* Cách 2: np_2d[row,column]* Dấu ``:`` vẫn sẽ được sử dụng giống như 1D array ở trên nếu muốn chọn cụ thể vùng giá trị muốn lấy. *Với row, column là index, chú ý ở đây vẫn sử dụng zero-index cho 2D array. *##### 4. Toán tử Boolean trong NumpyVới Numpy array ta có thể sử dụng phép so sánh như các ví dụ trên, nhưng nếu sử dụng kết hợp and, or, not thì sẽ throw Error. Do vậy, ta phải sử dụng:* np. logical_and()* np. logical_or() * np. logical_not()Ex:np. logical_and(my_house &gt; 13,        your_house &lt; 15) Kết quả trả về 1 Boolean Series, thích hợp dùng để filter dataframe```Với my_house và your_house là 2 Numpy array. "
    }, {
    "id": 25,
    "url": "https://tuyen-nnt.github.io/spark/",
    "title": "(ENG) The basics of Spark PART 1",
    "body": "2021/06/27 - Đầu tiên phải kết nối với Cluster. Cluster được host trên remote machine mà được connect với tất cả các node khác. Sẽ có 1 máy tính gọi là master phụ trách việc bóc tách dữ liệu và tính toán. master được kết nối với tất cả những máy tính còn lại trong cluster, các máy tính còn lại gọi là worker. master gửi cho workers data và các phép toán để chạy và chúng sẽ trả về cho master kết quả. The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that’s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master. Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like: Is my data too big to work with on a single machine?Can my calculations be easily parallelized?Bước 1: Create an instance of the SparkContext class to connect to a Spark cluster from PySpark. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you’re connecting to. An object holding all these attributes can be created with the SparkConf() constructor. Take a look at the documentation for all the details! You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection. from pyspark. sql import SparkSession, SQLContext# Create SparkSession. Creating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession. builder. getOrCreate() method. This returns an existing SparkSession if there's already one in the environment, or creates a new one if necessary!spark = SparkSession. builder. appName( CAR DAILY ). getOrCreate()# Create sparkContextsc = spark. sparkContextsqlContext = SQLContext(sc)# Verify SparkContextprint(sc)# Print Spark versionprint(sc. version)&lt;SparkContext master=local[*] appName=pyspark-shell&gt;You may also find that running simpler computations might take longer than expected. That’s because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions! Using DataFrames: Spark’s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you’ll be using the Spark DataFrame abstraction built on top of RDDs. The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it’s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in! To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection. I. SELECT flights. select(flights. air_time/60) returns a column of flight durations in hours instead of minutes. if you wanted to . select() the column duration_hrs (which isn’t in your DataFrame) you could do  flights. select((flights. air_time/60). alias(“duration_hrs”)) The equivalent Spark DataFrame method . selectExpr() takes SQL expressions as a string:  flights. selectExpr(“air_time/60 as duration_hrs”)with the SQL as keyword being equivalent to the . alias() method. To select multiple columns, you can pass multiple strings.  View tables:Once you’ve created a SparkSession, you can start poking around to see what data is in your cluster!Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. One of the most useful is the . listTables() method, which returns the names of all the tables in your cluster as a list. spark. catalog. listTables() Querying:Running a query on this table is as easy as using the . sql() method on your SparkSession. This method takes a string containing the query and returns a DataFrame with the results!If you look closely, you’ll notice that the table flights dataframe is only mentioned in the query, not as an argument to any of the methods. This is because there isn’t a local object in your environment that holds that data, so it wouldn’t make sense to pass the table as an argument.  Convert Spark DF to Pandas DF - Pandafy a Spark DataFrameSuppose you’ve run a query on your huge dataset and aggregated it down to something a little more manageable. Sometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the . toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame. It’s as simple as that! This time the query counts the number of flights to each airport from SEA and PDX. Remember, there’s already a SparkSession called spark in your workspace! # Don't change this queryquery =  SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest # Run the queryflight_counts = spark. sql(query)# Convert the results to a pandas DataFramepd_counts = flight_counts. toPandas()# Print the head of pd_countsprint(pd_counts. head())Convert from Pandas DF to Spark DF - Put some Spark in your dataIn the last exercise, you saw how to move data from Spark to pandas. However, maybe you want to go the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well. The . createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame. The output of this method is stored locally, not in the SparkSession catalog. This means that you can use all the Spark DataFrame methods on it, but you can’t access the data in other contexts. For example, a SQL query (using the . sql() method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table. You can do this using the . createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you’d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame. There is also the method . createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You’ll use this method to avoid running into problems with duplicate tables. Giải thích thêm: Trong 1 SparkSession có nhiều SparkContext như sql(),… Trong SparkSession nó có method giúp chuyển đổi từ Pandas DF thành Spark DF. Spark Dataframe tạo ra từ . createDataFrame() cho kết quả về được lưu locally nhưng không được lưu vào SparkSession catalog. Khi đó ta không thể truy cập từ các context khác nhau trong cùng SparkSession như không thể query . sql(). Do vậy sau khi dùng method trên, ta dùng . createTempView() là method dành cho Spark DF để đăng ký Spark DF này thành 1 temporary table trong catalog và có thể sử dụng chung cho các context khác nhau. Tuy nhiên, vì là temporary nên Spark DF này không dùng chung được cho các SparkSession khác nhau mà chỉ dùng cho Session mà tạo ra nó. Ngoài ra, . createOrReplaceTempView() tương tự method tạo temp table nhưng nó giúp tránh duplicate table nếu như nó đã có tạo rồi thì update.  # Create pd_temppd_temp = pd. DataFrame(np. random. random(10))# Create spark_temp from pd_tempspark_temp = spark. createDataFrame(pd_temp)# Examine the tables in the catalogspark. catalog. listTables()# Add spark_temp to the catalogspark_temp. createOrReplaceTempView( temp )# Examine the tables in the catalog againprint(spark. catalog. listTables())Xem cách các Spark data structure tương tác với nhau bằng nhiều cách trong biểu đồ dưới đây: Convert thành Spark DF trực tiếp từ file . csv mà không cần thông qua pandas DF:Now you know how to put data into Spark via pandas, but you’re probably wondering why deal with pandas at all? Wouldn’t it be easier to just read a text file straight into Spark? Of course it would! Luckily, your SparkSession has a . read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a . csv file just like with regular pandas DataFrames! The variable file_path is a string with the path to the file airports. csv. This file contains information about different airports all over the world. # Don't change this file pathfile_path =  /usr/local/share/datasets/airports. csv # Read in the airports dataairports = spark. read. csv(file_path, header=True)# Show the dataairport. head()Creating columnsIn this chapter, you’ll learn how to use the methods defined by Spark’s DataFrame class to perform common data operations. Let’s look at performing column-wise operations. In Spark you can do this using the . withColumn() method, which takes two arguments. First, a string with the name of your new column, and second the new column itself. The new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df. colName. Updating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can’t be changed, and so columns can’t be updated in place. Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so: df = df. withColumn( newCol , df. oldCol + 1)The above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one. To overwrite an existing column, just pass the name of the column as the first argument! # Create the DataFrame flightsflights = spark. table( flights )# Show the headflights. show()# Add duration_hrsflights = flights. withColumn( duration_hrs , flights. air_time/60)flights. show()Use the spark. table() method with the argument  flights  to create a DataFrame containing the values of the flights table in the . catalog. Save it as flights. Show the head of flights using flights. show(). Check the output: the column air_time contains the duration of the flight in minutes. Update flights to include a new column called duration_hrs, that contains the duration of each flight in hours (you'll need to divide duration_hrs by the number of minutes in an hour). Filtering DataNow that you have a bit of SQL know-how under your belt, it’s easier to talk about the analogous operations using Spark DataFrames. Let’s take a look at the . filter() method. As you might suspect, this is the Spark counterpart of SQL’s WHERE clause. The . filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values. For example, the following two expressions will produce the same output: flights. filter(“air_time &gt; 120”). show()flights. filter(flights. air_time &gt; 120). show() Notice that in the first case, we pass a string to . filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time &gt; 120. Spark’s . filter() can accept any expression that could go in the WHEREclause of a SQL query (in this case, “air_time &gt; 120”), as long as it is passed as a string. Notice that in this case, we do not reference the name of the table in the string – as we wouldn’t in the SQL request. In the second case, we actually pass a column of boolean values to . filter(). Remember that flights. air_time &gt; 120 returns a column of boolean values that has True in place of those records in flights. air_time that are over 120, and False otherwise. # Filter flights by passing a stringlong_flights1 = flights. filter( distance &gt; 1000 )# Filter flights by passing a column of boolean valueslong_flights2 = flights. filter(flights. distance &gt; 1000)# Print the data to check they're equallong_flights1. show()long_flights2. show()=&gt; Kết quả như nhau SelectingThe Spark variant of SQL’s SELECT is the . select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df. colName syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside . withColumn(). The difference between . select() and . withColumn() methods is that . select() returns only the columns you specify, while . withColumn() returns all the columns of the DataFrame in addition to the one you defined. It’s often a good idea to drop columns you don’t need at the beginning of an operation so that you’re not dragging around extra data as you’re wrangling. In this case, you would use . select() and not . withColumn(). Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. # Select the first set of columnsselected1 = flights. select( tailnum ,  origin , dest )# Select the second set of columnstemp = flights. select(flights. origin, flights. dest, flights. carrier)# Define first filterfilterA = flights. origin ==  SEA # Define second filterfilterB = flights. dest ==  PDX # Filter the data, first by filterA then by filterBselected2 = temp. filter(filterA). filter(filterB)Select the columns “tailnum”, “origin”, and “dest” from flights by passing the column names as strings. Save this as selected1. Select the columns “origin”, “dest”, and “carrier” using the df. colName syntax and then filter the result using both of the filters already defined for you (filterA and filterB) to only keep flights from SEA to PDX. Save this as selected2. Selecting IISimilar to SQL, you can also use the . select() method to perform column-wise operations. When you’re selecting a column using the df. colName notation, you can perform any column operation and the . select() method will return the transformed column. For example, flights. select(flights. air_time/60) returns a column of flight durations in hours instead of minutes. You can also use the . alias() method to rename a column you’re selecting. So if you wanted to . select() the column duration_hrs (which isn’t in your DataFrame) you could do flights. select((flights. air_time/60). alias( duration_hrs )) The equivalent Spark DataFrame method . selectExpr() takes SQL expressions as a string: flights. selectExpr( air_time/60 as duration_hrs ) with the SQL as keyword being equivalent to the . alias() method. To select multiple columns, you can pass multiple strings. Remember, a SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. # Define avg_speedavg_speed = (flights. distance/(flights. air_time/60)). alias( avg_speed )# Select the correct columnsspeed1 = flights. select( origin ,  dest ,  tailnum , avg_speed)# Create the same table using a SQL expressionspeed2 = flights. selectExpr( origin ,  dest ,  tailnum ,  distance/(air_time/60) as avg_speed )AggregatingAll of the common aggregation methods, like . min(), . max(), and . count() are GroupedData methods. These are created by calling the . groupBy() DataFrame method. You’ll learn exactly what that means in a few exercises. For now, all you have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, col, in a DataFrame, df, you could do df. groupBy(). min( col ). show() This creates a GroupedData object (so you can use the . min() method), then finds the minimum value in col, and returns it as a DataFrame. Now you’re ready to do some aggregating of your own! A SparkSession called spark is already in your workspace, along with the Spark DataFrame flights. Ex: flights. show()# Find the shortest flight from PDX in terms of distanceflights. filter(flights. origin ==  PDX ). groupBy(). min( distance ). show()# Find the longest flight from SEA in terms of air timeflights. filter(flights. origin ==  SEA ). groupBy(). max( air_time ). show()Grouping and Aggregating IPart of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: pyspark. sql. GroupedData, which you saw in the last two exercises. You’ve learned how to create a grouped DataFrame by calling the . groupBy() method on a DataFrame with no arguments. Now you’ll see that when you pass the name of one or more columns in your DataFrame to the . groupBy() method, the aggregation methods behave like when you use a GROUP BY statement in a SQL query! # Group by tailnumby_plane = flights. groupBy( tailnum )# Number of flights each plane madeby_plane. count(). show()# Group by originby_origin = flights. groupBy( origin )# Average duration of flights from PDX and SEAby_origin. avg( air_time ). show()Create a DataFrame called by_plane that is grouped by the column tailnum. Use the . count() method with no arguments to count the number of flights each plane made. Create a DataFrame called by_origin that is grouped by the column origin. Find the . avg() of the air_time column to find average duration of flights from PDX and SEA. Grouping and Aggregating IIIn addition to the GroupedData methods you’ve already seen, there is also the . agg() method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the pyspark. sql. functions submodule. This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table. # Import pyspark. sql. functions as Fimport pyspark. sql. functions as F# Group by month and destby_month_dest = flights. groupBy( month ,  dest )# Average departure delay by month and destinationby_month_dest. avg( dep_delay ). show()# Standard deviation of departure delayby_month_dest. agg(F. stddev( dep_delay )). show()Import the submodule pyspark. sql. functions as F. Create a GroupedData table called by_month_dest that's grouped by both the month and dest columns. Refer to the two columns by passing both strings as separate arguments. Use the . avg() method on the by_month_dest DataFrame to get the average dep_delay in each month for each destination. Find the standard deviation of dep_delay by using the . agg() method with the function F. stddev(). JoiningIn PySpark, joins are performed using the DataFrame method . join(). This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, on, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. The third argument, how, specifies the kind of join to perform. In this course we’ll always use the value how=”leftouter”. # Examine the dataprint(airports. show())print(flights. show())# Rename the faa columnairports = airports. withColumnRenamed( faa ,  dest )# Join the DataFramesflights_with_airports = flights. join(airports, dest ,how= leftouter )# Examine the new DataFrameprint(flights_with_airports. show())Examine the airports DataFrame by calling . show(). Note which key column will let you join airports to the flights table.   Rename the faa column in airports to dest by re-assigning the result of airports. withColumnRenamed(“faa”, “dest”) to airports.   Join the flights with the airports DataFrame on the dest column by calling the . join() method on flights. Save the result as flights_with_airports.     The first argument should be the other DataFrame, airports.     The argument on should be the key column.     The argument how should be “leftouter”.   Call . show() on flights_with_airports to examine the data again. Note the new information that has been added. Machine Learning Pipeline: Learn more at: http://spark. apache. org/docs/2. 1. 0/api/python/pyspark. html "
    }, {
    "id": 26,
    "url": "https://tuyen-nnt.github.io/data-visualization/",
    "title": "Data Visualization",
    "body": "2021/06/23 - 1. Các loại chart cơ bản: BAR CHART &amp; COLUMN CHART  Giúp ta nhìn vào giá trị cụ thể cho mỗi loại Có 4 loại:  Stacked bar và column chart     Biểu đồ chồng nhau theo giá trị,     Clustered bar và column chart     Biểu đồ nhiều cột trong 1 phân loại     100% stacked bar và column chart     Biểu đồ chồng theo %     Combo chart     Biểu đồ kết hợp cột và biểu đồ đường    LINE CHART  Giúp biểu diễn chuỗi giá trị theo dạng có hình, thường là thông qua diễn tiến thay đổi của thời gian AREA CHART  Dựa trên Line chart nhưng được fill màu PIE CHART &amp; DONUT CHART  Biểu diễn mỗi quan hệ giữa các thành phần và tổng thể TREE MAP  Biểu diễn mỗi quan hệ giữa các thành phần và tổng thể, với các hình vuông có màu có kích cỡ riêng biểu thị phần mà các giá trị chiếm. CARD &amp; MULTI-ROW CARD  Card     biểu diễn 1 giá trị    *Multi-row card  dùng để biểu diễn nhiều giá trị GAUGE CHART &amp; KPI  được thiết kế để hiển thị dữ liệu thực tế so sánh với dữ liệu ngân sách/doanh thu hoặc mục tiêu đã lên kế hoạch. TABLE &amp; MATRIX  biểu diễn chi tiết dữ liệu văn bản bằng định dạng Bảng  Table:     chứa dữ liệu liên quan trong chuỗi logical của dòng và cột, có thể bao gồm header &amp; footer của bảng     Matrix:     giống như bảng nhưng matrix có thể thu lại hoặc mở rộng ra bằng dòng hoặc cột    HIERARCHIES:  Level data từ cao đến thấp Ví dụ:Year =&gt; Quarter =&gt; Month =&gt; DayCompany =&gt; Region =&gt; Country =&gt; DIvision =&gt; Unit HÀM:  Là các công thức được định nghĩa trước sẵn để biểu diễn các phép tính trên các giá trị gọi là tham trị arguments. 2. Kiến thức nền tảng để visualize data: Có 3 cách để lấy được insight của data:    Cách 1: Tính toán thống kêmean (trung bình), median(trung vị), standard deviation (phương sai)     Cách 2: Run model/Chạy mô hìnhLinear (Tuyến tính) hoặc hồi quy logistic     Cách 3: Vẽ plotscatter, bar, histogram (biểu đồ tần suất),…Scatter plot:  The Datasaurus Dozen  Có 13 datasets, mỗi Dataset có 2 trục x và y được gọi là variable Variable đơn giản chỉ là biệt ngữ thống kê để chỉ cột dữ liệu Khi tính phương sai của mỗi dataset, ta sẽ tính ra 2 phương sai cho x và y trên tất cả các record. Vì dataset trên có 2 giá trị x và y.  Phương sai dùng để tính toán sự biến thiên của dữ liệu. Chọn biểu đồ chấm plot loại gì thì phù hợp?Trước tiên phải xác định variable x và y của data thuộc loại nào. Có 3 loại:    Continuous : thường là số và có thể làm các phép toán cho nó. Ví dụ như nhiệt độ, chiều cao, doanh thu,…     Categorical : thường là dạng văn bản text, những thứ được phân loại hay mô tả. Ví dụ như màu mắt, quốc gia,…     Cả hai loại trênVí dụ như tuổi thì dạng continuous, còn nhóm tuổi từ 25-30 thì lại là categorical. Thời gian thì continuous còn tháng thì categorical.      Ở đây tùy vào mục đích visualization mà bạn sẽ quyết định nó thuộc loại nào cho phù hợp.    Khi nào nên dùng biểu đồ tần suất Histogram? Nếu bạn có biến continuous như trên Khi bạn muốn biết hình dạng của sự phân tán data, ví dụ như bạn muốn biểu đồ thể hiện rõ giá trị cao nhất và thấp nhất. Một số thuật ngữ:  Bin (interval) : khoảng cách các ô trục trên biểu đồ, ví dụ 0-5, 5-10,… Nếu muốn rõ data hơn thì ta co bin lại 0-1, 1-2,…Như hình, bin 10-15 tuổi có giá trị trục y là 4, nghĩa là có 4 người từ 10-15 tuổi. Sự lựa chọn binwidth sẽ ảnh hưởng lớn đến hình ảnh biểu đồ. Dưới đây cho thấy nếu thu nhỏ bin lại còn 1 năm tuổi thì nhìn rất lộn xộn.  Các tiêu chí trải nghiệm qua để chọn binwidth phù hợp:    Modality : có bao nhiêu đỉnh trong biểu đồ?Unimodal, bimodal hay trimodal?     Skewness: lệch lạc hay cân xứng?lệch trái, phải hay cân đối ở giữa?     Kurtosis: có bao nhiêu điểm có giá trị = 0? (extreme value)  Khi nào nên vẽ Box Plots? Khi chúng ta có 1 variable là continuous, được tách ra phân loại bởi 1 variable categorical.  Khi chúng ta muốn so sánh sự phân tán dữ liệu của variable continuous cho mỗi category (phân loại). Các chỉ số trên box plots:  lower quartile: 1/4 có giá trị dưới số này median : trung vị upper quartile: 1/4 có giá trị trên số này inter-quartile range: khoảng cách từ lower đến upper whiskers: đường thẳng ngang hai bên, đường kẻ ra gấp 1-1,5 lần inter-quarter, dài đến mức độ đủ để biết rằng ngoài đường kẻ thì không có giá trị nào cả (extreme value).  Phân loại nào chỉ có đường thẳng nghĩa là chỉ có 1 giá trị. "
    }, {
    "id": 27,
    "url": "https://tuyen-nnt.github.io/phpmyadmin/",
    "title": "Set-up phpMyAdmin",
    "body": "2021/06/15 -    Initial setup with Ubuntu 20. 4: https://www. digitalocean. com/community/tutorials/initial-server-setup-with-ubuntu-20-04     Install Mysql on Ubuntu 20. 4:https://www. digitalocean. com/community/tutorials/how-to-install-mysql-on-ubuntu-20-04     Install LAMP (Linux, Apache, MySQL, PHP)https://www. digitalocean. com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-on-ubuntu-20-04     Install and Secure phpMyAdmin:https://www. digitalocean. com/community/tutorials/how-to-install-and-secure-phpmyadmin-on-ubuntu-20-04     Secure Apache with Let’s Encrypt on Ubuntu 20. 04 (free TLS/SSL certificate)https://www. digitalocean. com/community/tutorials/how-to-secure-apache-with-let-s-encrypt-on-ubuntu-20-04     DOCKER COMPOSE &amp; NETWORKhttps://vsudo. net/blog/docker-toan-tap. html  CÓ 2 CÁCH ĐỂ SỬ DỤNG phpMyAdmin:C1: cài đặt thủ công và sử dụng trên localhost hoặc domain trên server mình host (xem link trên)https://www. digitalocean. com/community/tutorials/how-to-install-and-secure-phpmyadmin-on-ubuntu-20-04 =&gt; đã thử và okC2:dùng Docker network chứa 2 container là MySQL và phpMyAdmin để truy cập MySQL server trên phpMyAdmin hosthttps://vsudo. net/blog/docker-toan-tap. html =&gt; đã thử và ok Cách để đưa db của mình chạy trên container có sẵn của phpMyAdminB1: tạo image chạy trên base php cho file . php chứa truy vấn bảng của db (như 1 ứng dụng) và phải cài đặt biến môi trường trong này cho giống với config của container. B2: compose với container của phpMyAdmin có sẵn (có thể chọn các image phù hợp nhu cầu) và bổ sung image ở trên vào file docker-compose. yml --------------------------------------------------WORKDIR /var/www/tuyen. techCOPY /etc/apache2/sites-available/tuyen. tech. conf /etc/apache2/sites-available/COPY /etc/apache2/conf-available/phpmyadmin. conf /etc/apache2/conf-available/COPY /usr/share/phpmyadmin/. htaccess /usr/share/phpmyadmin/COPY . /var/www/tuyen. tech/RUN sudo apt update &amp;&amp; apt install -y \&amp;&amp; apache2 \&amp;&amp; php libapache2-mod-php php-mysql \&amp;&amp; mysql-server \&amp;&amp; phpmyadmin php-mbstring php-zip php-gd php-json php-curl \&amp;&amp; phpmyadmin RUN sudo mysql_secure_installation \&amp;&amp; sudo mysql \&amp;&amp; sudo phpenmod mbstring \&amp;&amp; sudo htpasswd -c /etc/phpmyadmin/. htpasswd tuyen \&amp;&amp; sudo systemctl restart apache2 EXPOSE 9090--------------------------FROM ubuntu:latestMAINTAINER tuyennnt &lt;tuyendev96@gmail. com&gt;WORKDIR /var/www/tuyen. techCOPY /etc/apache2/sites-available/tuyen. tech. conf /etc/apache2/sites-available/COPY /etc/apache2/conf-available/phpmyadmin. conf /etc/apache2/conf-available/COPY /usr/share/phpmyadmin/. htaccess /usr/share/phpmyadmin/COPY . /var/www/tuyen. tech/RUN sudo apt update &amp;&amp; apt install -y \&amp;&amp; apache2 \&amp;&amp; php libapache2-mod-php php-mysql \&amp;&amp; mysql-server \&amp;&amp; phpmyadmin php-mbstring php-zip php-gd php-json php-curl \&amp;&amp; phpmyadmin RUN sudo mysql_secure_installation \&amp;&amp; sudo mysql \&amp;&amp; sudo phpenmod mbstring \&amp;&amp; sudo htpasswd -c /etc/phpmyadmin/. htpasswd tuyen \&amp;&amp; sudo systemctl restart apache2 EXPOSE 9090: : "
    }, {
    "id": 28,
    "url": "https://tuyen-nnt.github.io/data-etl/",
    "title": "Extract - Transform - Load (ETL process)",
    "body": "2021/05/29 - Tìm hiểu về ETLI. EXTRACT DATA: 1. Các cách để trích xuất dữ liệu: Có 3 cách:    Trích xuất từ file text, như file . txt or . csvText file có 3 loại là : 	* Text thuần	* Flat file (là file có . tsv hoặc . csv cách nhau bởi dấu “,” hoặc “tab” giữa các giá trị). Những file này có dòng thể hiện các record và cột thể hiện attribute của record. 	*File JSON: bán cấu trúc, có 4 kiểu dữ liệu atomic là number, string, boolean và null và 2 kiểu dữ liệu dạng composite là array và object. 	JSON có package hỗ trợ là “json” để import data	     Trích xuất từ web hoặc APIs của web services, như là Hacker News API      Thông qua web:      Ví dụ: bạn tìm kiếm thông tin trên google. com thì trình duyệt của bạn sẽ gửi request của bạn đến server của google và google sẽ trả về dữ liệu mà bạn đang tìm kiếm. * Thông qua API của web:Không phải lúc nào các trang web cũng trả về kết quả mà người thường có thể đọc ngay, mà các trang web đó sẽ trả về định dạng JSON thông qua API mà chúng ta request. Ta xem ví dụ request API từ trang Hackernews:Ta import package “request” rồi dùng method . get() để chèn vào link web cần lấy file JSON. Sau đó ta dùng method . json() để phân tích file JSON từ kết quả đã lấy được và chuyển hóa nó thành Python object.  Trích xuất từ một database trên web services Hầu hết các ứng dụng web đều có database để backup và để không bị ảnh hưởng khi tắt server, v. v. Cần phân biệt 2 loại database chính trong trường hợp này:     Application database : dùng cho trường hợp có nhiều giao dịch được cập nhật, loại này còn có tên gọi là OLTP (online transaction processing)   Analytical database: được xây dựng cho việc phân tích dữ liệu còn gọi là OLAP (online analytical processing)   2. Trích xuất dữ liệu từ database như thế nào?:  Dùng URI/chuỗi connection, cú pháp như sau:  [database_type]://[user[:password]@][host][:port]  Trong Python, ta dùng connection URI thông qua package sqlalchemy để tạo database engine : Từ engine đã được tạo ra, ta có thể dùng nó để đặt vào 1 số package hỗ trợ nó tương tác với database, đặc biệt là package pandas.  import requests```Lấy dữ liệu từ bài viết của Hackernews về, F12 inspect để lấy URL của nóresp = requests. get(“https://hacker-news. firebaseio. com/v0/item/16222426. json”) in dữ liệu vừa parse thành file json ra màn hìnhprint(resp. json()) parse dữ liệu ra rồi gán value của key “score” vào biến post_score, sau đó in cái biến rapost_score = resp. json()[“score”]print(post_score) * Một số ví dụ mở rộng hơn (xem phần 3 để hiểu hơn):Đọc dữ liệu từ database của postgreSQL, hàm extract dùng SQL query có nhiệm vụ chuyển từ dữ liệu bảng thành kiểu object mà pandas dùng (là dataframe)Function to extract table to a pandas DataFramedef extract_table_to_pandas(tablename, db_engine):  query = “SELECT * FROM {}”. format(tablename)  return pd. read_sql(query, db_engine) Connect to the database using the connection URI, sử dụng package sqlalchemyconnection_uri = “postgresql://repl:password@localhost:5432/pagila” db_engine = sqlalchemy. create_engine(connection_uri) Extract the film table into a pandas DataFrame, lưu ý nhớ để tên bảng dạng chuỗiextract_table_to_pandas(“film”, db_engine) Extract the customer table into a pandas DataFrameextract_table_to_pandas(“customer”, db_engine)``` 2. TRANSFORM DATA: 1. Một số phương thức chuyển đổi dữ liệu: Có thể thực hiện 1 hoặc nhiều các hình thức trong giai đoạn chuyển đổi dữ liệu đã rút trích:  Select 1 hay nhiều cột Phiên dịch dữ liệu thành code. Ví dụ như New York sẽ biến thành NY Kiểm tra dữ liệu có đúng không, nếu dữ liệu không đúng với kiểu dữ liệu hoặc dữ liệu muốn nhận từ cột, ta có thể bỏ record đó đi. Ví dụ như cột ngày nhưng lại chứa giá trị khác ngày.  Tách dữ liệu của 1 cột thành nhiều cột Join dữ liệu từ các nguồn, các bảng khác nhau. 2. Một số ví dụ: Bạn có thể dùng package pandas để chuyển đổi dữ liệu nếu lượng dữ liệu nhỏ. Ta có ví dụ tách dữ liệu từ 1 cột thành 2 cột sử dụng pandas: Nếu dữ liệu lớn, thông thường người ta sẽ dùng PySpark. Ta có ví dụ chuyển đổi dữ liệu bằng cách join các bảng với nhau. Nhưng trước hết chúng ta cần đẩy dữ liệu lên Spark: jbdc:để nhắn nhủ với Spark là phải dùng JBDC để kết nối, sau đó, ta input vào tên của bảng và cuối cùng trong properties chúng ta đặt thông tin kết nối vào. Dưới đây là 2 bảng cần join với nhau thông qua customer_id để tính rating trung bình của mỗi customer dành cho các phim: Và làm sao để dùng PySpark join và tính toán dữ liệu? Xem ảnh dưới nhé: Mình sẽ cho ra thêm các bài viết tìm hiểu sâu hơn về PySpark trong thời gian tới, các bạn hãy cùng chờ đợi nhé! III. LOAD DATA: "
    }, {
    "id": 29,
    "url": "https://tuyen-nnt.github.io/react-native-css/",
    "title": "React Native - CSS rules",
    "body": "2021/05/24 - Phân biệt các attribute quan trọng sau để build giao diện trong react native:  flexDirection: “row” hoặc “column” justify-content : “center”main/primary axis phụ thuộc vào flexDirection, ta có space-evenly   align-items : “center”căn chỉnh secondary axis cho mỗi line, có các loại như center, baseline,…   flexWrap: “wrap” để các component ko bị chèn ép mất đi   align-Content : căn chỉnh toàn bộ nội dung của 1 block trên secondary axis, chỉ hoạt động khi có flexWrap.   align-self flex: 1 flexBasis: 100, //width (nếu main là row) or height, phụ thuộc vào primary axis (main) flexGrow: 1 //giống flex : 1, trải ra đầy màn hình   flexShrink: 1 //giống flex : -1, co lại để ko tràn màn hình     bottom, right, left, top : di chuyển qua trái, phải bao nhiêu pixel   position: “absolute” hoặc “relative” //giá trị mặc định trong react native là “relative”*Dimensions: import thư viện này trong react DIP công thức như sau: Physical Pixels = DIPs x Scale Factor vd điện thoại có scale là 2 và point điểm ảnh là 320x480 thì View đối với Width có giá trị 150 là 150x2=300 =&gt; nghĩa là bề ngang là 1/2 màn hình "
    }, {
    "id": 30,
    "url": "https://tuyen-nnt.github.io/docker/",
    "title": "Basics of Docker",
    "body": "2021/05/02 - Một số định nghĩa cơ bản:: Docker imageLà file chứa tất cả các thông tin của chương trình, kể cả system, dependency,… được đóng gói vào. Bạn có thể xem các image đã tạo trong folder /var/lib/docker/images kết hợp sudo tree &lt;path trên&gt; Cách diễn giải khác thì đây là một file bất biến - không thay đổi, chứa các source code, libraries, dependencies, tools và các files khác cần thiết cho một ứng dụng để chạy. Docker container:Là 1 môi trường tách biệt (như 1 container) được đặt trên máy ở 1 không gian riêng và không liên quan đến các chương trình khác. Cách diễn giải khác là một run-time environment mà ở đó người dùng có thể chạy một ứng dụng độc lập. Những container này rất gọn nhẹ và cho phép bạn chạy ứng dụng trong đó rất nhanh chóng và dễ dàng. Sự liên quan:   Image có thể tồn tại mà không cần container, trong khi container chạy thì cần có image đã tồn tại. Vì vậy, container phụ thuộc vào image và sử dụng nó để tạo ra run-time environment và chạy ứng dụng trên đó.     Cả image và container đều là thành phần quan trọng để tạo ra 1 running container. Docker image là cực kì quan trọng để chi phối và định hình 1 Docker container.  Cách RUN image trên UBUNTU: docker run ubuntu //nếu chưa pull ubuntu image về thì sẽ pull rồi start, còn nếu pull rồi thì nó sẽ start container này luôn =&gt; nếu khi start mà thấy chưa có interact gì thì nó sẽ stop container lại luôn docker ps //xem list các tiến trình đang chạy của container hoặc các container đang chạy docker ps -a //xem các container đã dừng lại docker run -it ubuntu // start container với interaction mode và load ubuntu image lên trong cái container này Các câu lệnh: history !2 //để output cmd thứ 2 trong list history ls -1 //líst ds theo dạng dọc ls -l //list ds chi tiet touch &lt;tên file&gt; //lệnh tạo file mkdir &lt;tên thư mục&gt; //lệnh tạo thư mục mv &lt;tên file hoặc địa chỉ&gt; &lt;tên file hoặc địa chỉ&gt; //đổi tên hoặc di chuyển cat &lt;tên file&gt; more &lt;tên file hoặc địa chỉ&gt; //xem nội dung theo từng page, bấm phím space để qua trang tiếp theo nhưng ko scroll up lên được, nhấn enter để chạy từng line để xem less &lt;tên file hoặc địa chỉ&gt; // cat &lt;tên file&gt; &gt; &lt;tên file khác&gt;//copy nội dung file này vào file khác cat &lt;tên file&gt; &lt;tên file khác&gt; &gt; &lt;tên combined file&gt;	//combine nội dung 2 file vào file khác dấu &gt; áp dụng cho hầu hết các câu lệnhvd: echo hello &gt; hello. txtls -l /etc &gt; hello. txtsẽ cho kết quả ra file hello. txt trên. Trong Linux, mọi thứ đều ở duới dangj file, từ process, devices, hoặc địa chỉ thư mục đều là files Cấu trúc Linux:  / bin: chưa các file binaries, chương trình boot: các file liên quan booting dev: devices etc: editable text configuration, chứa file config home: thư mục của user root: chỉ có root mới vào được thư mục này lib: libary files như software library dependencies var: như biến, chứa các file được update thường xuyên như log,… proc: chứa các tiến trình đang runningCác bước để sử dụng Docker:  Cài đặt Docker Tạo 1 program và Tạo file plain text Dockerfile ghi hướng dẫn để đưa cho docker đóng gói ứng dụng thành 1 image. Image chứa tất cả mọi thứ để chương trình chạy. IMAGE bao gồm:     A cut-down OS   Môi trường runtime (vd như Node, Python)   File ứng dụng   Thư viện thứ 3   Biến môi trường =&gt; Sau khi có cái Image này, ta sẽ nói với Docker để start container Container như 1 process, nhưng là process đăc biệt vì nó có file system được cung cấp bởi Image. Ứng dụng của chúng ta sẽ được chạy trong cái process/container này.    Ok giờ chúng ta start docker. Thay vì chạy ứng dụng như bình thường, ta gọi Docker để chạy bên trong cái container nhé docker run . . Ví dụ sự khác nhau : Để chạy 1 chương trình JS đơn giản, ta cần phải qua 4 bước:  Start OS Cài Node Copy file ứng dụng Chạy node app. jsVới docker, ta chỉ cần:    Bước 1:      Vào thư mục project trên vscode code . Tạo file Dockerfile không ext, và input các thông tin sau:   FROM node:alpine  Chọn image node trên trang docker hub và : distribution của linux, ở đây chọn alpine vì nó nhẹ nhất . COPY . /app  copy tất cả file execute vào thư mục app tạo mới, vào rồi copy vào trong image  hoặc có thể hiểu là trong cái image có file system, và chúng ta tạo thư mục app bên trong file system đó, trong thư mục app chứa tất cả các file của program.  File requirements. txt chứa các thư viện mà chương trình cần. Cần COPY trước khi RUN các cài đặt trong Dockerfile. VD của file:tornado==6. 1. Với Pycharm thì ta có thể dùng Tools=&gt;Sync Python Requirement để tự động generate. WORKDIR /app  tạo đường dẫn rút gọn CMD node /app. js  câu lệnh chạy ứng dụng, để docker biết mà chạy câu nào khi run  Bước 2: Ta nói docker đóng gói úng dụng thành image docker build -t hello-docker . trong đó, dấu . chỉ cho docker biết chỗ nào chứa Dockerfile hello-docker là tên repos docker mình tự tạo để quản lýĐể xem có bao nhiêu docker đang chạy: docker image ls kết quả câu lệnh có cột SIZE chỉ size của OS (alpine) + ứng dụng + node  Bước 3: Chạy thử docker image docker run hello-dockerĐây là câu run cơ bản, ta có thể chạy ở thư mục nào cũng được, vì image đã chứa mọi thứ cần. Hầu hết các docker run dùng cú pháp chuẩn này: Cách 1:: docker run -d -p 80:8888 shorten-link Với 8888 ở đây là listen port của program trên container tại đây https://github. com/tuyen-nnt/shortenLink/  Với 80 là port trên máy host của mình, -d là chạy ngầm. Sau khi docker run thì ta test container bằng cách chạy câu lệnh: docker exec -it &lt;CONTAINER ID&gt; bash Trả về: root@05b418a61ca3:/app# ls  ls để list ra các file trong docker image, có thể check xem bước COPY đã đúng những file mình cần để chạy chương trình chưa? Để test wep app trên browser, ta dùng địa chỉ ip của máy:port máy host (expose) và thêm /đường dẫn như cài đặt. Cách 2:: Không cần port mà dùng luôn localhost, nghĩa là user bên ngoài chỉ cần dùng IP gateway:port của chương trình là được: docker run -d --net=host shorten-linkTuy nhiên nên hạn chế cách này, vì sẽ khiến conflict với các chương trình khác nếu chẳng may host mình cũng dùng port 8888 này ở chương trình đâu đó, vì mình không quyết định và control được port map (interface) như cách 1 để tùy chỉnh. Còn để host kiểu này khó biết được.  Bước 4 Publish lên docker hub: hub. docker. comKhi ở máy khác, ta muốn pull image về chạy:    Bước 1: Kiểm tra version docker, nếu chưa download thì chạy lệnh install docker vesion     Bước 2: Kéo image về docker pull senrie/hello-docker  Kiểm tra lại = cách: docker image ls  Bước 3: Chạy chương trình trên machine docker run senrie/hello-dockerVề PORTS trong Docker: Ta có thể xem các container đã build bằng lệnh docker ps : danh sách container đang chạy, và lệnh docker ps -a: tất cả container đã build. Ta thấy có cột PORTS, ví dụ: 0. 0. 0. 0:80-&gt;8888/tcp, :::8888-&gt;8888/tcpMình sẽ giải thích từng chỉ số:  0. 0. 0. 0: nghĩa là có thể dùng tất cả các IP của máy (IP card mạng LAN, IP wifi,. . có thể check bằng ifconfig) để làm gateway cho bên ngoài truy cập vào thông qua port cụ thể bên dưới.  80 (layer ngoài): là port của máy host expose cho bên ngoài dùng để truy cập vào, như cánh cổng mở ra 8888 (layer trong): là listen port của chương trình trên container trong máy host của chúng ta.      Port 80 sẽ được setup trên tất cả các gateway và centralize lại để link với container có port 8888. Nghĩa là ta dùng IP gateway:80 thì sẽ tiếp cận được đến chương trình container.    Để check xem máy chúng ta đang chạy các port nào tránh conflict, ta dùng lệnh:sudo netstat -plunt Xem qua về Docker run để hiểu hơn:https://docs. docker. com/engine/reference/commandline/run/ Xem cấu hình: Các cmd thường dùng:  docker inspect &lt;containerID&gt; : xem các gateway, ip, network,… của container để biết nó đang xài network nào     Nếu cài port khi run thì network là “bridge” (khi netstat sẽ ra docker-proxy)   Nếu dùng –net=host thì network là “host”   …      wrk -t4 -c400 -d30s &lt;url&gt; : xem perform mance của web (tham khảo thêm tại https://github. com/wg/wrk):   Các bước fix bugs:   Bước 1: Xác định xem lỗi gì (dùng curl, Postman hoặc F12) Bước 2: Coi log. Ví dụ:  docker logs -f $(docker ps | grep  83-&gt;  | awk '{print $1}')  Thêm tag -f để xem log real time không cần in hết log.   Bước 3: Debug     Dynamic Analysis: dùng break point hoặc print ra console,…=&gt; Cách này áp dụng tốt cho các trường hợp phức tạp và đơn giản cũng ok nếu đoán được code chỗ nào lỗi.    Static Analysis: đọc code chay (khi không biết chỗ nào run function đó, thường thì trong hướng đối tượng hay gặp phải)   Tìm hiểu về Volumn trong Docker: "
    }, {
    "id": 31,
    "url": "https://tuyen-nnt.github.io/import-begin/",
    "title": "Import Data - Part 1",
    "body": "2021/04/10 - 1. Flat file:  Là file text thông thường chứa các record tổ hợp bởi các trường hoặc attribute (thuộc tính) và mỗi trường chứa nhiều nhất 1 thông tin.  Flat file chứa các row và mỗi row là 1 record.  Flat file chỉ dùng để chứa dữ liệu của 1 bảng tính.  Nó không có relationships như relational database Rất phổ biến trong data scienceCác loại flat file:  . csv . txt Có dấu phân cách , hoặc tab2. Các cách import Flat file: Có 2 loại package chính để import, tùy trường hợp sử dụng:  NumPy : nếu dữ liệu column cơ bản là số, vì:     Nó là lưu dữ liệu số dạng mảng một cách hiệu quả và nhanh   Nó cần thiết cho các package khác như scikit-learn sử dụng trong Machine-learning.    Nó có nhiều hàm hỗ trợ để dễ dàng import các mảng data số như loadtxt() và genfromtxt().     pandas : nếu dữ liệu column là cả số lẫn string, dataframe là dtype sinh ra để lưu dạng hỗn hợp này. Import flat file bằng NumPy# Import packageimport numpy as np# Assign filename to variable: filefile = 'digits. csv'# Load file as array: digitsdigits = np. loadtxt(file, delimiter=',')# Print datatype of digitsprint(type(digits)) Hàm np. loadtxt()tạo object numpy dạng mảng, cần 2 tham số là:     Tên file   Dấu phân cách    Chúng ta có thể tùy chỉnh bằng cách thêm các tham số khác vì NumPy có nhiều sự lựa chọn:```  Import numpy  import numpy as np  Assign the filename: filefile = ‘digits_header. txt’ Load the data: datadata = np. loadtxt(file, delimiter=’\t’, skiprows=1, usecols=[0,2]) Print dataprint(data) * Các tham số ở trên bao gồm:	* ``skiprows= n`` : không lấy data của n dòng đầu tiên để đưa vào mảng, vì có thể đó là header hoặc các record mà ta không muốn lấy. 	* ``usecols=[]`` : tùy chọn những cột mà ta muốn lấy giá trị cách nhau bởi dấu phẩy, index côt bắt đầu từ 0 đến n-1 (với n là số côt). 	* ``print(array_object)`` : dùng để in object mảng ra console. 		Assign filename: filefile = ‘seaslug. txt’ Import file: datadata = np. loadtxt(file, delimiter=’\t’, dtype=str) Print the first element of dataprint(data[0]) Import data as floats and skip the first row: data_floatdata_float = np. loadtxt(file, delimiter=’\t’, dtype=float, skiprows=1) Print the 10th element of data_floatprint(data_float[9]) * Trong trường hợp set data của bạn có chứa các value với kiểu dữ liêu khác nhau ví dụ như giá trị dòng đầu tiên là header chứa string data. Có 2 cách để xử lý:	* *Cách 1*: Thêm tham số ``dtype=str`` để tất cả các giá trị khi import vào đều là string, và không bị báo lỗi ``ValueError``. 	* *Cách 2*: dùng tham số ``skiprows=n``, với n là số row sẽ skip từ row đầu tiên. Cách này nếu chúng ta biết được row nào có chứa giá trị khác các giá trị còn lại mà gây ra ``ValueError``. 	Plot a scatterplot of the dataplt. scatter(data_float[:, 0], data_float[:, 1])plt. xlabel(‘time (min. )’)plt. ylabel(‘percentage of larvae’)plt. show() * Sau khi có mảng data ta có thể tùy biến thành biểu đồ như mã code trên. #### 3. Import các loại file khác##### Excel spreadsheetsImport pandasimport pandas as pd Assign spreadsheet filename: filefile = ‘battledeath. xlsx’ Load spreadsheet: xlsxls = pd. ExcelFile(file) Print sheet namesprint(xls. sheet_names) * Lưu ý ở đây biến``xls`` mới chỉ là object Excel do pandas định nghĩa, chưa phải là dataframe. Vì excel có nhiều sheet nhưng 1 dataframe chỉ có thể là 1 sheet. Nên ta sẽ xử lý ở bước sau. * Hàm ``. sheet_names`` in ra tên tất cả các sheet trong bảng tính. Load a sheet into a DataFrame by name: df1df1 = xls. parse(‘2004’) Print the head of the DataFrame df1print(df1. head()) Load a sheet into a DataFrame by index: df2df2 = xls. parse(0) Print the head of the DataFrame df2print(df2. head()) * Hàm ``. parse()`` sẽ giúp rút trích df (sheet) của excel file và cần ta đưa vào tham số là tên của sheet hoặc chỉ số từ 0-(n-1), với n là số lượng sheet để load dataframe. ##### Pickled file (chuỗi byte hay bytestream =&gt; native trong Python)* Thực tế có những dạng datatype như dictionary hay list không có cách nào rõ ràng đưa vào lưu trong flat file như các datatype numpy. array hay pandas. dataframe. Do đó, pickle file ra đời. Đây là loại file dùng ngôn ngữ native mà con người đọc không hiểu. Nếu như bạn chỉ muốn import data thì chỉ cần *serialize* datatype dict hay list,. . . bằng cách convert nó sang dạng bytestream để trở thành pickled file. * Ở bài này, chúng ta chưa nói đến cách convert, mà sẽ học cách mở file pickled đã được convert sẵn và lưu ở local thay vì mở flat file như đã học trước đó. &gt; Có thể hiểu pickled file là file hỗ trợ bạn lưu dữ liệu có datatype kiểu dictionary, list,. . . ##### SAS7BDAT file ``. sas``SAS là viết tắt của Statistical Analysis System, dùng trong BA, BI, tính toán phân tích data hay thống kê về sinh học,. . . Import sas7bdat packagefrom sas7bdat import SAS7BDAT Save file to a DataFrame: df_saswith SAS7BDAT(‘sales. sas7bdat’) as file:  df_sas = file. to_data_frame() Print head of DataFrameprint(df_sas. head()) Plot histogram of DataFrame features (pandas and pyplot already imported)pd. DataFrame. hist(df_sas[[‘P’]])plt. ylabel(‘count’)plt. show() ##### Stata file ``. dta``Là sự kết hợp giữa statistics + data, dùng trong các nghiên cứu học thuật data về dịch tễ hay khoa học xã hội. import pandas as pddf = pd. read_stata(‘disarea. dta’) ##### HDF5Là loại file dùng để lưu trữ lượng data số lớn lên đến hàng trăm GBs hay TBs, thậm chí có thể scale lên ExabytesImport packagesimport numpy as npimport h5py Assign filename: filefile = ‘LIGO_data. hdf5’ Load file: datadata = h5py. File(file, ‘r’) Print the datatype of the loaded fileprint(type(data)) Print the keys of the filefor key in data. keys():  print(key) Get the HDF5 group: groupgroup = data[‘strain’] Check out keys of groupfor key in group. keys():  print(key) Set variable equal to time series data: strainstrain = data[‘strain’][‘Strain’]. value Set number of time points to sample: num_samplesnum_samples = 10000 Set time vectortime = np. arange(0, 1, 1/num_samples) Plot dataplt. plot(time, strain[:num_samples])plt. xlabel(‘GPS Time (s)’)plt. ylabel(‘strain’)plt. show()``` MATLAB . mateach row is an instance of entity type "
    }, {
    "id": 32,
    "url": "https://tuyen-nnt.github.io/csdl/",
    "title": "The origin of Database",
    "body": "2021/04/01 - Đi từ các cấu trúc dữ liệu như Array, Linked list, B-tree,… =&gt; Các cấu trúc dl này chỉ lưu những con số, không lưu được 1 tập dữ liệu liên quan với nhau =&gt; để lưu trữ tập hợp thông tin, người ta tạo ra CSDL =&gt; Để thêm xóa sửa nhanh chóng, người ta lại tạo ra ngôn ngữ SQL để làm phương tiện truy vấn dữ liệu Trừu tượng hóa dữ liệu cho phép mô tả dữ liệu theo  Đối tượng Thuộc tính Liên kết"
    }, {
    "id": 33,
    "url": "https://tuyen-nnt.github.io/ubuntu-basic/",
    "title": "Learn Linux commands on Ubuntu",
    "body": "2021/03/22 - Làm quen với Ubuntu1. Một số câu lệnh thường dùng: Part 1: Tải ứng dụng trên ubuntu: sudo apt-get install alacarte Uninstall ứng dụng: sudo apt-get remove alacarte Đổi tên file: mv [đường dẫn với tên cũ] [đường dẫn với tên mới] Ví dụ: Đổi tên tập tin test1. txt trong /root thành test. txt: mv /root/test1. txt /root/test. txtmv Di chuyển file:mv [đường dẫn nguồn] [đường dẫn đích] Ví dụ: Di chuyển và đổi tên tập tin *test1. txt trong /root sang /etc đổi tên thành test. txt: mv /root/test1. txt /etc/test. txt Mở file xem dùng lệnh:cat &lt;đường dẫn hoặc tên file trong thư mục hiện hành&gt; Edit file hoặc tạo file mới dùng lệnh:nano &lt;đường dẫn hoặc tên file trong thư mục hiện hành&gt; Tìm file trong thư mục hiện hành: find . [tên file]find . //list ra tất cả các file trong thư mụcCập nhật phiên bản, ví dụ npm 7. 13. 0: npm install -g npm@7. 13. 0 Mở OVPN: sudo openvpn --config Downloads/client. ovpn Mở ứng dụng sau khi cd vào folder chứa file thực thi, dùng lệnh sau để mở ứng dụng : . /datagrip. sh Tải Remarkable để viết markdown thông qua wget thay vì snap hoặc apt-get : wget https://remarkableapp. github. io/files/remarkable_1. 87_all. debsudo dpkg -i remarkable_1. 87_all. deb*Chạy đến đây nếu gặp lỗi  dpkg: dependency problems prevent configuration of remarkable  ta chạy câu lệnh*&gt; sudo apt-get install -fsudo apt-get install -fReset lai cau commit trước đó git reset --soft HEAD^^COveride commit đã push^C git push origin +masterPush lên brach master git push origin +master Part 2: update 23. 05. 2021: Xem lịch sử viết cmd:cat ~/. bash_history Giải nén với file taztar -xvzf . . .  Giải nén với file taz:tar -xvf . . .  Giải nén với file deb:sudo dpkg -i . . .  Để xem nó là dạng file gì :file . . . .  Xem là file hay thư mục và các quyền:ls -l Nếu cần quyền root để thực thi thì đổi sang quyền owner cho user sudo chown -R tuyen:tuyen . //thay đổi owner cho user, dấu  .   chỉ tất cả các file trong thư mục hiện hành, tuyen là user : tuyen là user groupchmod +x Để bổ sung cấu hình cho hệ thống: sudo nano hostsEx:# Một ứng dụng cần config&lt;ip&gt; www. XXX. com&lt;ip&gt; licxxxx. XXXX. comvoi ip la so dang XXX. X. X. X//Lưu ý dùng quyền sudo với những file của hệ thốngGiải nén file . deb dùng lệnh:sudo dpkg -i remarkable_1. 87_all. deb 2. Cài đặt server tải package apt-get về từ VN để nhanh hơn dùng mặc định của nước ngoài: cd /etc/aptlssudo cp sources. list sourceslist. bak//back-up filesudo sed -i 's/vn. archive. ubuntu. com/mirror. bizflycloud. vn/' sources. list//thay thế tất cả link /vn. archive. ubuntu. com = link mirror. bizflycloud. vn trong file sources. listcat sources. listsudo apt-get updatesudo rm sourceslist. bak 3. Compile ứng dụng từ source code thủ công: Đối với những ứng dụng dùng apt-get gặp vấn đề version hoặc không tương thích với máy, ta sẽ chuyển sang dùng cách này. Tuy nhiên cách này nếu may mắn sẽ compile nhanh, ngược lại có nhiều trường hợp thiếu thư viện khi . /configure dẫn đến mất nhiều thời gian. Đó là lí do nhiều người sẽ cài đặt Docker để compile từ source code nhanh =&gt; mình sẽ hướng dẫn ở phần sau.  B1: Download source code về từ git dùng git clonegit clone &lt;đường dẫn&gt; B2: cd vào thư mục ứng dụng B3: Cấu hình cho ứng dụng chạy trên máy bạn, ở bước này đòi hỏi sự kiên nhẫn để cài đặt thêm những thư viện cần, chú ý xem lỗi từ cmd để fix chính xác. . /configureKhi tải thêm thư viện thì cú pháp:  sudo apt-get install &lt;tên thư viện&gt;-dev//Lưu ý phải có -dev vì -dev là source code của mấy cái thư viện  B4: Compile từ source code thành file binary  make//Trong quá trình runtime nếu được yêu cầu tải thêm thư viện thì đuôi thư viện ko cần -dev nữa, vì các file config đã được compile ra binary hết rồi, ko còn là source code nữa =&gt; do vậy bỏ đuôi -dev để tương thích  B5: Install file binary vô hệ thống hay nói cách khác là chuyển những file đã được compile ở trên vô hệ thốngsudo make install4. Cách pin ứng dụng lên thanh dock ubuntu:  Bước 1: Tải file tar ứng dụng về và giải nén bằng lệnh tar -xvzf [tên_file] Bước 2: Tạo file tên ứng dụng có đuôi . desktop trong thư mục . local/share/applications, ví dụ webstorm. desktop bằng lệnh nano webstorm. desktop Bước 3: Mở file . desktop và paste dòng sau:# https://askubuntu. com/questions/975178/duplicate-icons-in-the-dock-of-gnome-shell-ubuntu-17-10/9752 30#975230# https://askubuntu. com/questions/990833/cannot-add-custom-launcher-to-dock-add-to-favorites# dconf-editor[Desktop Entry]Type=ApplicationTerminal=falseIcon[en_US]=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. svgName[en_US]=WebStormExec=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. shName=WebStormIcon=/home/quocbao/Tools/WebStorm-2019. 2/bin/webstorm. svgStartupWMClass=jetbrains-webstormTrong đó: * Icon: chứa đường dẫn thư mục có hình ảnh biểu tượng cho ứng dụng* Name: tên của ứng dụng* StartupWMClass: được lấy bằng cách chạy câu lệnh ``xprop WM_CLASS`` và bấm vào ứng dụng để hiện mã trên Terminal sau đó copy vào.  Bước 4: Gõ câu lệnh sudo apt-get install dconf-editor nếu chưa installTiếp theo gõ lệnh dconf-editor Sau đó chọn đến thư mục trong cửa số hiện lên:org/gnome/shell/favorite-apps Tại mục Customize value gõ tại vị trí mong muốn xuất hiện:  tên_file. desktop Lưu ý: các tên file cách nhau bởi dấu phẩy Một số lưu ý trong lúc thực hiện::  Khi file . sh không có quyền thực thi (kiểm tra quyền bằng câu lệnh ls -l &lt;tên_file&gt;, dùng tên file nếu đang ở trong thư mục chứa file còn không dùng đường dẫn đến file), ta cần thêm quyền thực thi (quyền mở file) bằng cách gõ câu lệnh sau:chmod +x &lt;đường dẫn file&gt;Tìm hiểu thêm các quyền tại đâyTìm hiểu thêm các lệnh khác tại đây Các loại đường dẫn:  ~/ : thay cho home /thư mục/… : là đường dẫn tuyệt đối, phải tuyệt đối chính xác, không rút gọn thư mục/…/ : là đường dẫn tương đối, đôi khi không có đoạn thư mục đầu vì đã ở trong thư mục đó rồi, v. v . / : chỉ thư mục hiện hành, nhưng đôi khi có các câu lệnh như “git . / bấm tiếp tab” sẽ gợi ý ra các file có thể áp dụng, nếu “git” thôi thì sẽ được list ra các câu lệnh có thể dùng với git, ví dụ . /vscode. shCre:    Part 1:https://askubuntu. com/questions/975178/duplicate-icons-in-the-dock-of-gnome-shell-ubuntu-17-10/975230#975230https://askubuntu. com/questions/990833/cannot-add-custom-launcher-to-dock-add-to-favoriteshttps://vinasupport. com/chmod-la-gi-huong-dan-su-dung-lenh-chmod-tren-linux-unix/https://blogd. net/linux/lam-viec-voi-tap-tin-va-thu-muc-tren-linux/     Part 2:https://www. liquidweb. com/kb/how-to-install-software-from-source-on-ubuntu/https://help. ubuntu. com/community/CompilingEasyHowTohttps://help. ubuntu. com/community/CompilingEasyHowTo  "
    }, {
    "id": 34,
    "url": "https://tuyen-nnt.github.io/data-toolbox/",
    "title": "Data Toolbox",
    "body": "2021/03/17 - Chapter 1 : Các công cụ làm việc trong mảng Data EngineeringI. Database: 1. Database là gì?: Database là tập hợp dữ liệu lớn được tổ chức đặc biệt để tìm kiếm và rút trích dữ liệu nhanh.  Các đặc điểm của nó:  Lưu trữ dữ liệu Tổ chức dữ liệu Rút trích/Tìm kiến dữ liệu thông qua DBMS2. Phân biệt dữ liệu có cấu trúc và dữ liệu không cấu trúc: Có cấu trúc:: Database có schema, ví dụ như các relational database Bán cấu trúc (semi-structured):: Ví dụ như file JSON { “key” : “value” }Không cấu trúc:: Không có schema, giống như các tập tin : videos, hình ảnh 3. Phân biệt SQL và NoSQL: SQL database có những đặc điểm sau::  Có các bảng để truy vấn Có Database schema (xác định sự liên hệ và thuộc tính của database) Có các Relational database (có quan hệ với nhau) Các hệ quản trị cơ sở dữ liệu phổ biến của ngôn ngữ này: MySQL, PostgreSQLNoSQL database có những đặc điểm sau::  Có các Non-relational database Thường gắn với các dữ liệu không có schema (unstructured), nhưng đôi khi có các dữ liệu có schema (structured) Lưu trữ các cặp key-value (e. g. dùng cho caching hoặc cấu hình phân tán “distributed configuration”) Những ứng dụng thường dùng NoSQL là những ứng dụng chứa cặp key-value như Redis hoặc MongoDB có model là document database. *Các giá trị trong document database thường là các object có cấu trúc hoặc bán cấu trúc, ví dụ như JSON object. 4. SQL: Tìm hiểu về Database chema: Database schema có vai trò gì?:  Có nhiệm vụ mô tả cấu trúc và các mối quan hệ của các bảng trong database thông qua SQL code và sơ đồ schema Star schema: Trong datawarehouse database, thường những schema chúng ta thấy là star schema có sơ đồ giống như hình ngôi sao. Fact table ở giữa và xung quanh là các Dimension tables, trong đó:  Fact table: là table chứa các dữ liệu xảy ra trên thế giới như product orders, v. v Dimension table: là các table mang thuộc tính để mô tả cho fact table như màu sắc, kích thước, v. vTìm hiểu thêm về Star chema tại đây Ví dụ thực tế:Giả sử ta có database mô tả cấu trúc dữ liệu cho hệ thống bán hàng ở 1 cửa hàng quần áo: Giả sử ta có bảng invoice lưu dữ liệu bán hàng (fact table) có các cột sau:       id (primary key)   created_date_time   items_id   payment_method   customer_id   card_id             123   12:00:00   12/12/2021   1   Bank   111   9116   Ta sẽ có các bảng dimension sau để bảng invoice liên kết mối quan hệ nhằm mô tả chi tiết các giao dịch invoice: Bảng Item       id   item_name   category   color   price         1   Quần đùi lửng   Quần   Đen   200000   Bảng Customer       id   name   birthday   card_id   is_valid   num_of_orders         111   Thanh Tuyền   09/XX/XXXX   9116   0   5   Bảng Card       id   customer_id   point   expired_date         9116   111   2100   31/12/2025   II. Parallel computing: Parallel computing là gì?: Trước tiên ta đi từ ý tưởng của việc tạo ra parallel computing.  Nó hình thành nên nền móng cho các công cụ xử lý data hiện đại (data processing tool) Tuy nhiên lí do quan trọng nhất phải nói đến là để tối ưu Bộ nhớ và hiệu năng xử lý dữ liệuCách thức hoạt động::  Chia task thành nhiều task phụ Phân tán task phụ trên một vài máy tính (thường là các commodity computer hoặc đã có sẵn để ít tốn phí thay vì sử dụng các siêu máy tính) Các máy tính làm việc song song với nhau trên các task phụ, do đó các task được hoàn thành nhanh hơnChúng ta hãy cùng xem ví dụ vận hành shop may đồ: Đặt mục tiêu may 100 cái áo, thì shop nhận thấy:  Nhân viên may giỏi nhất nhóm may 1 cái áo trong 20 phút =&gt; 12 cái áo trong 4 tiếng Những nhân viên khác thì may 1 cái áo trong 1 tiếngSau khi áp dụng mô hình parallel bằng cách:  Chia ra 4 chu kỳ Mỗi chu kì 25 cái áo và 4 nhân viên may làm việc song songThì ta thấy được nếu các nhân viên may có khả năng thấp hơn khi cùng làm việc song song với nhau thì sẽ làm được 16 cái áo trong 4 tiếng. Nói về thực tế máy tính, 4 nhân viên may có năng lực thấp hơn thì sẽ giống như là các máy tính có sẵn, ít tốn phí, thay vì ta phải thuê 1 nhân viên giỏi hay có thể nói là siêu máy tính thì chi phí sẽ cao hơn nhiều nhưng không chắc chắc hiệu quả bằng. Tóm lại, lợi ích của parallel computing:  Hiệu suất xử lý Bộ nhớ: chia dữ liệu thành các tập hợp con và đưa vào bộ nhớ của các máy tính khác nhau=&gt; mức chiếm dụng bộ nhớ tương đối nhỏ và dữ liệu được đưa vào trong bộ nhớ gần RAM nhẩtNhững rủi ro khi áp dụng: Overhead due to communication  Yêu cầu chưa đủ lớn để áp dụng Cần nhiều processing units hơn Đôi khi tách ra sẽ gặp vấn đề về program runtime, yêu cầu tốn nhiều thời gian để xử lý giao tiếp giữa các processes hơn so với không tách ra. Việc có nhiều chương trình hơn sẽ khiến thời gian contact switch giữa các chương trình nhiều hơn không phù hợp khi yêu cầu chưa đủ lớn. Xem hình bên dưới:Code Python để chia thành nhiều task phụ: Sử dụng API multiprocessing. Pool Sử dụng DASK framework để tránh viết API dưới hệ thống Một số ghi chú cho Parallel computing:    Không phải lúc nào cũng tăng thời gian xử lý công việc, do hiệu ứng của overhead communication ở phần contact switch giữa các ứng dụng.   GIúp tối ưu việc sử dụng bộ xử lý đa nhiệm (multiple processing unit)  Giúp tối ưu bộ nhớ giữa một số hệ thống Bài viết về Parallel computing đến đây là kết thúc, các bạn có thể xem qua 1 số đoạn code liên quan đến chủ đề này ở repo data-engineering của mình nhé! III. Parallel computation framework: 1. Hadoop: Hadoop là tập hợp các dự án open-source được maintain bởi Apache Software Foudation. Có 2 dự án phổ biến thường được nhắc đến là MapReduce và HDFS. a. Nói về HDFS: Đây là tên gọi của hệ thống phân tán tập tin (distributed file system). Cũng là các tập tin trong máy tính nhưng được điều đặc biệt là nó được phân tán trên nhiều máy tính khác nhau. HDFS là một phần thiết yếu trong Big data để lưu trữ dữ liệu lớn.  b. Nói về MapReduce: Đây là một trong các mô hình đầu tiên phổ biến trong việc xử lý Big data. Cách hoạt động của nó là chia task lớn thành nhiều task nhỏ rồi phân tán khối lượng dữ liệu và dữ liệu đến các đơn vị xử lý (processing unit). Tuy nhiên, MapReduce có một số khuyết điểm có thể kể đến như khó viết các job để chia task và phân tán chẳng hạn.  Chương trình phần mềm Hive và một só phần mềm khác ra đời để giải quyết khó khăn trên. Hive như là lớp vỏ trên cùng trong hệ sinh thái của Hadoop giúp các dữ liệu đến từ những nguồn khác nhau có thể truy vấn bằng cách sử dụng ngôn ngữ truy vấn có cấu trúc Hive SQL (HQL). Ví dụ đoạn truy vấn Hive SQL:  Nhận xét: trông chả khác câu lệnh SQL thông thường phải không ? ^^ Tuy nhiên sau tấm màn đó thì mọi thứ sẽ khác. Câu truy vấn trên sẽ chuyển đổi thành job có nhiệm vụ phân tán đến tập hợp các máy tính đấy (cluster). 2. Spark framework: Ngoài Hadoop, ta còn có Spark, cũng có nhiệm vụ phân tán các task xử lý dữ liệu giữa các cluster, ngày nay được sử dụng phổ biến hơn. Trong khi hệ thống Hadoop MapReduce cần ổ đĩa đắt tiền để ghi dữ liệu giữa các job, thì Spark lại sử dụng một cách tối ưu bộ nhớ xử lý tránh sử dụng ổ đĩa để ghi dữ liệu. Spark ra đời đã cho thấy những hạn chế của MapReduce, trong đó bao gồm việc MapReduce hạn chế tương tác của người dùng khi truy cập phân tích dữ liệu và mỗi bước build sẽ phải dựa trên bước trước đó. =&gt; không linh hoạt, khó tương tác hơn Spark. a. Kiến trúc của Spark: Dựa trên RDDs (Resilient distributed datasets) RDD là một loại cấu trúc dữ liệu có nhiệm vụ duy trì các dữ liệu được phân tán giữa nhiều node. Không giống với DataFrame, RDDs không có các cột. Về khái niệm thì có thể xem nó là một dãy các tuples, ví dụ về tuple “day”: day = ('monday', 'tuesday', 'wednesday' , 'thursday', 'friday', 'saturday' , 'sunday')Với các dữ liệu có cấu trúc RDD ta có thể thực thi 2 loại lệnh :  Chuyển đổi: dùng method . map() hoặc . filter() =&gt; output ra các dạng dữ liệu đã được chuyển đổi Hành động: dùng method . count() hoặc . first() =&gt; ra 1 output duy nhất (số, chữ, v. v)Spark framework có nhiều interface ứng với các ngôn ngữ lập trình, phổ biến nhất là PySpark dùng ngôn ngữ Python. Ngoài ra còn có các ngôn ngữ khác như Scala, R.  **PySpark dùng host dựa trên Dataframe trừu tượng, đó là lí do nếu chúng ta thường sử dụng thư viện pandas của dataframe sẽ dễ làm quen hơn vì hoạt động của PySpark sẽ tương tự thế.    Tóm lại là các công việc về parallel computing từ đơn giản đến phức tạp cứ để nhà Spark lo :smile: còn chaỵ như thế nào là tùy bạn.  Xem ví dụ về PySpark khi tính trung bình các vận động viên theo nhóm tuổi nhé: "
    }, {
    "id": 35,
    "url": "https://tuyen-nnt.github.io/one-day-just-started-adventure/",
    "title": "One day we just started our adventure",
    "body": "2020/01/12 - The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.  It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story. The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. "
    }, {
    "id": 36,
    "url": "https://tuyen-nnt.github.io/git-commit/",
    "title": "How to write effective git-commit?",
    "body": "2019/07/31 - Mẹo viết commit message hữu ích khi dùng gitGit là một hệ thống quản lý phiên bản phân tán phổ biến với mục đích theo dõi sự thay đổi của các tập tin trên máy tính và phối hợp công việc nhóm trên những tập tin này. Là một lập trình viên thì một trong những công cụ cơ bản mà chúng tôi không thể bỏ qua chính là việc sử dụng git trong quy trình làm việc. Sau đây là một trong những sai lầm mà dân lập trình hay mắc phải, về mặt kỹ thuật thì không hẳn nhưng chúng ta hãy tự hỏi rằng “Nếu bạn có thể làm tốt hơn thì tại sao không làm?” Một số quy tắc nên ghi nhớ khi viết commit message :    Tách dòng chủ đề ra khỏi phần thân bằng một dòng trống     Giới hạn dòng chủ đề tối đa 50 kí tự     Viết hoa dòng chủ đề     Sử dụng câu mệnh lệnh ở dòng chủ đề     Không kết thúc dòng chủ đề bằng dấu chấm     Gói gọn phần thân tối đa 72 kí tự     Sử dụng phần thân để giải thích “Cái gì”, “Tại sao” với “Như thế nào”  Sai lầm #1:: Chúng ta có khuynh hướng trộn lẫn chủ đề với phần thân của một commit message : Cách viết phần chủ đề chung với phần thân của thông điệp là một cách làm sai. Khi bạn nhận thấy commit message quá dài để giải thích thì có nghĩa là commit đang thực hiện quá nhiều thứ sẽ phá vỡ nó đi. Bạn viết:  git commit –m “added new css styling for danger button in order to differentiate between the primary button and other button styles. ” Thay vì nên viết:  git commit –m “Add new feature” Sai lầm #2:: Không giới hạn chủ đề tối đa 50 kí tự, phần thân của commit message tối đa 72 kí tự: Luôn đảm bảo chủ đề của commit không bao giờ vượt quá 50 kí tự và đây là qui tắc ngón tay cái. Thêm nhiều hơn số kí tự nên có sẽ có khuynh hướng sẽ bị Github cắt bớt đi, và vì những gì chúng ta đang cố gắng truyền tải là để một user nào đó trong nháy mắt biết được một commit đang làm gì. Một vài commit message yêu cầu giải thích nhiều hơn đặc biệt là khi dòng chủ đề có nội dung mơ hồ, do đó thêm nội dung phần thân sẽ hữu ích trong những trường hợp này. Luôn cố gắng giới hạn số lượng kí tự phần thân tối đa 72 kí tự, và hãy để phần thân giải thích những gì commit đang thực hiện và tại sao lại làm như vậy. Sai lầm #3:: Không viết hoa cho chủ đề của commit Bạn viết:  git commit –m “added new feature” Thay vì nên viết:  git commit –m “Add new feature” Sai lầm #4:: Kết thúc dòng chủ đề bằng dấu chấm Bạn viết:  git commit –m “Modified codebase. ”Thay vì nên viết: git commit –m “Modify codebase” Sai lầm #5:: Không sử dụng câu mệnh lệnh ở dòng chủ đề: Mọi git commit đúng chuẩn nên đặt ở dạng mệnh lệnh. “Đơn giản điều này có nghĩa là câu được viết theo dạng một hành động”. Một dòng chủ đề git commit thích hợp nên hoàn thành đúng cấu trúc cho câu sau đây:  If applied, this commit will “your subject line here” Ví dụ:  If applied, this commit will “Add auth to X” Từ đó, ta nhận thấy sẽ không đúng với những dạng không mệnh lệnh:  If applied, this commit will “added auth to Y” Sai lầm #6:: Phần thân giải thích Cái gì, Tại sao với Như thế nào: Như đã giải thích ở số #2 trên, luôn đảm bảo phần thân của commit giải thích chính xác cái gì và tại sao commit đó đang thực hiện. Giải thích tại sao sự thay đổi đó là cần thiết, và khía cạnh khác mà nó mang lại. Thay vì miêu tả cách mà bạn giải quyết vấn đề trên. Một số câu nói vui về git:  Why did the commit cross the rebase? To git to the other repoTạm dịch là: Sao commit qua được rebase? Vì ta git đến repo khác :v  In case of fire: git commit, git push, leave the buildingTạm dịch là: Khi gặp cháy thì phải git commit, git push, rồi mới rời khỏi tòa nhà :v Cre: https://code. likeagirl. io/useful-tips-for-writing-better-git-commit-messages-808770609503?fbclid=IwAR16siuZjdzxxQ4z9gz89BST5DPxINisBxzxVB-emDa-tuLl26KdkV15vsI Bài viết về git commit đến đây là hết rồi, mời các bạn xem thêm các bài viết khác nhé ! Tuyen Nguyen "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});